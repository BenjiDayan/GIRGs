\chapter{Graph Kernels}
\label{chap:graph_kernels}
\minitoc

In this chapter we try to compare the bayesian evidence based plausibility of different GGMs in generating facebook graphs, with the assistance of graph kernels. Unfortunately the whole endeavour was unsuccessful, but we record our experiments here nonetheless.

\section{Bayes Factor Theory}
A bayesian approach to model selection is to compare $p(M_1 | D)$ and $p(M_2 | D)$. $M_1, M_2$ are possible models of the data, e.g. $M_1 = \cG_{1D\; \GIRG}$ and $M_2 = \cG_{CL}$. $D$ is a single graph instance $G$, or alternatively a whole dataset of our 100 facebook graphs, assuming that they all come from the same generative model family. We have some prior of $p(M_1)$ vs $p(M_2)$, e.g. $50 : 50$. We could possibly even have a $M_{d=1}, M_{d=2}, M_{d=3}, ...$ set of models for different dimensional GIRGs with some kind of decaying $p(1d\; \GIRG) > p(2d\; \GIRG) > p(3d\; \GIRG) > \dots$.

For now focussing on just $M_1$ vs $M_2$,
\begin{equation}
  p(M_1 | D) = \frac{p(D | M_1) p(M_1)}{p(D)} 
  \;
  \implies
  \;
  \frac{p(M_1 | D)}{p(M_2 | D)} = \frac{p(D | M_1) p(M_1)}{p(D | M_2) p(M_2)}
\end{equation}
Hence model selection is done with the ratio $\frac{p(M_1 | D)}{p(M_2 | D)}$ which is called the \textbf{Bayes Factor}.
We will ignore the priors $p(M_1), p(M_2)$ for now, and focus on the likelihoods $p(D | M_1), p(D | M_2)$.



\paragraph{More rigorous bayesian model comparison}
In our case if $M_1$ is a 1D GIRG, and we've decided to fix 
% $\alpha, e, \{w_u\}_{u \in V}$
$\alpha, \{w_u\}_{u \in V}$
, and just vary $\theta = c, \{\vec{x}_u\}_{u \in V}$, then we could Monte Carlo sample $\theta \sim p(\theta)$ from the prior to get an estimate for
\begin{equation} \label{eq:incorrect_girg_model_likelihood}
  p(D| M_1) = p(G | \cG_{\text{d-}\GIRG}) = \int_\theta p(G | \theta, \cG_{\text{d-}\GIRG}) p(\theta | \cG_{\text{d-}\GIRG}) d\theta
\end{equation}
% $p(D | M_1) = p(G) = \int_\theta p(G | \theta) p(\theta) d\theta$, 
The simplified notation is dropping the $| M_1$ as we fix into some dimensional GIRG universe, $\int_\theta p(G | \theta) p(\theta) d\theta$.

There's a problem here. Our data is a graph $D=G$. In the computation we actually need to inspect $p(G | \theta) = \sum_{\sigma} p(G' \stackrel{\sigma}{\cong} G | \theta) p(\sigma)$. Here $\sigma$ is a permutation of the node IDs. This is a confusing concept and here an abuse of notation - normally $p(G | \theta)$ is not a sum over permutations, but rather the probability of generating this exact graph. This will have to be contextually apparent. So instead of evaluating $p(G | \theta)$, we rather want to evalute "the probability of producing $G'$, given parameters $\theta$, which is isomorphic to $G$.

% Hence our simple Monte Carlo computation of $p(G | \theta, \GIRG) < p(G | CL)$ which was sad, is actually not quite valid. So Yay! We can still hope that the GIRG model is better than Chung-Lu, or at least bayesianly more likely.

% To be concrete, \cref{fig:automorphic_isomorphic_graphs} shows an example of graph auto/isomorphisms with different permutations $\sigma$.
% % As a concrete example, let's say $G=(1,2), (2, 3), (2, 4), (2, 5)$. Then we say $G$ is isomorphic to $G'$ with permutation $\sigma= (1,4)$ ($G \stackrel{\sigma}{\to} G'$) where $G' = (4, 2), (2, 3), (2, 1), (2, 5)$.
% So the bayesian evidence compuation is actually to sample $\theta$,  then for each $\sigma \in S_n$ create $G'$ where $G$ is isomorphic to $G'$ with permutation $sigma$ ($G \stackrel{\sigma}{\to} G'$), and calculate $p(G' | \theta)$; add these up and take the average.
% Finally repeat for further $\theta$ samples and take an outer average.


% \begin{tikzpicture}
%     \node[circle, draw] (1) at (0, 0) {1};
%     \node[circle, draw] (2) at (2, 0) {2};
%     \node[circle, draw] (3) at (1, 1.732) {3};
  
%     \draw[-] (1) -- (2);
  
% \end{tikzpicture}

\begin{figure}
    \begin{tikzpicture}[scale=1.5]
        % Graph 1
        \node[circle, draw] (g1v1) at (0, 0) {2};
        \node[circle, draw] (g1v2) at (1, 0) {1};
        \node[circle, draw] (g1v3) at (0.5, 0.866) {3};
    
        \draw[-] (g1v1) -- (g1v2);
    
        % Graph 2 (Automorphism)
        \node[circle, draw] (g2v2) at (3, 0) {1};
        \node[circle, draw] (g2v1) at (4, 0) {2};
        \node[circle, draw] (g2v3) at (3.5, 0.866) {3};
    
        \draw[-] (g2v2) -- (g2v1);
    
        % Graph 3
        \node[circle, draw] (g3v3) at (6, 0) {3};
        \node[circle, draw] (g3v2) at (7, 0) {2};
        \node[circle, draw] (g3v1) at (6.5, 0.866) {1};
    
        \draw[-] (g3v2) -- (g3v3);

        % % Arrows
        % \draw[->, thick] (g2v2) to[out=135,in=45] node[midway, above] {2 $\rightarrow$ 1} (g1v2);
        % \draw[->, thick] (g2v2) to[out=-135,in=-45] node[midway, below] {2 $\rightarrow$ 3} (g3v2);
      
        % Arrows
        \draw[<-, thick] (1.8, 0.2) -- node[midway, above] {$\sigma = (1\; 2)$} (2.3, 0.2);
        \draw[->, thick] (4.7, 0.2) -- node[midway, above] {$\sigma = (1\; 3)$} (5.2, 0.2);

    
        % Labels
        \node[align=center] at (0.5, -0.7) {$G_2$ \\Automorphic to $G_1$};
        \node[align=center] at (3.5, -0.7) {$G_1$};
        \node[align=center] at (6.5, -0.7) {$G_3$\\ Isomorphic to $G_1, G_2$};
    
    \end{tikzpicture}
    \caption{Example automorphic/isomorphic graphs}
    \label{fig:automorphic_isomorphic_graphs}
\end{figure}

To be concrete, take an example graph $G = (V, E) = (\{1, 2, 3\}, \{(1,2)\})$ which is shown as $G_1$ in \cref{fig:automorphic_isomorphic_graphs}. It's a "labelled graph". $G$ is isomorphic to any other labelled graph $H$ if they look the same if you anonymise the nodes.
More formally, if $f: V(G) \to V(H)$ is a bijection mapping nodes to nodes, such that $u \sim v$ in $G$ $\iff f(u) \sim f(v)$ in $H$, then $G$ is isomorphic to $H$, and the permutation $\sigma$ we denoted previously is precisely $f$. So $G$ is isomorphic to $H$ with edge set $\{(1, 3)\}$. You can even say that it is automorphic to the graph $H$ with edge set $\{(2, 1)\}$, i.e. just switching nodes $1,2$.
On this graph $G$ of 3 nodes, there are $3! = 6$ node ID permutations, each of which produces an isomorphic graph $G'$. For our particular $G$, we could group together these 6 isomorphic graphs $G'$ into 3 pairs of automorphic graphs.

Hence the bayesian evidence compuation is actually to sample $\theta$,  then for each $\sigma \in S_n$ create $G'$ where $G$ is isomorphic to $G'$ with permutation $sigma$ ($G \stackrel{\sigma}{\to} G'$), and calculate $p(G' | \theta)$; add these up and take the average.
Finally repeat for further $\theta$ samples and take an outer average.


Consider a simplified Chung-Lu / GIRG model where all nodes have the same weight $1$, and $\theta = (\vec{x}_1, \vec{x}_2, \vec{x}_3)$ of the three nodes. The Chung-Lu has identical $p(G')$ for each of the 6 ismorphisms. The GIRG  model does not - if $\vec{x}_1, \vec{x}_2$ are close, and $\vec{x}_3$ is far from both, then it awards higher probability to $G' = (V, \{(1,2)\})$ and $G' = (V, \{(2, 1)\})$. Furthermore both models always award the same probability to any equivalent class of automorphic graphs - this is because the adjacency matrix is the same for automorphic graphs, which is all that the models care about.

Unfortunately computing the $n!$ sized $\sum_{\sigma} p(G' \stackrel{\sigma}{\cong} G | \theta) p(\sigma)$ is infeasible for all but tiny graphs. As an alternative, we hoped to use a graph similarity kernel $k$ which compares two graphs $G, H$, giving some measure $k(G, H)$ of how similar they are up to ismorphism (in a node permutation invariant way).
Apparently one more computable example is the random walk kernel.
Therefore the idea would be to replace the incorrect $p(G | \cG_{\text{d-}\GIRG}) = \int_\theta p(G | \theta) p(\theta) d\theta$ with 
% $\mu(G) = \int_\theta k(G, G' \sim \cG_\theta) p(\theta) d\theta$. This is an idea.
$\mu(G) = E_{G' \sim \cG_{\text{d-}\GIRG}} \left [ k(G, G') \right ]$.

% Therefore the idea would be to replace the incorrect $p(G | \cG_{\text{d-}\GIRG}) = \int_\theta p(G | \theta, \cG_{\text{d-}\GIRG}) p(\theta | \cG_{\text{d-}\GIRG}) d\theta$ with $\mu(G) = E_{G' \sim \cG_{\text{d-}\GIRG}} \left [ k(G, G') \right ]$. This is an idea.

% So when we say $p(G | \theta)$, we actually mean the probability of getting a graph $G'$ which is isomorphic to $G$, given parameters $\theta$.



% TODO do we remove this weird figure - what's it doing here? 
\begin{figure}
    \centering

    \begin{subfigure}{0.49\textwidth}
      \centering
      \includegraphics[width=\linewidth]{figures/socfb-Amherst41-1d.png}
      \caption{$d=1$}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\textwidth}
      \centering
      \includegraphics[width=\linewidth]{figures/socfb-Amherst41-2d.png}
      \caption{$d=2$}
    \end{subfigure}
  
    \vspace{1em}
  
    \begin{subfigure}{0.49\textwidth}
      \centering
      \includegraphics[width=\linewidth]{figures/socfb-Amherst41-3d.png}
      \caption{$d=3$}
    \end{subfigure}
    \hfill
  
    \caption{MCMC runs for socfb-Amherst41, without failure rate}
    \label{fig:amherst_non_failure_mcmc}
\end{figure}

We see in \cref{fig:amherst_non_failure_mcmc} that Bayes Factor model comparison wise, 1D GIRGs are superior, even though according to edge accuracy, 2D GIRGs achieve a slight edge.


\section{Graph Kernel Introduction}
Graph Kernels provide a means for a similarity metric between graphs. They're ideally a positive semidefinite function $k: \Gamma \times \Gamma \rightarrow \mathbb{R}$, where $\Gamma$ is the set of all graphs. Such a function exists if and only if there is a corresponding feature map resentation of $\phi: \Gamma \to \cH_\phi$ where $\cH_\phi$ is a Hilbert space, and $k(G, G') = \langle \phi(G), \phi(G') \rangle_{\cH_\phi}$ is just an inner product.

Graph kernels give a simplified version of Blasius' classification framework. Blasius compares multiple different graph feature combinations on which to train an SVM for distinguishing two graph datasets. Instead we can replace this with a single kernel which hopefully encapsulates sufficient relevant information on the graph. The question of which graph features to use then shifts to which graph kernel to use!

Another benefit of graph kernels is, as a similarity metric, they give easier direct comparison between graphs.
In the previous sections' proposed paradigm, we can directly compare the similarity of a real graph $G$ with two synthetic graphs $G_1, G_2$, produced from different models $M_1, M_2$, using $k(G, G_1)$ and $k(G, G_2)$.
% This is simply done by comparing $k(G, G_1)$ and $k(G, G_2)$ - the higher of the two is the more similar.


% Graph Kernels have many uses, but we can use them in particular to help with bayesian model comparison, solving our issue of graph permutations.

% A survey on graph kernels https://appliednetsci.springeropen.com/articles/10.1007/s41109-019-0195-3



% We saw the equation $\mu(G) = E_{G' \sim \cG_{\text{d-}\GIRG}} \left [ k(G, G') \right ]$ in the previous chapter. We can Monte Carlo sample this expectation to get an estimate.

\section{Experiments with Random Walk Kernel; Weisfeiler-Lehman Kernel}
There are a range of graph kernels to choose from, however given the relatively large size of our graphs, runtime can be an issue. We ended up testing two kernels, however unfortunately both proved unsatisfactory. Without further expertise on graph kernels, we were forced to abandon this line of attack. We think that graph kernels might give more meaningful similarity metrics on smaller graphs with more unique structure (our random graph models and the real graphs themselves have a lot of homogenous structure) and particularly meangingful node labels.

\subsection{Random Walk Kernel}
The random walk kernel between two graphs is conceptually a count of the number of random walks of any length $l$ that exist in the product graph. This is generally an infinite sum, so geometric weighting with the factor $\lambda^l$ is used to decay the contribution of longer walks, where $0 < \lambda < 1$ (we tested with $\lambda=10^{-5}$).

The product graph between $G, G'$ is defined as $G_\times = (V_\times, E_\times)$ where $V_\times = \{(v, v') | v \in V, v' \in V'\}$ and $E_\times = \{((v_1, v'_1), (v_2, v'_2)) | v_1 \sim v_2 \in E_1, v'_1 \sim v'_2 \in E_2\}$. This definition can be extended to node-labelled graphs, where we only take product vertices $(v, v') \in V_\times$ if $v \in V, v' \in V'$ have the same label.

The naive implementation of the random walk kernel has complexity $O(n^6)$, however this can be sped up to $O(n^3)$. This is still too slow for our purposes. We only made limited tests with small graphs of $n \leq 3000$ nodes - see e.g. \cref{fig:rw_kernel_fitcopycube}.

\subsection{Weisfeiler-Lehman Kernel}
The Weisfeiler-Lehman kernel runs much faster in $O(h|E|)$ time, where $|E|$ is the number of edges in the graph and $h$ is the number of iterations of the algorithm. The algorithm requires some kind of node labelling - we colour nodes into a small discreet set of colours by grouping together nodes with similar sized degrees. It also uses a base kernel which is computed at each iteration - we used the simplest/speediest node label histogram dot product kernel: $k(G, G') = \langle \vec{f}, \vec{f}' \rangle$ where $\vec{f} = (f_1, \dots, f_k)$, $f_i = |\{ v \in V \;:\; l(v) = i\}|$ is the number of nodes with label $i$.

The output is $k_{WLK}(G, G') = k_{\text{base}}(G_1, G_1') + \dots + k_{\text{base}}(G_h, G_h')$. $G=G_1,\; G' = G'_1$, and $G_{i+1}$ is computed iteratively as relabelling each node in $G_i$ with $l(v) \gets (l(v), (l(u))_{u \sim v})$. This looks odd, but in practice each label is just rehashed as a new integer rather than becoming a highly nested tuple.



\subsection{Experiments}

Unfortunately both kernels failed to pass a basic test of ratifying one type of GIRG model over another. We tried a variety of combinations, but didn't get such reliable results: see \cref{fig:wl_kernel_gentorus} and \cref{fig:rw_kernel_fitcopycube}.

E.g. in \cref{fig:rw_kernel_fitcopycube}, the random walk kernel faield the basic test of having $k(G \sim \cG_1, G'_1 \sim \cG_1) > k(G \sim \cG_1, G'_2 \sim \cG_2)$, where $\cG_1$ is a 3D copy weight cube GIRG, and $\cG_2$ is copy weight Chung-Lu.

\begin{figure}
  \centering
\includegraphics[width=0.8\linewidth]{figures/d=2 alpha=1.2 n=70000 GIRG WL-Kernel with others.png}
\caption{WL-Kernel of a d=2, alpha=1.2, n=70000 Torus GIRG with other generated graphs (13 per model). All the GIRGs are more similar to the original than Chung-Lu, but we cannot differentiate between GIRGs of different dimensions.}
\label{fig:wl_kernel_gentorus}
\end{figure}

% socfb-Brandeis99 d=3.png


\begin{figure}
  \centering
\includegraphics[width=0.8\linewidth]{figures/socfb-Brandeis99 d=3.png}
\caption{RW-Kernel of a d=3 copy weight cube GIRG fit to socfb-Brandeis99 (matching number of edges and local clustering coefficient), with other generated graphs (6 per model type). Chung-Lu graphs have highest similarity to the original, despite it being a 3D GIRG}
\label{fig:rw_kernel_fitcopycube}
\end{figure}



% We think that 


% \subsection{Experiments}
% we do the comparison

% Cube Similarity Plots:
% We fit alpha, c for a uniform cube GIRG for dimensions d=1-3, produce a graph, and look at similarity with real graph.

\chapter{Graph Kernels}
\label{chap:graph_kernels}
\minitoc

In this chapter we try to compare the bayesian evidence based plausibility of different GGMs in generating Facebook graphs by using the method of \textit{graph kernels}. Unfortunately the whole endeavour was unsuccessful, but we nonetheless record here our experiments and the framework.

\section{Bayes Factor Theory}
A bayesian approach to model selection is to compare the posterior probability of different models $p(M_1 | D)$ vs $p(M_2 | D)$, given the model generative processes $P(D | M)$. 
$M_1, M_2$ are possible models of the data, e.g. $M_1 = \cG_{1D\; \GIRG}$ and $M_2 = \cG_{CL}$.
$D$ is a single graph instance $G$, or alternatively a whole dataset of our 100 Facebook graphs, assuming that they all come from the same generative model family.
We have some prior of $p(M_1)$ vs $p(M_2)$, e.g. $50 : 50$. We could even possibly have a set of models $M_{d=1}, M_{d=2}, M_{d=3}, ...$ for different dimensional GIRGs with some kind of decaying $p(1d\; \GIRG) > p(2d\; \GIRG) > p(3d\; \GIRG) > \dots$.

For now focussing on just $M_1$ vs $M_2$,
\begin{equation}
  p(M_1 | D) = \frac{p(D | M_1) p(M_1)}{p(D)} 
  \;
  \implies
  \;
  \frac{p(M_1 | D)}{p(M_2 | D)} = \frac{p(D | M_1) p(M_1)}{p(D | M_2) p(M_2)}
\end{equation}
Model selection is done with the ratio $\frac{p(M_1 | D)}{p(M_2 | D)}$ which is called the \textit{Bayes Factor}.
In order to compute the bayes factor, we will for now ignore the priors $p(M_1), p(M_2)$, and hence focus just on the likelihoods $p(D | M_1), p(D | M_2)$.



\subsection{Graph isomorphism problem in computing model evidence}
In our case if $M_1$ is a 1D GIRG, and we've decided to fix 
% $\alpha, e, \{w_u\}_{u \in V}$
$\alpha, \{w_u\}_{u \in V}$
 and just vary $\theta = c, \{\vec{x}_u\}_{u \in V}$, then we could Monte Carlo sample $\theta \sim p(\theta)$ from the prior to get an estimate for
\begin{equation} \label{eq:incorrect_girg_model_likelihood}
  p(D| M_1) = p(G | \cG_{\text{1d-}\GIRG}) = \int_\theta p(G | \theta, \cG_{\text{d-}\GIRG}) p(\theta | \cG_{\text{d-}\GIRG}) d\theta
\end{equation}
% $p(D | M_1) = p(G) = \int_\theta p(G | \theta) p(\theta) d\theta$, 
The simplified notation is dropping the $| M_1$ as we fix into some dimensional GIRG universe, $\int_\theta p(G | \theta) p(\theta) d\theta$.

There's a problem here. Since our data is a graph $D=G$, in the computation we actually need to inspect $p(G | \theta) = \sum_{\sigma} p(G' \stackrel{\sigma}{\cong} G | \theta) p(\sigma)$. Here $\sigma$ is a permutation of the node ID, and $\stackrel{\sigma}{\cong}$ denotes \textit{graph isomorphism} under this permutation. Note here our abuse of notation - normally $p(G | \theta)$ is not a sum over permutations, but rather the probability of generating this exact graph $G$; this will have to be contextually apparent. So instead of evaluating $p(G | \theta)$, we rather want to evalute \q{the probability of producing $G'$, given parameters $\theta$, which is isomorphic to $G$}.

% Hence our simple Monte Carlo computation of $p(G | \theta, \GIRG) < p(G | CL)$ which was sad, is actually not quite valid. So Yay! We can still hope that the GIRG model is better than Chung-Lu, or at least bayesianly more likely.

% To be concrete, \cref{fig:automorphic_isomorphic_graphs} shows an example of graph auto/isomorphisms with different permutations $\sigma$.
% % As a concrete example, let's say $G=(1,2), (2, 3), (2, 4), (2, 5)$. Then we say $G$ is isomorphic to $G'$ with permutation $\sigma= (1,4)$ ($G \stackrel{\sigma}{\to} G'$) where $G' = (4, 2), (2, 3), (2, 1), (2, 5)$.
% So the bayesian evidence compuation is actually to sample $\theta$,  then for each $\sigma \in S_n$ create $G'$ where $G$ is isomorphic to $G'$ with permutation $sigma$ ($G \stackrel{\sigma}{\to} G'$), and calculate $p(G' | \theta)$; add these up and take the average.
% Finally repeat for further $\theta$ samples and take an outer average.


% \begin{tikzpicture}
%     \node[circle, draw] (1) at (0, 0) {1};
%     \node[circle, draw] (2) at (2, 0) {2};
%     \node[circle, draw] (3) at (1, 1.732) {3};
  
%     \draw[-] (1) -- (2);
  
% \end{tikzpicture}

\begin{figure}
    \begin{tikzpicture}[scale=1.5]
        % Graph 1
        \node[circle, draw] (g1v1) at (0, 0) {2};
        \node[circle, draw] (g1v2) at (1, 0) {1};
        \node[circle, draw] (g1v3) at (0.5, 0.866) {3};
    
        \draw[-] (g1v1) -- (g1v2);
    
        % Graph 2 (Automorphism)
        \node[circle, draw] (g2v2) at (3, 0) {1};
        \node[circle, draw] (g2v1) at (4, 0) {2};
        \node[circle, draw] (g2v3) at (3.5, 0.866) {3};
    
        \draw[-] (g2v2) -- (g2v1);
    
        % Graph 3
        \node[circle, draw] (g3v3) at (6, 0) {3};
        \node[circle, draw] (g3v2) at (7, 0) {2};
        \node[circle, draw] (g3v1) at (6.5, 0.866) {1};
    
        \draw[-] (g3v2) -- (g3v3);

        % % Arrows
        % \draw[->, thick] (g2v2) to[out=135,in=45] node[midway, above] {2 $\rightarrow$ 1} (g1v2);
        % \draw[->, thick] (g2v2) to[out=-135,in=-45] node[midway, below] {2 $\rightarrow$ 3} (g3v2);
      
        % Arrows
        \draw[<-, thick] (1.8, 0.2) -- node[midway, above] {$\sigma = (1\; 2)$} (2.3, 0.2);
        \draw[->, thick] (4.7, 0.2) -- node[midway, above] {$\sigma = (1\; 3)$} (5.2, 0.2);

    
        % Labels
        \node[align=center] at (0.5, -0.7) {$G_2$ \\Automorphic to $G_1$};
        \node[align=center] at (3.5, -0.7) {$G_1$};
        \node[align=center] at (6.5, -0.7) {$G_3$\\ Isomorphic to $G_1, G_2$};
    
    \end{tikzpicture}
    \caption{Example automorphic/isomorphic graphs}
    \label{fig:automorphic_isomorphic_graphs}
\end{figure}

\paragraph{Small concrete example} of a labelled graph $G_1 = (V, E) = (\{1, 2, 3\}, \{(1,2)\})$ which is shown in \cref{fig:automorphic_isomorphic_graphs}. $G_1$ is isomorphic to any other labelled graph $H$ if they look the same if you anonymise the nodes.
More formally, if $\sigma: V(G) \to V(H)$ is a bijection mapping nodes to nodes, such that $u \sim v$ in $G$ $\iff \sigma(u) \sim \sigma(v)$ in $H$, then $G$ is isomorphic to $H$ with permutation $\sigma$, written $G \stackrel{\sigma}{\cong} H$.
So $G_1$ is isomorphic to $G_3$ with edge set $\{(3,2)\}$. You can even say that it is automorphic to the graph $H$ with edge set $\{(2, 1)\}$, i.e. just switching nodes $1,2$.
On this graph $G_1$ of 3 nodes, there are $3! = 6$ node ID permutations, each of which produces an isomorphic graph $G'$. For our particular $G_1$, we could group together these 6 isomorphic graphs $G'$ into 3 pairs of automorphic graphs.

% Hence the bayesian evidence compuation is actually to sample $\theta$,  then for each $\sigma \in S_n$ create $G'$ where $G$ is isomorphic to $G'$ with permutation $sigma$ ($G \stackrel{\sigma}{\to} G'$), and calculate $p(G' | \theta)$; add these up and take the average.
Finally repeat for further $\theta$ samples and take an outer average.


Consider a simplified Chung-Lu / GIRG model of three nodes where of $\theta = (\vec{x}_1, \vec{x}_2, \vec{x}_3)$ and all nodes have the same weight $1$.
The Chung-Lu has identical $p(G')$ for each of the 6 ismorphisms.
The GIRG  model does not - if $\vec{x}_1, \vec{x}_2$ are close, and $\vec{x}_3$ is far from both, then it awards higher probability to $G' = (V, \{(1,2)\})$ and $G' = (V, \{(2, 1)\})$.
Furthermore both models always award the same probability to any equivalent class of automorphic graphs - this is because the adjacency matrix is the same for automorphic graphs, which is all that the models care about.

Unfortunately computing the $n!$ length sum $\sum_{\sigma} p(G' \stackrel{\sigma}{\cong} G | \theta) p(\sigma)$ is infeasible for all but tiny graphs. As an alternative, we hoped to use a graph similarity kernel $k$ which compares two graphs $G, H$, giving some measure $k(G, H)$ of how similar they are up to ismorphism (in a node permutation invariant way).
% Apparently one more computable example is the random walk kernel.
Therefore the idea would be to replace the incorrect $p(G | \cG_\GIRG) = \int_\theta p(G | \theta) p(\theta) d\theta$ with 
% $\mu(G) = \int_\theta k(G, G' \sim \cG_\theta) p(\theta) d\theta$. This is an idea.
$E_{G' \sim \cG_{\GIRG}} \left [ k(G, G') \right ]\approxeq \frac{1}{m} \sum_{i=1}^m k(G, G'_i \sim \cG_\GIRG)$, computable by Monte Carlo sampling.

% Therefore the idea would be to replace the incorrect $p(G | \cG_{\text{d-}\GIRG}) = \int_\theta p(G | \theta, \cG_{\text{d-}\GIRG}) p(\theta | \cG_{\text{d-}\GIRG}) d\theta$ with $\mu(G) = E_{G' \sim \cG_{\text{d-}\GIRG}} \left [ k(G, G') \right ]$. This is an idea.

% So when we say $p(G | \theta)$, we actually mean the probability of getting a graph $G'$ which is isomorphic to $G$, given parameters $\theta$.



% TODO do we remove this weird figure - what's it doing here? 
% \begin{figure}
%     \centering

%     \begin{subfigure}{0.49\textwidth}
%       \centering
%       \includegraphics[width=\linewidth]{figures/socfb-Amherst41-1d.png}
%       \caption{$d=1$}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}{0.49\textwidth}
%       \centering
%       \includegraphics[width=\linewidth]{figures/socfb-Amherst41-2d.png}
%       \caption{$d=2$}
%     \end{subfigure}
  
%     \vspace{1em}
  
%     \begin{subfigure}{0.49\textwidth}
%       \centering
%       \includegraphics[width=\linewidth]{figures/socfb-Amherst41-3d.png}
%       \caption{$d=3$}
%     \end{subfigure}
%     \hfill
  
%     \caption{MCMC runs for socfb-Amherst41, without failure rate}
%     \label{fig:amherst_non_failure_mcmc}
% \end{figure}

% We see in \cref{fig:amherst_non_failure_mcmc} that Bayes Factor model comparison wise, 1D GIRGs are superior, even though according to edge accuracy, 2D GIRGs achieve a slight edge.


\section{Graph Kernel Introduction}
Graph Kernels provide a means for a similarity metric between graphs. They're ideally a positive semidefinite function $k: \Gamma \times \Gamma \rightarrow \mathbb{R}$, where $\Gamma$ is the set of all graphs. Such a function exists if and only if there is a corresponding feature map resentation of $\phi: \Gamma \to \cH_\phi$ where $\cH_\phi$ is a Hilbert space, and $k(G, G') = \langle \phi(G), \phi(G') \rangle_{\cH_\phi}$ is just an inner product.

Graph kernels can be contrasted with the Blasius' GGM realism framework . Blasius compares multiple different graph feature combinations on which to train an SVM for distinguishing two graph datasets.
Viewing the graph kernel as an inner product of graph feature maps, these hopefully encapsulate all the relevant information about the graph including the specifically chosen features in Blasius' framework like graph diameter, average degree, etc. The question of which chosen graph features to consider then shifts to which graph kernel to use!

% Instead we can replace this with a single kernel which hopefully encapsulates sufficient relevant information about the graphs - viewed as an inner product between graph . The question of which graph features to use then shifts to which graph kernel to use!

% Another benefit of graph kernels is, as a similarity metric, they give easier direct comparison between graphs.
% In the previous sections' proposed paradigm, we can directly compare the similarity of a real graph $G$ with two synthetic graphs $G_1, G_2$, produced from different models $M_1, M_2$, using $k(G, G_1)$ and $k(G, G_2)$.
% This is simply done by comparing $k(G, G_1)$ and $k(G, G_2)$ - the higher of the two is the more similar.


% Graph Kernels have many uses, but we can use them in particular to help with bayesian model comparison, solving our issue of graph permutations.

% A survey on graph kernels https://appliednetsci.springeropen.com/articles/10.1007/s41109-019-0195-3



% We saw the equation $\mu(G) = E_{G' \sim \cG_{\text{d-}\GIRG}} \left [ k(G, G') \right ]$ in the previous chapter. We can Monte Carlo sample this expectation to get an estimate.

% \section{Experiments with Random Walk Kernel; Weisfeiler-Lehman Kernel}
There are a range of graph kernels to choose from, however given the relatively large size of our graphs, runtime can be an issue. We ended up testing two kernels; alas both proved unsatisfactory. Without further expertise on graph kernels, we were forced to abandon this line of attack. We think that graph kernels might be more effective on smaller graphs with more unique structure (our random graph models and the real graphs themselves have a lot of homogenous structure) and particularly graphs with meangingful node labels.

\subsection{Random walk kernel}
The \textit{random walk kernel} between two graphs is just a count of the number of common walks in the pair,  of any length $l$. This is generally an infinite sum, so geometric weighting with the factor $\lambda^l$ is used to decay the contribution of longer walks, where $0 < \lambda < 1$ (we tested with $\lambda=10^{-5}$).

The product graph of $G$ and  $G'$ is defined as $G_\times = (V_\times, E_\times)$ where $V_\times = \{(v, v') | v \in V, v' \in V'\}$ and $E_\times = \{((v_1, v'_1), (v_2, v'_2)) | v_1 \sim v_2 \in E_1, v'_1 \sim v'_2 \in E_2\}$. This definition can be extended to node-labelled graphs, where we only take product vertices $(v, v') \in V_\times$ if $v \in V, v' \in V'$ have the same label. Common walks in the pair of graphs $G,G'$ are just walks in the product graph.

The naive implementation of the random walk kernel has complexity $O(n^6)$, however this can be sped up to $O(n^3)$. This is still too slow for our purposes. We only made limited tests with small graphs of $n \leq 3000$ nodes - see e.g. \cref{fig:rw_kernel_fitcopycube}.

\subsection{Weisfeiler-Lehman kernel}
The \textit{Weisfeiler-Lehman kernel} runs much faster in $O(h|E|)$ time, where $|E|$ is the number of edges in the graph and $h$ is a chosen parameter, the number of rounds of relabelling within the algorithm. The algorithm requires some starting node labels $(l(v))_{v \in V}$ - we colour nodes into a small discreet set of colours by grouping together nodes with similar sized degrees. It also uses a base kernel $k_{\text{base}}$ which is computed at each relabelling round  - we used the simplest/speediest node label histogram dot product kernel: $k(G, G') = \langle \vec{f}, \vec{f}' \rangle$ where $\vec{f} = (f_1, \dots, f_k)$, $f_i = |\{ v \in V \;:\; l(v) = i\}|$ is the number of nodes with label $i$.

The Weisfeiler-Lehman kernel is then defined as $k_{WLK}(G, G') = k_{\text{base}}(G_1, G_1') + \dots + k_{\text{base}}(G_h, G_h')$. $G=G_1,\; G' = G'_1$, and $G_{i+1}$ is computed iteratively as relabelling each node in $G_i$ with $l(v) \gets (l(v), (l(u))_{u \sim v})$. This looks odd, but in practice each label is just rehashed as a new integer rather than becoming a highly nested tuple.



\subsection{Experiments}

Unfortunately both kernels failed to pass the basic test of showing higher similarity between two graphs generated from the same GGM than from a different GGM. We tried a variety of combinations, but didn't get very reliable results: see \cref{fig:wl_kernel_gentorus} and \cref{fig:rw_kernel_fitcopycube}.

% E.g. in \cref{fig:rw_kernel_fitcopycube}, the random walk kernel faield the basic test of having $k(G \sim \cG_1, G'_1 \sim \cG_1) > k(G \sim \cG_1, G'_2 \sim \cG_2)$, where $\cG_1$ is a 3D copy weight cube GIRG, and $\cG_2$ is copy weight Chung-Lu.

\begin{figure}
  \centering

  \begin{subfigure}{0.49 \textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/d=2 alpha=1.2 n=70000 GIRG WL-Kernel with others.png}
    \caption{Weisfeiler-Lehman Kernel of a d=2, alpha=1.2, n=70000 Torus GIRG with other generated graphs (13 per model). All the GIRGs are more similar to the original than Chung-Lu, which is good, but we cannot differentiate between GIRGs of different dimensions.}
    \label{fig:wl_kernel_gentorus}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.49 \textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/socfb-Brandeis99 d=3.png}
    \caption{Random Walk Kernel of a d=3 copy weight cube GIRG fit to socfb-Brandeis99 (matching number of edges and mean LCC), with other graphs generated from different GGMs (6 per model type). We hoped for 3d GIRGs to have the highest similarity to the input graph, however instead Chung-Lu graphs matched closest.}
    \label{fig:rw_kernel_fitcopycube}
  \end{subfigure}
\end{figure}


% \begin{figure}
%   \centering
% \includegraphics[width=0.8\linewidth]{figures/d=2 alpha=1.2 n=70000 GIRG WL-Kernel with others.png}
% \caption{WL-Kernel of a d=2, alpha=1.2, n=70000 Torus GIRG with other generated graphs (13 per model). All the GIRGs are more similar to the original than Chung-Lu, but we cannot differentiate between GIRGs of different dimensions.}
% \label{fig:wl_kernel_gentorus}
% \end{figure}

% % socfb-Brandeis99 d=3.png


% \begin{figure}
%   \centering
% \includegraphics[width=0.8\linewidth]{figures/socfb-Brandeis99 d=3.png}
% \caption{RW-Kernel of a d=3 copy weight cube GIRG fit to socfb-Brandeis99 (matching number of edges and local clustering coefficient), with other generated graphs (6 per model type). Chung-Lu graphs have highest similarity to the original, despite it being a 3D GIRG}
% \label{fig:rw_kernel_fitcopycube}
% \end{figure}



% We think that 


% \subsection{Experiments}
% we do the comparison

% Cube Similarity Plots:
% We fit alpha, c for a uniform cube GIRG for dimensions d=1-3, produce a graph, and look at similarity with real graph.

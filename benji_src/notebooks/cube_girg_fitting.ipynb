{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../../nemo-eva/src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/home/bdayan/girgs/venv/lib64/python3.8/site-packages/scipy/__init__.py:143: UserWarning: A NumPy version >=1.19.5 and <1.27.0 is required for this version of SciPy (detected version 1.19.0)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "\t\t\t<script type=\"text/javascript\">\n",
       "\t\t\t<!--\n",
       "\t\t\t\t\n",
       "\t\t\t{\n",
       "\t\t\t\tvar element = document.getElementById('NetworKit_script');\n",
       "\t\t\t\tif (element) {\n",
       "\t\t\t\t\telement.parentNode.removeChild(element);\n",
       "\t\t\t\t}\n",
       "\t\t\t\telement = document.createElement('script');\n",
       "\t\t\t\telement.type = 'text/javascript';\n",
       "\t\t\t\telement.innerHTML = 'function NetworKit_pageEmbed(id) { var i, j; var elements; elements = document.getElementById(id).getElementsByClassName(\"Plot\"); for (i=0; i<elements.length; i++) { elements[i].id = id + \"_Plot_\" + i; var data = elements[i].getAttribute(\"data-image\").split(\"|\"); elements[i].removeAttribute(\"data-image\"); var content = \"<div class=\\\\\"Image\\\\\" id=\\\\\"\" + elements[i].id + \"_Image\\\\\" />\"; elements[i].innerHTML = content; elements[i].setAttribute(\"data-image-index\", 0); elements[i].setAttribute(\"data-image-length\", data.length); for (j=0; j<data.length; j++) { elements[i].setAttribute(\"data-image-\" + j, data[j]); } NetworKit_plotUpdate(elements[i]); elements[i].onclick = function (e) { NetworKit_overlayShow((e.target) ? e.target : e.srcElement); } } elements = document.getElementById(id).getElementsByClassName(\"HeatCell\"); for (i=0; i<elements.length; i++) { var data = parseFloat(elements[i].getAttribute(\"data-heat\")); var color = \"#00FF00\"; if (data <= 1 && data > 0) { color = \"hsla(0, 100%, 75%, \" + (data) + \")\"; } else if (data <= 0 && data >= -1) { color = \"hsla(240, 100%, 75%, \" + (-data) + \")\"; } elements[i].style.backgroundColor = color; } elements = document.getElementById(id).getElementsByClassName(\"Details\"); for (i=0; i<elements.length; i++) { elements[i].setAttribute(\"data-title\", \"-\"); NetworKit_toggleDetails(elements[i]); elements[i].onclick = function (e) { NetworKit_toggleDetails((e.target) ? e.target : e.srcElement); } } elements = document.getElementById(id).getElementsByClassName(\"MathValue\"); for (i=elements.length-1; i>=0; i--) { value = elements[i].innerHTML.trim(); if (value === \"nan\") { elements[i].parentNode.innerHTML = \"\" } } elements = document.getElementById(id).getElementsByClassName(\"SubCategory\"); for (i=elements.length-1; i>=0; i--) { value = elements[i].innerHTML.trim(); if (value === \"\") { elements[i].parentNode.removeChild(elements[i]) } } elements = document.getElementById(id).getElementsByClassName(\"Category\"); for (i=elements.length-1; i>=0; i--) { value = elements[i].innerHTML.trim(); if (value === \"\") { elements[i].parentNode.removeChild(elements[i]) } } var isFirefox = false; try { isFirefox = typeof InstallTrigger !== \"undefined\"; } catch (e) {} if (!isFirefox) { alert(\"Currently the function\\'s output is only fully supported by Firefox.\"); } } function NetworKit_plotUpdate(source) { var index = source.getAttribute(\"data-image-index\"); var data = source.getAttribute(\"data-image-\" + index); var image = document.getElementById(source.id + \"_Image\"); image.style.backgroundImage = \"url(\" + data + \")\"; } function NetworKit_showElement(id, show) { var element = document.getElementById(id); element.style.display = (show) ? \"block\" : \"none\"; } function NetworKit_overlayShow(source) { NetworKit_overlayUpdate(source); NetworKit_showElement(\"NetworKit_Overlay\", true); } function NetworKit_overlayUpdate(source) { document.getElementById(\"NetworKit_Overlay_Title\").innerHTML = source.title; var index = source.getAttribute(\"data-image-index\"); var data = source.getAttribute(\"data-image-\" + index); var image = document.getElementById(\"NetworKit_Overlay_Image\"); image.setAttribute(\"data-id\", source.id); image.style.backgroundImage = \"url(\" + data + \")\"; var link = document.getElementById(\"NetworKit_Overlay_Toolbar_Bottom_Save\"); link.href = data; link.download = source.title + \".svg\"; } function NetworKit_overlayImageShift(delta) { var image = document.getElementById(\"NetworKit_Overlay_Image\"); var source = document.getElementById(image.getAttribute(\"data-id\")); var index = parseInt(source.getAttribute(\"data-image-index\")); var length = parseInt(source.getAttribute(\"data-image-length\")); var index = (index+delta) % length; if (index < 0) { index = length + index; } source.setAttribute(\"data-image-index\", index); NetworKit_overlayUpdate(source); } function NetworKit_toggleDetails(source) { var childs = source.children; var show = false; if (source.getAttribute(\"data-title\") == \"-\") { source.setAttribute(\"data-title\", \"+\"); show = false; } else { source.setAttribute(\"data-title\", \"-\"); show = true; } for (i=0; i<childs.length; i++) { if (show) { childs[i].style.display = \"block\"; } else { childs[i].style.display = \"none\"; } } }';\n",
       "\t\t\t\telement.setAttribute('id', 'NetworKit_script');\n",
       "\t\t\t\tdocument.head.appendChild(element);\n",
       "\t\t\t}\n",
       "\t\t\n",
       "\t\t\t\t\n",
       "\t\t\t{\n",
       "\t\t\t\tvar element = document.getElementById('NetworKit_style');\n",
       "\t\t\t\tif (element) {\n",
       "\t\t\t\t\telement.parentNode.removeChild(element);\n",
       "\t\t\t\t}\n",
       "\t\t\t\telement = document.createElement('style');\n",
       "\t\t\t\telement.type = 'text/css';\n",
       "\t\t\t\telement.innerHTML = '.NetworKit_Page { font-family: Arial, Helvetica, sans-serif; font-size: 14px; } .NetworKit_Page .Value:before { font-family: Arial, Helvetica, sans-serif; font-size: 1.05em; content: attr(data-title) \":\"; margin-left: -2.5em; padding-right: 0.5em; } .NetworKit_Page .Details .Value:before { display: block; } .NetworKit_Page .Value { font-family: monospace; white-space: pre; padding-left: 2.5em; white-space: -moz-pre-wrap !important; white-space: -pre-wrap; white-space: -o-pre-wrap; white-space: pre-wrap; word-wrap: break-word; tab-size: 4; -moz-tab-size: 4; } .NetworKit_Page .Category { clear: both; padding-left: 1em; margin-bottom: 1.5em; } .NetworKit_Page .Category:before { content: attr(data-title); font-size: 1.75em; display: block; margin-left: -0.8em; margin-bottom: 0.5em; } .NetworKit_Page .SubCategory { margin-bottom: 1.5em; padding-left: 1em; } .NetworKit_Page .SubCategory:before { font-size: 1.6em; display: block; margin-left: -0.8em; margin-bottom: 0.5em; } .NetworKit_Page .SubCategory[data-title]:before { content: attr(data-title); } .NetworKit_Page .Block { display: block; } .NetworKit_Page .Block:after { content: \".\"; visibility: hidden; display: block; height: 0; clear: both; } .NetworKit_Page .Block .Thumbnail_Overview, .NetworKit_Page .Block .Thumbnail_ScatterPlot { width: 260px; float: left; } .NetworKit_Page .Block .Thumbnail_Overview img, .NetworKit_Page .Block .Thumbnail_ScatterPlot img { width: 260px; } .NetworKit_Page .Block .Thumbnail_Overview:before, .NetworKit_Page .Block .Thumbnail_ScatterPlot:before { display: block; text-align: center; font-weight: bold; } .NetworKit_Page .Block .Thumbnail_Overview:before { content: attr(data-title); } .NetworKit_Page .HeatCell { font-family: \"Courier New\", Courier, monospace; cursor: pointer; } .NetworKit_Page .HeatCell, .NetworKit_Page .HeatCellName { display: inline; padding: 0.1em; margin-right: 2px; background-color: #FFFFFF } .NetworKit_Page .HeatCellName { margin-left: 0.25em; } .NetworKit_Page .HeatCell:before { content: attr(data-heat); display: inline-block; color: #000000; width: 4em; text-align: center; } .NetworKit_Page .Measure { clear: both; } .NetworKit_Page .Measure .Details { cursor: pointer; } .NetworKit_Page .Measure .Details:before { content: \"[\" attr(data-title) \"]\"; display: block; } .NetworKit_Page .Measure .Details .Value { border-left: 1px dotted black; margin-left: 0.4em; padding-left: 3.5em; pointer-events: none; } .NetworKit_Page .Measure .Details .Spacer:before { content: \".\"; opacity: 0.0; pointer-events: none; } .NetworKit_Page .Measure .Plot { width: 440px; height: 440px; cursor: pointer; float: left; margin-left: -0.9em; margin-right: 20px; } .NetworKit_Page .Measure .Plot .Image { background-repeat: no-repeat; background-position: center center; background-size: contain; height: 100%; pointer-events: none; } .NetworKit_Page .Measure .Stat { width: 500px; float: left; } .NetworKit_Page .Measure .Stat .Group { padding-left: 1.25em; margin-bottom: 0.75em; } .NetworKit_Page .Measure .Stat .Group .Title { font-size: 1.1em; display: block; margin-bottom: 0.3em; margin-left: -0.75em; border-right-style: dotted; border-right-width: 1px; border-bottom-style: dotted; border-bottom-width: 1px; background-color: #D0D0D0; padding-left: 0.2em; } .NetworKit_Page .Measure .Stat .Group .List { -webkit-column-count: 3; -moz-column-count: 3; column-count: 3; } .NetworKit_Page .Measure .Stat .Group .List .Entry { position: relative; line-height: 1.75em; } .NetworKit_Page .Measure .Stat .Group .List .Entry[data-tooltip]:before { position: absolute; left: 0; top: -40px; background-color: #808080; color: #ffffff; height: 30px; line-height: 30px; border-radius: 5px; padding: 0 15px; content: attr(data-tooltip); white-space: nowrap; display: none; } .NetworKit_Page .Measure .Stat .Group .List .Entry[data-tooltip]:after { position: absolute; left: 15px; top: -10px; border-top: 7px solid #808080; border-left: 7px solid transparent; border-right: 7px solid transparent; content: \"\"; display: none; } .NetworKit_Page .Measure .Stat .Group .List .Entry[data-tooltip]:hover:after, .NetworKit_Page .Measure .Stat .Group .List .Entry[data-tooltip]:hover:before { display: block; } .NetworKit_Page .Measure .Stat .Group .List .Entry .MathValue { font-family: \"Courier New\", Courier, monospace; } .NetworKit_Page .Measure:after { content: \".\"; visibility: hidden; display: block; height: 0; clear: both; } .NetworKit_Page .PartitionPie { clear: both; } .NetworKit_Page .PartitionPie img { width: 600px; } #NetworKit_Overlay { left: 0px; top: 0px; display: none; position: absolute; width: 100%; height: 100%; background-color: rgba(0,0,0,0.6); z-index: 1000; } #NetworKit_Overlay_Title { position: absolute; color: white; transform: rotate(-90deg); width: 32em; height: 32em; padding-right: 0.5em; padding-top: 0.5em; text-align: right; font-size: 40px; } #NetworKit_Overlay .button { background: white; cursor: pointer; } #NetworKit_Overlay .button:before { size: 13px; display: inline-block; text-align: center; margin-top: 0.5em; margin-bottom: 0.5em; width: 1.5em; height: 1.5em; } #NetworKit_Overlay .icon-close:before { content: \"X\"; } #NetworKit_Overlay .icon-previous:before { content: \"P\"; } #NetworKit_Overlay .icon-next:before { content: \"N\"; } #NetworKit_Overlay .icon-save:before { content: \"S\"; } #NetworKit_Overlay_Toolbar_Top, #NetworKit_Overlay_Toolbar_Bottom { position: absolute; width: 40px; right: 13px; text-align: right; z-index: 1100; } #NetworKit_Overlay_Toolbar_Top { top: 0.5em; } #NetworKit_Overlay_Toolbar_Bottom { Bottom: 0.5em; } #NetworKit_Overlay_ImageContainer { position: absolute; top: 5%; left: 5%; height: 90%; width: 90%; background-repeat: no-repeat; background-position: center center; background-size: contain; } #NetworKit_Overlay_Image { height: 100%; width: 100%; background-repeat: no-repeat; background-position: center center; background-size: contain; }';\n",
       "\t\t\t\telement.setAttribute('id', 'NetworKit_style');\n",
       "\t\t\t\tdocument.head.appendChild(element);\n",
       "\t\t\t}\n",
       "\t\t\n",
       "\t\t\t\t\n",
       "\t\t\t{\n",
       "\t\t\t\tvar element = document.getElementById('NetworKit_Overlay');\n",
       "\t\t\t\tif (element) {\n",
       "\t\t\t\t\telement.parentNode.removeChild(element);\n",
       "\t\t\t\t}\n",
       "\t\t\t\telement = document.createElement('div');\n",
       "\t\t\t\telement.innerHTML = '<div id=\"NetworKit_Overlay_Toolbar_Top\"><div class=\"button icon-close\" id=\"NetworKit_Overlay_Close\" /></div><div id=\"NetworKit_Overlay_Title\" /> <div id=\"NetworKit_Overlay_ImageContainer\"> <div id=\"NetworKit_Overlay_Image\" /> </div> <div id=\"NetworKit_Overlay_Toolbar_Bottom\"> <div class=\"button icon-previous\" onclick=\"NetworKit_overlayImageShift(-1)\" /> <div class=\"button icon-next\" onclick=\"NetworKit_overlayImageShift(1)\" /> <a id=\"NetworKit_Overlay_Toolbar_Bottom_Save\"><div class=\"button icon-save\" /></a> </div>';\n",
       "\t\t\t\telement.setAttribute('id', 'NetworKit_Overlay');\n",
       "\t\t\t\tdocument.body.appendChild(element);\n",
       "\t\t\t\tdocument.getElementById('NetworKit_Overlay_Close').onclick = function (e) {\n",
       "\t\t\t\t\tdocument.getElementById('NetworKit_Overlay').style.display = 'none';\n",
       "\t\t\t\t}\n",
       "\t\t\t}\n",
       "\t\t\n",
       "\t\t\t-->\n",
       "\t\t\t</script>\n",
       "\t\t"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import networkit as nk\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "\n",
    "from benji_girgs import generation, utils, plotting, fitting\n",
    "import seaborn as sns\n",
    "\n",
    "import feature_extractor\n",
    "from scipy import optimize\n",
    "from girg_sampling import girgs\n",
    "\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'benji_girgs.generation' from '../benji_girgs/generation.py'>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<module 'benji_girgs.fitting' from '../benji_girgs/fitting.py'>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<module 'feature_extractor' from '../../nemo-eva/src/feature_extractor.py'>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(generation)\n",
    "importlib.reload(fitting)\n",
    "importlib.reload(feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "d = 2\n",
    "tau=2.1\n",
    "alpha=1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = generation.cgirg_gen_cube(n, d, tau, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = nk.readGraph('/cluster/scratch/bdayan/GIRG_data/bn-fly-drosophila_medulla_1.SpaceOne', nk.Format.EdgeListSpaceOne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no results_df\n"
     ]
    }
   ],
   "source": [
    "fe = feature_extractor.FeatureExtractor([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "\n",
      "b\n"
     ]
    }
   ],
   "source": [
    "print('a')\n",
    "print()\n",
    "print('b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.removeSelfLoops()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Values less than or equal to 0 in data. Throwing out 0 or negative values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calling goal_f(alpha=100.0)\n",
      "starting search at const=1.981951904296875; depth=6: For alpha=100.0\n",
      "a: 0.9909759521484375, m: 1.981951904296875, b: 3.96390380859375, f_a: 5.6814856499718625, f_m: 11.16826111423748, f_b: 21.846933033202024, goal: 9.76702307259426\n",
      "a: 0.9909759521484375, m: 1.4864639282226562, b: 1.981951904296875, f_a: 5.6814856499718625, f_m: 8.489589195272933, f_b: 11.16826111423748, goal: 9.76702307259426\n",
      "a: 1.4864639282226562, m: 1.7342079162597654, b: 1.981951904296875, f_a: 8.489589195272933, f_m: 10.043894203714125, f_b: 11.16826111423748, goal: 9.76702307259426\n",
      "a: 1.4864639282226562, m: 1.6103359222412108, b: 1.7342079162597654, f_a: 8.489589195272933, f_m: 9.19189645469893, f_b: 10.043894203714125, goal: 9.76702307259426\n",
      "a: 1.6103359222412108, m: 1.672271919250488, b: 1.7342079162597654, f_a: 9.19189645469893, f_m: 9.502532357906585, f_b: 10.043894203714125, goal: 9.76702307259426\n",
      "calling goal_f(alpha=2.0)\n",
      "starting search at const=0.975; depth=6: For alpha=2.0\n",
      "a: 0.4875, m: 0.975, b: 1.95, f_a: 5.567810917276308, f_m: 11.113111986494092, f_b: 21.308947664603263, goal: 9.76702307259426\n",
      "a: 0.4875, m: 0.73125, b: 0.975, f_a: 5.567810917276308, f_m: 8.420934158694429, f_b: 11.113111986494092, goal: 9.76702307259426\n",
      "a: 0.73125, m: 0.8531249999999999, b: 0.975, f_a: 8.420934158694429, f_m: 9.698368036015756, f_b: 11.113111986494092, goal: 9.76702307259426\n",
      "a: 0.8531249999999999, m: 0.9140625, b: 0.975, f_a: 9.698368036015756, f_m: 10.226223972988182, f_b: 11.113111986494092, goal: 9.76702307259426\n",
      "a: 0.8531249999999999, m: 0.88359375, b: 0.9140625, f_a: 9.698368036015756, f_m: 9.966235227912211, f_b: 10.226223972988182, goal: 9.76702307259426\n",
      "calling goal_f(alpha=1.0101010101010102)\n",
      "starting search at const=0.24648751757641893; depth=6: For alpha=1.0101010101010102\n",
      "a: 0.12324375878820946, m: 0.24648751757641893, b: 0.49297503515283786, f_a: 5.387732132808104, f_m: 9.882948790095666, f_b: 17.844682048396173, goal: 9.76702307259426\n",
      "a: 0.12324375878820946, m: 0.1848656381823142, b: 0.24648751757641893, f_a: 5.387732132808104, f_m: 7.710748452447946, f_b: 9.882948790095666, goal: 9.76702307259426\n",
      "a: 0.1848656381823142, m: 0.21567657787936656, b: 0.24648751757641893, f_a: 7.710748452447946, f_m: 8.951041080472708, f_b: 9.882948790095666, goal: 9.76702307259426\n",
      "a: 0.21567657787936656, m: 0.23108204772789276, b: 0.24648751757641893, f_a: 8.951041080472708, f_m: 9.20990433314575, f_b: 9.882948790095666, goal: 9.76702307259426\n",
      "a: 0.23108204772789276, m: 0.23878478265215586, b: 0.24648751757641893, f_a: 9.20990433314575, f_m: 9.600450196961171, f_b: 9.882948790095666, goal: 9.76702307259426\n",
      "\n",
      "a: 0.01, m: 0.5, b: 0.99, f_a: 0.6179221853571496, f_m: 0.4549985778658486, f_b: 0.12383967994082769, goal: 0.25789878516108755\n",
      "calling goal_f(alpha=1.342281879194631)\n",
      "starting search at const=0.5484671654459358; depth=6: For alpha=1.342281879194631\n",
      "a: 0.2742335827229679, m: 0.5484671654459358, b: 1.0969343308918715, f_a: 5.478897017445132, f_m: 10.652785593697242, f_b: 19.765897580191332, goal: 9.76702307259426\n",
      "a: 0.2742335827229679, m: 0.41135037408445185, b: 0.5484671654459358, f_a: 5.478897017445132, f_m: 8.037141249296567, f_b: 10.652785593697242, goal: 9.76702307259426\n",
      "a: 0.41135037408445185, m: 0.4799087697651938, b: 0.5484671654459358, f_a: 8.037141249296567, f_m: 9.220033764772088, f_b: 10.652785593697242, goal: 9.76702307259426\n",
      "a: 0.4799087697651938, m: 0.5141879676055647, b: 0.5484671654459358, f_a: 9.220033764772088, f_m: 9.869442881260552, f_b: 10.652785593697242, goal: 9.76702307259426\n",
      "a: 0.4799087697651938, m: 0.49704836868537927, b: 0.5141879676055647, f_a: 9.220033764772088, f_m: 9.701744513224536, f_b: 9.869442881260552, goal: 9.76702307259426\n",
      "\n",
      "a: 0.5, m: 0.745, b: 0.99, f_a: 0.4549985778658486, f_m: 0.2722674907752117, f_b: 0.12383967994082769, goal: 0.25789878516108755\n",
      "calling goal_f(alpha=1.1527377521613833)\n",
      "starting search at const=0.3799570944221709; depth=6: For alpha=1.1527377521613833\n",
      "a: 0.18997854721108545, m: 0.3799570944221709, b: 0.7599141888443418, f_a: 5.558806978052898, f_m: 10.142937535171638, f_b: 18.936409679234664, goal: 9.76702307259426\n",
      "a: 0.18997854721108545, m: 0.2849678208166282, b: 0.3799570944221709, f_a: 5.558806978052898, f_m: 8.142937535171638, f_b: 10.142937535171638, goal: 9.76702307259426\n",
      "a: 0.2849678208166282, m: 0.3324624576193995, b: 0.3799570944221709, f_a: 8.142937535171638, f_m: 9.386606640405176, f_b: 10.142937535171638, goal: 9.76702307259426\n",
      "a: 0.3324624576193995, m: 0.3562097760207852, b: 0.3799570944221709, f_a: 9.386606640405176, f_m: 9.54305008441193, f_b: 10.142937535171638, goal: 9.76702307259426\n",
      "a: 0.3562097760207852, m: 0.368083435221478, b: 0.3799570944221709, f_a: 9.54305008441193, f_m: 9.92459200900394, f_b: 10.142937535171638, goal: 9.76702307259426\n",
      "\n",
      "a: 0.745, m: 0.8674999999999999, b: 0.99, f_a: 0.2722674907752117, f_m: 0.18113559800473175, f_b: 0.12383967994082769, goal: 0.25789878516108755\n",
      "calling goal_f(alpha=1.24031007751938)\n",
      "starting search at const=0.4555341299959639; depth=6: For alpha=1.24031007751938\n",
      "a: 0.22776706499798194, m: 0.4555341299959639, b: 0.9110682599919278, f_a: 5.379853685987619, f_m: 10.459200900393922, f_b: 18.916150815981993, goal: 9.76702307259426\n",
      "a: 0.22776706499798194, m: 0.3416505974969729, b: 0.4555341299959639, f_a: 5.379853685987619, f_m: 7.930219471018571, f_b: 10.459200900393922, goal: 9.76702307259426\n",
      "a: 0.3416505974969729, m: 0.3985923637464684, b: 0.4555341299959639, f_a: 7.930219471018571, f_m: 9.275182892515476, f_b: 10.459200900393922, goal: 9.76702307259426\n",
      "a: 0.3985923637464684, m: 0.4270632468712161, b: 0.4555341299959639, f_a: 9.275182892515476, f_m: 9.646595385481149, f_b: 10.459200900393922, goal: 9.76702307259426\n",
      "a: 0.4270632468712161, m: 0.44129868843359, b: 0.4555341299959639, f_a: 9.646595385481149, f_m: 10.26336522228475, f_b: 10.459200900393922, goal: 9.76702307259426\n",
      "\n",
      "a: 0.745, m: 0.8062499999999999, b: 0.8674999999999999, f_a: 0.2722674907752117, f_m: 0.2252725900864235, f_b: 0.18113559800473175, goal: 0.25789878516108755\n",
      "calling goal_f(alpha=1.2892828364222402)\n",
      "starting search at const=0.5032667004852589; depth=6: For alpha=1.2892828364222402\n",
      "a: 0.25163335024262945, m: 0.5032667004852589, b: 1.0065334009705178, f_a: 5.430500844119302, f_m: 10.545863815419246, f_b: 19.504783342712436, goal: 9.76702307259426\n",
      "a: 0.25163335024262945, m: 0.3774500253639442, b: 0.5032667004852589, f_a: 5.430500844119302, f_m: 7.968486212718064, f_b: 10.545863815419246, goal: 9.76702307259426\n",
      "a: 0.3774500253639442, m: 0.4403583629246015, b: 0.5032667004852589, f_a: 7.968486212718064, f_m: 9.402363534046145, f_b: 10.545863815419246, goal: 9.76702307259426\n",
      "a: 0.4403583629246015, m: 0.4718125317049302, b: 0.5032667004852589, f_a: 9.402363534046145, f_m: 9.785030951041081, f_b: 10.545863815419246, goal: 9.76702307259426\n",
      "a: 0.4403583629246015, m: 0.45608544731476586, b: 0.4718125317049302, f_a: 9.402363534046145, f_m: 9.684862127180642, f_b: 9.785030951041081, goal: 9.76702307259426\n",
      "\n",
      "a: 0.745, m: 0.775625, b: 0.8062499999999999, f_a: 0.2722674907752117, f_m: 0.24471276016329052, f_b: 0.2252725900864235, goal: 0.25789878516108755\n",
      "calling goal_f(alpha=1.3152486642005756)\n",
      "starting search at const=0.5235879597821937; depth=6: For alpha=1.3152486642005756\n",
      "a: 0.2617939798910969, m: 0.5235879597821937, b: 1.0471759195643875, f_a: 5.586944288126055, f_m: 10.401800787844682, f_b: 19.621834552616768, goal: 9.76702307259426\n",
      "a: 0.2617939798910969, m: 0.3926909698366453, b: 0.5235879597821937, f_a: 5.586944288126055, f_m: 7.972988182329769, f_b: 10.401800787844682, goal: 9.76702307259426\n",
      "a: 0.3926909698366453, m: 0.4581394648094195, b: 0.5235879597821937, f_a: 7.972988182329769, f_m: 9.105233539673607, f_b: 10.401800787844682, goal: 9.76702307259426\n",
      "a: 0.4581394648094195, m: 0.4908637122958066, b: 0.5235879597821937, f_a: 9.105233539673607, f_m: 9.841305571187394, f_b: 10.401800787844682, goal: 9.76702307259426\n",
      "a: 0.4581394648094195, m: 0.47450158855261304, b: 0.4908637122958066, f_a: 9.105233539673607, f_m: 9.480022509848059, f_b: 9.841305571187394, goal: 9.76702307259426\n",
      "\n",
      "a: 0.745, m: 0.7603125, b: 0.775625, f_a: 0.2722674907752117, f_m: 0.2658338931571939, f_b: 0.24471276016329052, goal: 0.25789878516108755\n",
      "calling goal_f(alpha=1.3021363173957274)\n",
      "starting search at const=0.5122641321465815; depth=6: For alpha=1.3021363173957274\n",
      "a: 0.25613206607329075, m: 0.5122641321465815, b: 1.024528264293163, f_a: 5.376477208778841, f_m: 10.498593134496343, f_b: 19.875070343275183, goal: 9.76702307259426\n",
      "a: 0.25613206607329075, m: 0.38419809910993613, b: 0.5122641321465815, f_a: 5.376477208778841, f_m: 7.9898705683736635, f_b: 10.498593134496343, goal: 9.76702307259426\n",
      "a: 0.38419809910993613, m: 0.4482311156282588, b: 0.5122641321465815, f_a: 7.9898705683736635, f_m: 9.29206527855937, f_b: 10.498593134496343, goal: 9.76702307259426\n",
      "a: 0.4482311156282588, m: 0.48024762388742015, b: 0.5122641321465815, f_a: 9.29206527855937, f_m: 9.886325267304446, f_b: 10.498593134496343, goal: 9.76702307259426\n",
      "a: 0.4482311156282588, m: 0.46423936975783947, b: 0.48024762388742015, f_a: 9.29206527855937, f_m: 9.622960045019695, f_b: 9.886325267304446, goal: 9.76702307259426\n",
      "\n",
      "a: 0.7603125, m: 0.76796875, b: 0.775625, f_a: 0.2658338931571939, f_m: 0.2417617534921328, f_b: 0.24471276016329052, goal: 0.25789878516108755\n",
      "calling goal_f(alpha=1.3086596462529394)\n",
      "starting search at const=0.5190880705016792; depth=6: For alpha=1.3086596462529394\n",
      "a: 0.2595440352508396, m: 0.5190880705016792, b: 1.0381761410033583, f_a: 5.536297129994373, f_m: 10.569499155880697, f_b: 19.592571750140685, goal: 9.76702307259426\n",
      "a: 0.2595440352508396, m: 0.38931605287625937, b: 0.5190880705016792, f_a: 5.536297129994373, f_m: 8.047270680922903, f_b: 10.569499155880697, goal: 9.76702307259426\n",
      "a: 0.38931605287625937, m: 0.45420206168896926, b: 0.5190880705016792, f_a: 8.047270680922903, f_m: 9.220033764772088, f_b: 10.569499155880697, goal: 9.76702307259426\n",
      "a: 0.45420206168896926, m: 0.4866450660953242, b: 0.5190880705016792, f_a: 9.220033764772088, f_m: 10.011254924029263, f_b: 10.569499155880697, goal: 9.76702307259426\n",
      "a: 0.45420206168896926, m: 0.47042356389214673, b: 0.4866450660953242, f_a: 9.220033764772088, f_m: 9.427124366910522, f_b: 10.011254924029263, goal: 9.76702307259426\n",
      "\n",
      "a: 0.7603125, m: 0.764140625, b: 0.76796875, f_a: 0.2658338931571939, f_m: 0.25209372716849954, f_b: 0.2417617534921328, goal: 0.25789878516108755\n",
      "calling goal_f(alpha=1.311945882232358)\n",
      "starting search at const=0.5178585261617651; depth=6: For alpha=1.311945882232358\n",
      "a: 0.25892926308088254, m: 0.5178585261617651, b: 1.0357170523235302, f_a: 5.565559932470456, f_m: 10.449071468767587, f_b: 19.518289251547554, goal: 9.76702307259426\n",
      "a: 0.25892926308088254, m: 0.3883938946213238, b: 0.5178585261617651, f_a: 5.565559932470456, f_m: 7.956105796285875, f_b: 10.449071468767587, goal: 9.76702307259426\n",
      "a: 0.3883938946213238, m: 0.45312621039154444, b: 0.5178585261617651, f_a: 7.956105796285875, f_m: 9.278559369724254, f_b: 10.449071468767587, goal: 9.76702307259426\n",
      "a: 0.45312621039154444, m: 0.48549236827665476, b: 0.5178585261617651, f_a: 9.278559369724254, f_m: 10.090039392234102, f_b: 10.449071468767587, goal: 9.76702307259426\n",
      "a: 0.45312621039154444, m: 0.4693092893340996, b: 0.48549236827665476, f_a: 9.278559369724254, f_m: 9.483398987056837, f_b: 10.090039392234102, goal: 9.76702307259426\n",
      "\n",
      "a: 0.7603125, m: 0.7622265625, b: 0.764140625, f_a: 0.2658338931571939, f_m: 0.25773425479034406, f_b: 0.25209372716849954, goal: 0.25789878516108755\n",
      "calling goal_f(alpha=1.3135951971675603)\n",
      "starting search at const=0.5207250556569002; depth=6: For alpha=1.3135951971675603\n",
      "a: 0.2603625278284501, m: 0.5207250556569002, b: 1.0414501113138004, f_a: 5.429375351716376, f_m: 10.42656162070906, f_b: 19.91783905458638, goal: 9.76702307259426\n",
      "a: 0.2603625278284501, m: 0.3905437917426752, b: 0.5207250556569002, f_a: 5.429375351716376, f_m: 8.343275182892516, f_b: 10.42656162070906, goal: 9.76702307259426\n",
      "a: 0.3905437917426752, m: 0.4556344236997877, b: 0.5207250556569002, f_a: 8.343275182892516, f_m: 9.335959482273495, f_b: 10.42656162070906, goal: 9.76702307259426\n",
      "a: 0.4556344236997877, m: 0.48817973967834394, b: 0.5207250556569002, f_a: 9.335959482273495, f_m: 9.72425436128306, f_b: 10.42656162070906, goal: 9.76702307259426\n",
      "a: 0.48817973967834394, m: 0.5044523976676221, b: 0.5207250556569002, f_a: 9.72425436128306, f_m: 10.257737760270118, f_b: 10.42656162070906, goal: 9.76702307259426\n",
      "\n",
      "a: 0.7603125, m: 0.76126953125, b: 0.7622265625, f_a: 0.2658338931571939, f_m: 0.25549572091038947, f_b: 0.25773425479034406, goal: 0.25789878516108755\n",
      "calling goal_f(alpha=1.3144214106925103)\n",
      "starting search at const=0.5151932972052481; depth=6: For alpha=1.3144214106925103\n",
      "a: 0.25759664860262405, m: 0.5151932972052481, b: 1.0303865944104962, f_a: 5.490151941474395, f_m: 10.254361283061339, f_b: 19.75801913337085, goal: 9.76702307259426\n",
      "a: 0.25759664860262405, m: 0.3863949729039361, b: 0.5151932972052481, f_a: 5.490151941474395, f_m: 7.918964546989308, f_b: 10.254361283061339, goal: 9.76702307259426\n",
      "a: 0.3863949729039361, m: 0.4507941350545921, b: 0.5151932972052481, f_a: 7.918964546989308, f_m: 9.066966797974114, f_b: 10.254361283061339, goal: 9.76702307259426\n",
      "a: 0.4507941350545921, m: 0.4829937161299201, b: 0.5151932972052481, f_a: 9.066966797974114, f_m: 9.931344963421497, f_b: 10.254361283061339, goal: 9.76702307259426\n",
      "a: 0.4507941350545921, m: 0.46689392559225606, b: 0.4829937161299201, f_a: 9.066966797974114, f_m: 9.620709060213844, f_b: 9.931344963421497, goal: 9.76702307259426\n",
      "\n",
      "a: 0.7603125, m: 0.760791015625, b: 0.76126953125, f_a: 0.2658338931571939, f_m: 0.25411918810395934, f_b: 0.25549572091038947, goal: 0.25789878516108755\n",
      "starting search at const=0.5207250556569002; depth=6: For alpha=1.3135951971675603\n",
      "a: 0.2603625278284501, m: 0.5207250556569002, b: 1.0414501113138004, f_a: 5.459763646595386, f_m: 10.559369724254362, f_b: 19.680360157568938, goal: 9.76702307259426\n",
      "a: 0.2603625278284501, m: 0.3905437917426752, b: 0.5207250556569002, f_a: 5.459763646595386, f_m: 8.077658975801913, f_b: 10.559369724254362, goal: 9.76702307259426\n",
      "a: 0.3905437917426752, m: 0.4556344236997877, b: 0.5207250556569002, f_a: 8.077658975801913, f_m: 9.34608891389983, f_b: 10.559369724254362, goal: 9.76702307259426\n",
      "a: 0.4556344236997877, m: 0.48817973967834394, b: 0.5207250556569002, f_a: 9.34608891389983, f_m: 10.045019696117052, f_b: 10.559369724254362, goal: 9.76702307259426\n",
      "a: 0.4556344236997877, m: 0.4719070816890658, b: 0.48817973967834394, f_a: 9.34608891389983, f_m: 9.685987619583567, f_b: 10.045019696117052, goal: 9.76702307259426\n"
     ]
    }
   ],
   "source": [
    "info, g_out = fe.fit_ndgirg_general(d, utils.LCC, cube=True, verbose=True)(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tau=5.059543451012543|alpha=1.0101010101010102|const=0.1502412941108109|target_lcc=0.011850175241937459|fit_lcc=0.0717709556587886|fitting_time=17.537780284881592|hist=[(2.0, 0.41907359307755165), (2.0, 0.41907359307755165), (2.0, 0.41907359307755165), (2.0, 0.41907359307755165), (2.0, 0.41907359307755165), (2.0, 0.41907359307755165), (2.0, 0.41907359307755165), (2.0, 0.41907359307755165), (2.0, 0.41907359307755165), (2.0, 0.41907359307755165), (1.0101010101010102, 0.07196589518661692)]'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network Properties:\n",
      "nodes, edges\t\t\t7393, 25914\n",
      "directed?\t\t\tFalse\n",
      "weighted?\t\t\tFalse\n",
      "isolated nodes\t\t\t16\n",
      "self-loops\t\t\t0\n",
      "density\t\t\t\t0.000948\n",
      "clustering coefficient\t\t0.073943\n",
      "min/max/avg degree\t\t0, 38, 7.010415\n",
      "degree assortativity\t\t0.087919\n",
      "number of connected components\t17\n",
      "size of largest component\t7377 (99.78 %)\n"
     ]
    }
   ],
   "source": [
    "nk.overview(g_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = g.numberOfNodes()\n",
    "tau = utils.powerlaw_fit_graph(g)\n",
    "temp = generation.cgirg_gen_cube(n, d, tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting search at const=3.858566284179687; depth=6\n",
      "a: 1.9292831420898435, m: 3.858566284179687, b: 7.717132568359374, f_a: 17.736, f_m: 31.036, f_b: 59.112, goal: 29.338\n",
      "a: 1.9292831420898435, m: 2.893924713134765, b: 3.858566284179687, f_a: 17.736, f_m: 25.096, f_b: 31.036, goal: 29.338\n",
      "a: 2.893924713134765, m: 3.376245498657226, b: 3.858566284179687, f_a: 25.096, f_m: 30.74, f_b: 31.036, goal: 29.338\n",
      "a: 2.893924713134765, m: 3.1350851058959956, b: 3.376245498657226, f_a: 25.096, f_m: 27.338, f_b: 30.74, goal: 29.338\n",
      "a: 3.1350851058959956, m: 3.255665302276611, b: 3.376245498657226, f_a: 27.338, f_m: 29.02, f_b: 30.74, goal: 29.338\n",
      "starting search at const=1.8540917587864953; depth=6\n",
      "a: 0.9270458793932477, m: 1.8540917587864953, b: 3.7081835175729907, f_a: 17.676, f_m: 32.072, f_b: 56.16, goal: 29.338\n",
      "a: 0.9270458793932477, m: 1.3905688190898715, b: 1.8540917587864953, f_a: 17.676, f_m: 25.97, f_b: 32.072, goal: 29.338\n",
      "a: 1.3905688190898715, m: 1.6223302889381834, b: 1.8540917587864953, f_a: 25.97, f_m: 28.472, f_b: 32.072, goal: 29.338\n",
      "a: 1.6223302889381834, m: 1.7382110238623394, b: 1.8540917587864953, f_a: 28.472, f_m: 30.244, f_b: 32.072, goal: 29.338\n",
      "a: 1.6223302889381834, m: 1.6802706564002614, b: 1.7382110238623394, f_a: 28.472, f_m: 31.074, f_b: 30.244, goal: 29.338\n",
      "starting search at const=0.7321222311803606; depth=6\n",
      "a: 0.3660611155901803, m: 0.7321222311803606, b: 1.4642444623607211, f_a: 18.342, f_m: 30.91, f_b: 50.564, goal: 29.338\n",
      "a: 0.3660611155901803, m: 0.5490916733852704, b: 0.7321222311803606, f_a: 18.342, f_m: 25.406, f_b: 30.91, goal: 29.338\n",
      "a: 0.5490916733852704, m: 0.6406069522828155, b: 0.7321222311803606, f_a: 25.406, f_m: 28.276, f_b: 30.91, goal: 29.338\n",
      "a: 0.6406069522828155, m: 0.686364591731588, b: 0.7321222311803606, f_a: 28.276, f_m: 28.912, f_b: 30.91, goal: 29.338\n",
      "a: 0.686364591731588, m: 0.7092434114559742, b: 0.7321222311803606, f_a: 28.912, f_m: 30.874, f_b: 30.91, goal: 29.338\n",
      "a: 0.01, m: 0.5, b: 0.99, f_a: 0.7455489978275746, f_m: 0.6270398173574556, f_b: 0.3743820821260001, goal: 0.6261814562507397\n",
      "starting search at const=1.2025717354321508; depth=6\n",
      "a: 0.6012858677160754, m: 1.2025717354321508, b: 2.4051434708643016, f_a: 18.012, f_m: 31.43, f_b: 54.466, goal: 29.338\n",
      "a: 0.6012858677160754, m: 0.9019288015741131, b: 1.2025717354321508, f_a: 18.012, f_m: 24.726, f_b: 31.43, goal: 29.338\n",
      "a: 0.9019288015741131, m: 1.052250268503132, b: 1.2025717354321508, f_a: 24.726, f_m: 28.196, f_b: 31.43, goal: 29.338\n",
      "a: 1.052250268503132, m: 1.1274110019676415, b: 1.2025717354321508, f_a: 28.196, f_m: 29.884, f_b: 31.43, goal: 29.338\n",
      "a: 1.052250268503132, m: 1.0898306352353868, b: 1.1274110019676415, f_a: 28.196, f_m: 30.004, f_b: 29.884, goal: 29.338\n",
      "a: 0.5, m: 0.745, b: 0.99, f_a: 0.6270398173574556, f_m: 0.4580820839721488, f_b: 0.3743820821260001, goal: 0.6261814562507397\n",
      "starting search at const=1.548482497105705; depth=6\n",
      "a: 0.7742412485528525, m: 1.548482497105705, b: 3.09696499421141, f_a: 18.362, f_m: 31.6, f_b: 53.802, goal: 29.338\n",
      "a: 0.7742412485528525, m: 1.1613618728292787, b: 1.548482497105705, f_a: 18.362, f_m: 24.496, f_b: 31.6, goal: 29.338\n",
      "a: 1.1613618728292787, m: 1.3549221849674917, b: 1.548482497105705, f_a: 24.496, f_m: 29.134, f_b: 31.6, goal: 29.338\n",
      "a: 1.3549221849674917, m: 1.4517023410365983, b: 1.548482497105705, f_a: 29.134, f_m: 30.122, f_b: 31.6, goal: 29.338\n",
      "a: 1.3549221849674917, m: 1.4033122630020451, b: 1.4517023410365983, f_a: 29.134, f_m: 29.526, f_b: 30.122, goal: 29.338\n",
      "a: 0.5, m: 0.6225, b: 0.745, f_a: 0.6270398173574556, f_m: 0.5745113184158738, f_b: 0.4580820839721488, goal: 0.6261814562507397\n",
      "starting search at const=1.6872847894112437; depth=6\n",
      "a: 0.8436423947056219, m: 1.6872847894112437, b: 3.3745695788224874, f_a: 17.618, f_m: 33.134, f_b: 55.524, goal: 29.338\n",
      "a: 0.8436423947056219, m: 1.2654635920584327, b: 1.6872847894112437, f_a: 17.618, f_m: 24.828, f_b: 33.134, goal: 29.338\n",
      "a: 1.2654635920584327, m: 1.4763741907348382, b: 1.6872847894112437, f_a: 24.828, f_m: 28.45, f_b: 33.134, goal: 29.338\n",
      "a: 1.4763741907348382, m: 1.581829490073041, b: 1.6872847894112437, f_a: 28.45, f_m: 30.15, f_b: 33.134, goal: 29.338\n",
      "a: 1.4763741907348382, m: 1.5291018404039396, b: 1.581829490073041, f_a: 28.45, f_m: 30.116, f_b: 30.15, goal: 29.338\n",
      "a: 0.5, m: 0.56125, b: 0.6225, f_a: 0.6270398173574556, f_m: 0.6077260481311407, f_b: 0.5745113184158738, goal: 0.6261814562507397\n",
      "starting search at const=1.8297919462690422; depth=6\n",
      "a: 0.9148959731345211, m: 1.8297919462690422, b: 3.6595838925380844, f_a: 18.116, f_m: 31.786, f_b: 51.684, goal: 29.338\n",
      "a: 0.9148959731345211, m: 1.3723439597017817, b: 1.8297919462690422, f_a: 18.116, f_m: 25.01, f_b: 31.786, goal: 29.338\n",
      "a: 1.3723439597017817, m: 1.601067952985412, b: 1.8297919462690422, f_a: 25.01, f_m: 27.75, f_b: 31.786, goal: 29.338\n",
      "a: 1.601067952985412, m: 1.715429949627227, b: 1.8297919462690422, f_a: 27.75, f_m: 28.04, f_b: 31.786, goal: 29.338\n",
      "a: 1.715429949627227, m: 1.7726109479481345, b: 1.8297919462690422, f_a: 28.04, f_m: 30.562, f_b: 31.786, goal: 29.338\n",
      "a: 0.5, m: 0.530625, b: 0.56125, f_a: 0.6270398173574556, f_m: 0.6171033764737228, f_b: 0.6077260481311407, goal: 0.6261814562507397\n",
      "starting search at const=1.8872087670488782; depth=6\n",
      "a: 0.9436043835244391, m: 1.8872087670488782, b: 3.7744175340977564, f_a: 17.64, f_m: 32.376, f_b: 58.758, goal: 29.338\n",
      "a: 0.9436043835244391, m: 1.4154065752866587, b: 1.8872087670488782, f_a: 17.64, f_m: 25.74, f_b: 32.376, goal: 29.338\n",
      "a: 1.4154065752866587, m: 1.6513076711677686, b: 1.8872087670488782, f_a: 25.74, f_m: 29.174, f_b: 32.376, goal: 29.338\n",
      "a: 1.6513076711677686, m: 1.7692582191083233, b: 1.8872087670488782, f_a: 29.174, f_m: 30.268, f_b: 32.376, goal: 29.338\n",
      "a: 1.6513076711677686, m: 1.710282945138046, b: 1.7692582191083233, f_a: 29.174, f_m: 29.012, f_b: 30.268, goal: 29.338\n",
      "a: 0.5, m: 0.5153125000000001, b: 0.530625, f_a: 0.6270398173574556, f_m: 0.5910013471400042, f_b: 0.6171033764737228, goal: 0.6261814562507397\n",
      "starting search at const=1.9706027621489026; depth=6\n",
      "a: 0.9853013810744513, m: 1.9706027621489026, b: 3.9412055242978052, f_a: 17.934, f_m: 34.172, f_b: 58.416, goal: 29.338\n",
      "a: 0.9853013810744513, m: 1.477952071611677, b: 1.9706027621489026, f_a: 17.934, f_m: 25.106, f_b: 34.172, goal: 29.338\n",
      "a: 1.477952071611677, m: 1.7242774168802897, b: 1.9706027621489026, f_a: 25.106, f_m: 29.768, f_b: 34.172, goal: 29.338\n",
      "a: 1.477952071611677, m: 1.6011147442459834, b: 1.7242774168802897, f_a: 25.106, f_m: 27.078, f_b: 29.768, goal: 29.338\n",
      "a: 1.6011147442459834, m: 1.6626960805631366, b: 1.7242774168802897, f_a: 27.078, f_m: 28.244, f_b: 29.768, goal: 29.338\n",
      "a: 0.5, m: 0.50765625, b: 0.5153125000000001, f_a: 0.6270398173574556, f_m: 0.6060813101559163, f_b: 0.5910013471400042, goal: 0.6261814562507397\n",
      "starting search at const=1.8993325641325005; depth=6\n",
      "a: 0.9496662820662503, m: 1.8993325641325005, b: 3.798665128265001, f_a: 18.286, f_m: 32.838, f_b: 58.984, goal: 29.338\n",
      "a: 0.9496662820662503, m: 1.4244994230993755, b: 1.8993325641325005, f_a: 18.286, f_m: 25.456, f_b: 32.838, goal: 29.338\n",
      "a: 1.4244994230993755, m: 1.661915993615938, b: 1.8993325641325005, f_a: 25.456, f_m: 28.958, f_b: 32.838, goal: 29.338\n",
      "a: 1.661915993615938, m: 1.7806242788742193, b: 1.8993325641325005, f_a: 28.958, f_m: 29.57, f_b: 32.838, goal: 29.338\n",
      "a: 1.661915993615938, m: 1.7212701362450786, b: 1.7806242788742193, f_a: 28.958, f_m: 30.346, f_b: 29.57, goal: 29.338\n",
      "a: 0.5, m: 0.5038281250000001, b: 0.50765625, f_a: 0.6270398173574556, f_m: 0.6173189011948486, f_b: 0.6060813101559163, goal: 0.6261814562507397\n",
      "starting search at const=1.9412889184419249; depth=6\n",
      "a: 0.9706444592209624, m: 1.9412889184419249, b: 3.8825778368838497, f_a: 17.84, f_m: 32.332, f_b: 59.348, goal: 29.338\n",
      "a: 0.9706444592209624, m: 1.4559666888314435, b: 1.9412889184419249, f_a: 17.84, f_m: 25.256, f_b: 32.332, goal: 29.338\n",
      "a: 1.4559666888314435, m: 1.6986278036366842, b: 1.9412889184419249, f_a: 25.256, f_m: 28.938, f_b: 32.332, goal: 29.338\n",
      "a: 1.6986278036366842, m: 1.8199583610393044, b: 1.9412889184419249, f_a: 28.938, f_m: 29.014, f_b: 32.332, goal: 29.338\n",
      "a: 1.8199583610393044, m: 1.8806236397406146, b: 1.9412889184419249, f_a: 29.014, f_m: 32.9, f_b: 32.332, goal: 29.338\n",
      "a: 0.5, m: 0.5019140625, b: 0.5038281250000001, f_a: 0.6270398173574556, f_m: 0.6159568462497073, f_b: 0.6173189011948486, goal: 0.6261814562507397\n",
      "starting search at const=1.8195642936387437; depth=6\n",
      "a: 0.9097821468193719, m: 1.8195642936387437, b: 3.6391285872774874, f_a: 17.466, f_m: 32.338, f_b: 57.756, goal: 29.338\n",
      "a: 0.9097821468193719, m: 1.364673220229058, b: 1.8195642936387437, f_a: 17.466, f_m: 25.114, f_b: 32.338, goal: 29.338\n",
      "a: 1.364673220229058, m: 1.5921187569339008, b: 1.8195642936387437, f_a: 25.114, f_m: 28.162, f_b: 32.338, goal: 29.338\n",
      "a: 1.5921187569339008, m: 1.7058415252863224, b: 1.8195642936387437, f_a: 28.162, f_m: 30.688, f_b: 32.338, goal: 29.338\n",
      "a: 1.5921187569339008, m: 1.6489801411101115, b: 1.7058415252863224, f_a: 28.162, f_m: 29.786, f_b: 30.688, goal: 29.338\n",
      "a: 0.5, m: 0.50095703125, b: 0.5019140625, f_a: 0.6270398173574556, f_m: 0.6316082453201753, f_b: 0.6159568462497073, goal: 0.6261814562507397\n",
      "starting search at const=1.764243759223949; depth=6\n",
      "a: 0.8821218796119745, m: 1.764243759223949, b: 3.528487518447898, f_a: 17.71, f_m: 32.008, f_b: 56.066, goal: 29.338\n",
      "a: 0.8821218796119745, m: 1.3231828194179618, b: 1.764243759223949, f_a: 17.71, f_m: 25.264, f_b: 32.008, goal: 29.338\n",
      "a: 1.3231828194179618, m: 1.5437132893209555, b: 1.764243759223949, f_a: 25.264, f_m: 27.862, f_b: 32.008, goal: 29.338\n",
      "a: 1.5437132893209555, m: 1.6539785242724523, b: 1.764243759223949, f_a: 27.862, f_m: 30.506, f_b: 32.008, goal: 29.338\n",
      "a: 1.5437132893209555, m: 1.5988459067967038, b: 1.6539785242724523, f_a: 27.862, f_m: 29.244, f_b: 30.506, goal: 29.338\n",
      "a: 0.50095703125, m: 0.501435546875, b: 0.5019140625, f_a: 0.6316082453201753, f_m: 0.643254420341531, f_b: 0.6159568462497073, goal: 0.6261814562507397\n",
      "starting search at const=1.8756727665534538; depth=6\n",
      "a: 0.9378363832767269, m: 1.8756727665534538, b: 3.7513455331069077, f_a: 17.41, f_m: 31.592, f_b: 54.588, goal: 29.338\n",
      "a: 0.9378363832767269, m: 1.4067545749150905, b: 1.8756727665534538, f_a: 17.41, f_m: 24.624, f_b: 31.592, goal: 29.338\n",
      "a: 1.4067545749150905, m: 1.6412136707342722, b: 1.8756727665534538, f_a: 24.624, f_m: 29.376, f_b: 31.592, goal: 29.338\n",
      "a: 1.4067545749150905, m: 1.5239841228246813, b: 1.6412136707342722, f_a: 24.624, f_m: 27.146, f_b: 29.376, goal: 29.338\n",
      "a: 1.5239841228246813, m: 1.5825988967794768, b: 1.6412136707342722, f_a: 27.146, f_m: 26.966, f_b: 29.376, goal: 29.338\n"
     ]
    }
   ],
   "source": [
    "info, g_out = fe.fit_ndgirg_general(d, utils.LCC, cube=True, verbose=True)(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = fitting.scipy_lbfgs_girg_cube_fitter(g, d, tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  message: CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH\n",
       "  success: True\n",
       "   status: 0\n",
       "      fun: 8.648813231993664\n",
       "        x: [ 5.000e-01 -2.160e-05]\n",
       "      nit: 2\n",
       "      jac: [-2.632e+07 -7.995e+07]\n",
       "     nfev: 63\n",
       "     njev: 21\n",
       " hess_inv: <2x2 LbfgsInvHessProduct with dtype=float64>"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "========\n",
      "minimize\n",
      "========\n",
      "\n",
      "\n",
      "bfgs\n",
      "====\n",
      "\n",
      "Minimization of scalar function of one or more variables using the\n",
      "BFGS algorithm.\n",
      "\n",
      "Options\n",
      "-------\n",
      "disp : bool\n",
      "    Set to True to print convergence messages.\n",
      "maxiter : int\n",
      "    Maximum number of iterations to perform.\n",
      "gtol : float\n",
      "    Terminate successfully if gradient norm is less than `gtol`.\n",
      "norm : float\n",
      "    Order of norm (Inf is max, -Inf is min).\n",
      "eps : float or ndarray\n",
      "    If `jac is None` the absolute step size used for numerical\n",
      "    approximation of the jacobian via forward differences.\n",
      "return_all : bool, optional\n",
      "    Set to True to return a list of the best solution at each of the\n",
      "    iterations.\n",
      "finite_diff_rel_step : None or array_like, optional\n",
      "    If `jac in ['2-point', '3-point', 'cs']` the relative step size to\n",
      "    use for numerical approximation of the jacobian. The absolute step\n",
      "    size is computed as ``h = rel_step * sign(x) * max(1, abs(x))``,\n",
      "    possibly adjusted to fit into the bounds. For ``method='3-point'``\n",
      "    the sign of `h` is ignored. If None (default) then step is selected\n",
      "    automatically.\n",
      "xrtol : float, default: 0\n",
      "    Relative tolerance for `x`. Terminate successfully if step size is\n",
      "    less than ``xk * xrtol`` where ``xk`` is the current parameter vector.\n",
      "\n",
      "cg\n",
      "==\n",
      "\n",
      "Minimization of scalar function of one or more variables using the\n",
      "conjugate gradient algorithm.\n",
      "\n",
      "Options\n",
      "-------\n",
      "disp : bool\n",
      "    Set to True to print convergence messages.\n",
      "maxiter : int\n",
      "    Maximum number of iterations to perform.\n",
      "gtol : float\n",
      "    Gradient norm must be less than `gtol` before successful\n",
      "    termination.\n",
      "norm : float\n",
      "    Order of norm (Inf is max, -Inf is min).\n",
      "eps : float or ndarray\n",
      "    If `jac is None` the absolute step size used for numerical\n",
      "    approximation of the jacobian via forward differences.\n",
      "return_all : bool, optional\n",
      "    Set to True to return a list of the best solution at each of the\n",
      "    iterations.\n",
      "finite_diff_rel_step : None or array_like, optional\n",
      "    If `jac in ['2-point', '3-point', 'cs']` the relative step size to\n",
      "    use for numerical approximation of the jacobian. The absolute step\n",
      "    size is computed as ``h = rel_step * sign(x) * max(1, abs(x))``,\n",
      "    possibly adjusted to fit into the bounds. For ``method='3-point'``\n",
      "    the sign of `h` is ignored. If None (default) then step is selected\n",
      "    automatically.\n",
      "\n",
      "cobyla\n",
      "======\n",
      "\n",
      "Minimize a scalar function of one or more variables using the\n",
      "Constrained Optimization BY Linear Approximation (COBYLA) algorithm.\n",
      "\n",
      "Options\n",
      "-------\n",
      "rhobeg : float\n",
      "    Reasonable initial changes to the variables.\n",
      "tol : float\n",
      "    Final accuracy in the optimization (not precisely guaranteed).\n",
      "    This is a lower bound on the size of the trust region.\n",
      "disp : bool\n",
      "    Set to True to print convergence messages. If False,\n",
      "    `verbosity` is ignored as set to 0.\n",
      "maxiter : int\n",
      "    Maximum number of function evaluations.\n",
      "catol : float\n",
      "    Tolerance (absolute) for constraint violations\n",
      "\n",
      "dogleg\n",
      "======\n",
      "\n",
      "Minimization of scalar function of one or more variables using\n",
      "the dog-leg trust-region algorithm.\n",
      "\n",
      "Options\n",
      "-------\n",
      "initial_trust_radius : float\n",
      "    Initial trust-region radius.\n",
      "max_trust_radius : float\n",
      "    Maximum value of the trust-region radius. No steps that are longer\n",
      "    than this value will be proposed.\n",
      "eta : float\n",
      "    Trust region related acceptance stringency for proposed steps.\n",
      "gtol : float\n",
      "    Gradient norm must be less than `gtol` before successful\n",
      "    termination.\n",
      "\n",
      "l-bfgs-b\n",
      "========\n",
      "\n",
      "Minimize a scalar function of one or more variables using the L-BFGS-B\n",
      "algorithm.\n",
      "\n",
      "Options\n",
      "-------\n",
      "disp : None or int\n",
      "    If `disp is None` (the default), then the supplied version of `iprint`\n",
      "    is used. If `disp is not None`, then it overrides the supplied version\n",
      "    of `iprint` with the behaviour you outlined.\n",
      "maxcor : int\n",
      "    The maximum number of variable metric corrections used to\n",
      "    define the limited memory matrix. (The limited memory BFGS\n",
      "    method does not store the full hessian but uses this many terms\n",
      "    in an approximation to it.)\n",
      "ftol : float\n",
      "    The iteration stops when ``(f^k -\n",
      "    f^{k+1})/max{|f^k|,|f^{k+1}|,1} <= ftol``.\n",
      "gtol : float\n",
      "    The iteration will stop when ``max{|proj g_i | i = 1, ..., n}\n",
      "    <= gtol`` where ``pg_i`` is the i-th component of the\n",
      "    projected gradient.\n",
      "eps : float or ndarray\n",
      "    If `jac is None` the absolute step size used for numerical\n",
      "    approximation of the jacobian via forward differences.\n",
      "maxfun : int\n",
      "    Maximum number of function evaluations. Note that this function\n",
      "    may violate the limit because of evaluating gradients by numerical\n",
      "    differentiation.\n",
      "maxiter : int\n",
      "    Maximum number of iterations.\n",
      "iprint : int, optional\n",
      "    Controls the frequency of output. ``iprint < 0`` means no output;\n",
      "    ``iprint = 0``    print only one line at the last iteration;\n",
      "    ``0 < iprint < 99`` print also f and ``|proj g|`` every iprint iterations;\n",
      "    ``iprint = 99``   print details of every iteration except n-vectors;\n",
      "    ``iprint = 100``  print also the changes of active set and final x;\n",
      "    ``iprint > 100``  print details of every iteration including x and g.\n",
      "maxls : int, optional\n",
      "    Maximum number of line search steps (per iteration). Default is 20.\n",
      "finite_diff_rel_step : None or array_like, optional\n",
      "    If `jac in ['2-point', '3-point', 'cs']` the relative step size to\n",
      "    use for numerical approximation of the jacobian. The absolute step\n",
      "    size is computed as ``h = rel_step * sign(x) * max(1, abs(x))``,\n",
      "    possibly adjusted to fit into the bounds. For ``method='3-point'``\n",
      "    the sign of `h` is ignored. If None (default) then step is selected\n",
      "    automatically.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "The option `ftol` is exposed via the `scipy.optimize.minimize` interface,\n",
      "but calling `scipy.optimize.fmin_l_bfgs_b` directly exposes `factr`. The\n",
      "relationship between the two is ``ftol = factr * numpy.finfo(float).eps``.\n",
      "I.e., `factr` multiplies the default machine floating-point precision to\n",
      "arrive at `ftol`.\n",
      "\n",
      "nelder-mead\n",
      "===========\n",
      "\n",
      "Minimization of scalar function of one or more variables using the\n",
      "Nelder-Mead algorithm.\n",
      "\n",
      "Options\n",
      "-------\n",
      "disp : bool\n",
      "    Set to True to print convergence messages.\n",
      "maxiter, maxfev : int\n",
      "    Maximum allowed number of iterations and function evaluations.\n",
      "    Will default to ``N*200``, where ``N`` is the number of\n",
      "    variables, if neither `maxiter` or `maxfev` is set. If both\n",
      "    `maxiter` and `maxfev` are set, minimization will stop at the\n",
      "    first reached.\n",
      "return_all : bool, optional\n",
      "    Set to True to return a list of the best solution at each of the\n",
      "    iterations.\n",
      "initial_simplex : array_like of shape (N + 1, N)\n",
      "    Initial simplex. If given, overrides `x0`.\n",
      "    ``initial_simplex[j,:]`` should contain the coordinates of\n",
      "    the jth vertex of the ``N+1`` vertices in the simplex, where\n",
      "    ``N`` is the dimension.\n",
      "xatol : float, optional\n",
      "    Absolute error in xopt between iterations that is acceptable for\n",
      "    convergence.\n",
      "fatol : number, optional\n",
      "    Absolute error in func(xopt) between iterations that is acceptable for\n",
      "    convergence.\n",
      "adaptive : bool, optional\n",
      "    Adapt algorithm parameters to dimensionality of problem. Useful for\n",
      "    high-dimensional minimization [1]_.\n",
      "bounds : sequence or `Bounds`, optional\n",
      "    Bounds on variables. There are two ways to specify the bounds:\n",
      "\n",
      "        1. Instance of `Bounds` class.\n",
      "        2. Sequence of ``(min, max)`` pairs for each element in `x`. None\n",
      "           is used to specify no bound.\n",
      "\n",
      "    Note that this just clips all vertices in simplex based on\n",
      "    the bounds.\n",
      "\n",
      "References\n",
      "----------\n",
      ".. [1] Gao, F. and Han, L.\n",
      "   Implementing the Nelder-Mead simplex algorithm with adaptive\n",
      "   parameters. 2012. Computational Optimization and Applications.\n",
      "   51:1, pp. 259-277\n",
      "\n",
      "newton-cg\n",
      "=========\n",
      "\n",
      "Minimization of scalar function of one or more variables using the\n",
      "Newton-CG algorithm.\n",
      "\n",
      "Note that the `jac` parameter (Jacobian) is required.\n",
      "\n",
      "Options\n",
      "-------\n",
      "disp : bool\n",
      "    Set to True to print convergence messages.\n",
      "xtol : float\n",
      "    Average relative error in solution `xopt` acceptable for\n",
      "    convergence.\n",
      "maxiter : int\n",
      "    Maximum number of iterations to perform.\n",
      "eps : float or ndarray\n",
      "    If `hessp` is approximated, use this value for the step size.\n",
      "return_all : bool, optional\n",
      "    Set to True to return a list of the best solution at each of the\n",
      "    iterations.\n",
      "\n",
      "powell\n",
      "======\n",
      "\n",
      "Minimization of scalar function of one or more variables using the\n",
      "modified Powell algorithm.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "fun : callable\n",
      "    The objective function to be minimized.\n",
      "\n",
      "        ``fun(x, *args) -> float``\n",
      "\n",
      "    where ``x`` is a 1-D array with shape (n,) and ``args``\n",
      "    is a tuple of the fixed parameters needed to completely\n",
      "    specify the function.\n",
      "x0 : ndarray, shape (n,)\n",
      "    Initial guess. Array of real elements of size (n,),\n",
      "    where ``n`` is the number of independent variables.\n",
      "args : tuple, optional\n",
      "    Extra arguments passed to the objective function and its\n",
      "    derivatives (`fun`, `jac` and `hess` functions).\n",
      "method : str or callable, optional\n",
      "    The present documentation is specific to ``method='powell'``, but other\n",
      "    options are available. See documentation for `scipy.optimize.minimize`.\n",
      "bounds : sequence or `Bounds`, optional\n",
      "    Bounds on decision variables. There are two ways to specify the bounds:\n",
      "\n",
      "        1. Instance of `Bounds` class.\n",
      "        2. Sequence of ``(min, max)`` pairs for each element in `x`. None\n",
      "           is used to specify no bound.\n",
      "\n",
      "    If bounds are not provided, then an unbounded line search will be used.\n",
      "    If bounds are provided and the initial guess is within the bounds, then\n",
      "    every function evaluation throughout the minimization procedure will be\n",
      "    within the bounds. If bounds are provided, the initial guess is outside\n",
      "    the bounds, and `direc` is full rank (or left to default), then some\n",
      "    function evaluations during the first iteration may be outside the\n",
      "    bounds, but every function evaluation after the first iteration will be\n",
      "    within the bounds. If `direc` is not full rank, then some parameters\n",
      "    may not be optimized and the solution is not guaranteed to be within\n",
      "    the bounds.\n",
      "\n",
      "options : dict, optional\n",
      "    A dictionary of solver options. All methods accept the following\n",
      "    generic options:\n",
      "\n",
      "        maxiter : int\n",
      "            Maximum number of iterations to perform. Depending on the\n",
      "            method each iteration may use several function evaluations.\n",
      "        disp : bool\n",
      "            Set to True to print convergence messages.\n",
      "\n",
      "    See method-specific options for ``method='powell'`` below.\n",
      "callback : callable, optional\n",
      "    Called after each iteration. The signature is:\n",
      "\n",
      "        ``callback(xk)``\n",
      "\n",
      "    where ``xk`` is the current parameter vector.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "res : OptimizeResult\n",
      "    The optimization result represented as a ``OptimizeResult`` object.\n",
      "    Important attributes are: ``x`` the solution array, ``success`` a\n",
      "    Boolean flag indicating if the optimizer exited successfully and\n",
      "    ``message`` which describes the cause of the termination. See\n",
      "    `OptimizeResult` for a description of other attributes.\n",
      "\n",
      "Options\n",
      "-------\n",
      "disp : bool\n",
      "    Set to True to print convergence messages.\n",
      "xtol : float\n",
      "    Relative error in solution `xopt` acceptable for convergence.\n",
      "ftol : float\n",
      "    Relative error in ``fun(xopt)`` acceptable for convergence.\n",
      "maxiter, maxfev : int\n",
      "    Maximum allowed number of iterations and function evaluations.\n",
      "    Will default to ``N*1000``, where ``N`` is the number of\n",
      "    variables, if neither `maxiter` or `maxfev` is set. If both\n",
      "    `maxiter` and `maxfev` are set, minimization will stop at the\n",
      "    first reached.\n",
      "direc : ndarray\n",
      "    Initial set of direction vectors for the Powell method.\n",
      "return_all : bool, optional\n",
      "    Set to True to return a list of the best solution at each of the\n",
      "    iterations.\n",
      "\n",
      "slsqp\n",
      "=====\n",
      "\n",
      "Minimize a scalar function of one or more variables using Sequential\n",
      "Least Squares Programming (SLSQP).\n",
      "\n",
      "Options\n",
      "-------\n",
      "ftol : float\n",
      "    Precision goal for the value of f in the stopping criterion.\n",
      "eps : float\n",
      "    Step size used for numerical approximation of the Jacobian.\n",
      "disp : bool\n",
      "    Set to True to print convergence messages. If False,\n",
      "    `verbosity` is ignored and set to 0.\n",
      "maxiter : int\n",
      "    Maximum number of iterations.\n",
      "finite_diff_rel_step : None or array_like, optional\n",
      "    If `jac in ['2-point', '3-point', 'cs']` the relative step size to\n",
      "    use for numerical approximation of `jac`. The absolute step\n",
      "    size is computed as ``h = rel_step * sign(x) * max(1, abs(x))``,\n",
      "    possibly adjusted to fit into the bounds. For ``method='3-point'``\n",
      "    the sign of `h` is ignored. If None (default) then step is selected\n",
      "    automatically.\n",
      "\n",
      "tnc\n",
      "===\n",
      "\n",
      "Minimize a scalar function of one or more variables using a truncated\n",
      "Newton (TNC) algorithm.\n",
      "\n",
      "Options\n",
      "-------\n",
      "eps : float or ndarray\n",
      "    If `jac is None` the absolute step size used for numerical\n",
      "    approximation of the jacobian via forward differences.\n",
      "scale : list of floats\n",
      "    Scaling factors to apply to each variable. If None, the\n",
      "    factors are up-low for interval bounded variables and\n",
      "    1+|x] fo the others. Defaults to None.\n",
      "offset : float\n",
      "    Value to subtract from each variable. If None, the\n",
      "    offsets are (up+low)/2 for interval bounded variables\n",
      "    and x for the others.\n",
      "disp : bool\n",
      "   Set to True to print convergence messages.\n",
      "maxCGit : int\n",
      "    Maximum number of hessian*vector evaluations per main\n",
      "    iteration. If maxCGit == 0, the direction chosen is\n",
      "    -gradient if maxCGit < 0, maxCGit is set to\n",
      "    max(1,min(50,n/2)). Defaults to -1.\n",
      "maxiter : int, optional\n",
      "    Maximum number of function evaluations. If `maxfun` is also provided\n",
      "    then `maxiter` is ignored.\n",
      "    Default is None.\n",
      "\n",
      "    .. deprecated :: 1.9.0\n",
      "        `maxiter` is deprecated in favor of `maxfun` and will removed in\n",
      "        SciPy 1.11.0.\n",
      "eta : float\n",
      "    Severity of the line search. If < 0 or > 1, set to 0.25.\n",
      "    Defaults to -1.\n",
      "stepmx : float\n",
      "    Maximum step for the line search. May be increased during\n",
      "    call. If too small, it will be set to 10.0. Defaults to 0.\n",
      "accuracy : float\n",
      "    Relative precision for finite difference calculations. If\n",
      "    <= machine_precision, set to sqrt(machine_precision).\n",
      "    Defaults to 0.\n",
      "minfev : float\n",
      "    Minimum function value estimate. Defaults to 0.\n",
      "ftol : float\n",
      "    Precision goal for the value of f in the stopping criterion.\n",
      "    If ftol < 0.0, ftol is set to 0.0 defaults to -1.\n",
      "xtol : float\n",
      "    Precision goal for the value of x in the stopping\n",
      "    criterion (after applying x scaling factors). If xtol <\n",
      "    0.0, xtol is set to sqrt(machine_precision). Defaults to\n",
      "    -1.\n",
      "gtol : float\n",
      "    Precision goal for the value of the projected gradient in\n",
      "    the stopping criterion (after applying x scaling factors).\n",
      "    If gtol < 0.0, gtol is set to 1e-2 * sqrt(accuracy).\n",
      "    Setting it to 0.0 is not recommended. Defaults to -1.\n",
      "rescale : float\n",
      "    Scaling factor (in log10) used to trigger f value\n",
      "    rescaling.  If 0, rescale at each iteration.  If a large\n",
      "    value, never rescale.  If < 0, rescale is set to 1.3.\n",
      "finite_diff_rel_step : None or array_like, optional\n",
      "    If `jac in ['2-point', '3-point', 'cs']` the relative step size to\n",
      "    use for numerical approximation of the jacobian. The absolute step\n",
      "    size is computed as ``h = rel_step * sign(x) * max(1, abs(x))``,\n",
      "    possibly adjusted to fit into the bounds. For ``method='3-point'``\n",
      "    the sign of `h` is ignored. If None (default) then step is selected\n",
      "    automatically.\n",
      "maxfun : int\n",
      "    Maximum number of function evaluations. If None, `maxfun` is\n",
      "    set to max(100, 10*len(x0)). Defaults to None.\n",
      "\n",
      "trust-ncg\n",
      "=========\n",
      "\n",
      "Minimization of scalar function of one or more variables using\n",
      "the Newton conjugate gradient trust-region algorithm.\n",
      "\n",
      "Options\n",
      "-------\n",
      "initial_trust_radius : float\n",
      "    Initial trust-region radius.\n",
      "max_trust_radius : float\n",
      "    Maximum value of the trust-region radius. No steps that are longer\n",
      "    than this value will be proposed.\n",
      "eta : float\n",
      "    Trust region related acceptance stringency for proposed steps.\n",
      "gtol : float\n",
      "    Gradient norm must be less than `gtol` before successful\n",
      "    termination.\n",
      "\n",
      "trust-constr\n",
      "============\n",
      "\n",
      "Minimize a scalar function subject to constraints.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    gtol : float, optional\n",
      "        Tolerance for termination by the norm of the Lagrangian gradient.\n",
      "        The algorithm will terminate when both the infinity norm (i.e., max\n",
      "        abs value) of the Lagrangian gradient and the constraint violation\n",
      "        are smaller than ``gtol``. Default is 1e-8.\n",
      "    xtol : float, optional\n",
      "        Tolerance for termination by the change of the independent variable.\n",
      "        The algorithm will terminate when ``tr_radius < xtol``, where\n",
      "        ``tr_radius`` is the radius of the trust region used in the algorithm.\n",
      "        Default is 1e-8.\n",
      "    barrier_tol : float, optional\n",
      "        Threshold on the barrier parameter for the algorithm termination.\n",
      "        When inequality constraints are present, the algorithm will terminate\n",
      "        only when the barrier parameter is less than `barrier_tol`.\n",
      "        Default is 1e-8.\n",
      "    sparse_jacobian : {bool, None}, optional\n",
      "        Determines how to represent Jacobians of the constraints. If bool,\n",
      "        then Jacobians of all the constraints will be converted to the\n",
      "        corresponding format. If None (default), then Jacobians won't be\n",
      "        converted, but the algorithm can proceed only if they all have the\n",
      "        same format.\n",
      "    initial_tr_radius: float, optional\n",
      "        Initial trust radius. The trust radius gives the maximum distance\n",
      "        between solution points in consecutive iterations. It reflects the\n",
      "        trust the algorithm puts in the local approximation of the optimization\n",
      "        problem. For an accurate local approximation the trust-region should be\n",
      "        large and for an  approximation valid only close to the current point it\n",
      "        should be a small one. The trust radius is automatically updated throughout\n",
      "        the optimization process, with ``initial_tr_radius`` being its initial value.\n",
      "        Default is 1 (recommended in [1]_, p. 19).\n",
      "    initial_constr_penalty : float, optional\n",
      "        Initial constraints penalty parameter. The penalty parameter is used for\n",
      "        balancing the requirements of decreasing the objective function\n",
      "        and satisfying the constraints. It is used for defining the merit function:\n",
      "        ``merit_function(x) = fun(x) + constr_penalty * constr_norm_l2(x)``,\n",
      "        where ``constr_norm_l2(x)`` is the l2 norm of a vector containing all\n",
      "        the constraints. The merit function is used for accepting or rejecting\n",
      "        trial points and ``constr_penalty`` weights the two conflicting goals\n",
      "        of reducing objective function and constraints. The penalty is automatically\n",
      "        updated throughout the optimization  process, with\n",
      "        ``initial_constr_penalty`` being its  initial value. Default is 1\n",
      "        (recommended in [1]_, p 19).\n",
      "    initial_barrier_parameter, initial_barrier_tolerance: float, optional\n",
      "        Initial barrier parameter and initial tolerance for the barrier subproblem.\n",
      "        Both are used only when inequality constraints are present. For dealing with\n",
      "        optimization problems ``min_x f(x)`` subject to inequality constraints\n",
      "        ``c(x) <= 0`` the algorithm introduces slack variables, solving the problem\n",
      "        ``min_(x,s) f(x) + barrier_parameter*sum(ln(s))`` subject to the equality\n",
      "        constraints  ``c(x) + s = 0`` instead of the original problem. This subproblem\n",
      "        is solved for decreasing values of ``barrier_parameter`` and with decreasing\n",
      "        tolerances for the termination, starting with ``initial_barrier_parameter``\n",
      "        for the barrier parameter and ``initial_barrier_tolerance`` for the\n",
      "        barrier tolerance. Default is 0.1 for both values (recommended in [1]_ p. 19).\n",
      "        Also note that ``barrier_parameter`` and ``barrier_tolerance`` are updated\n",
      "        with the same prefactor.\n",
      "    factorization_method : string or None, optional\n",
      "        Method to factorize the Jacobian of the constraints. Use None (default)\n",
      "        for the auto selection or one of:\n",
      "\n",
      "            - 'NormalEquation' (requires scikit-sparse)\n",
      "            - 'AugmentedSystem'\n",
      "            - 'QRFactorization'\n",
      "            - 'SVDFactorization'\n",
      "\n",
      "        The methods 'NormalEquation' and 'AugmentedSystem' can be used only\n",
      "        with sparse constraints. The projections required by the algorithm\n",
      "        will be computed using, respectively, the normal equation  and the\n",
      "        augmented system approaches explained in [1]_. 'NormalEquation'\n",
      "        computes the Cholesky factorization of ``A A.T`` and 'AugmentedSystem'\n",
      "        performs the LU factorization of an augmented system. They usually\n",
      "        provide similar results. 'AugmentedSystem' is used by default for\n",
      "        sparse matrices.\n",
      "\n",
      "        The methods 'QRFactorization' and 'SVDFactorization' can be used\n",
      "        only with dense constraints. They compute the required projections\n",
      "        using, respectively, QR and SVD factorizations. The 'SVDFactorization'\n",
      "        method can cope with Jacobian matrices with deficient row rank and will\n",
      "        be used whenever other factorization methods fail (which may imply the\n",
      "        conversion of sparse matrices to a dense format when required).\n",
      "        By default, 'QRFactorization' is used for dense matrices.\n",
      "    finite_diff_rel_step : None or array_like, optional\n",
      "        Relative step size for the finite difference approximation.\n",
      "    maxiter : int, optional\n",
      "        Maximum number of algorithm iterations. Default is 1000.\n",
      "    verbose : {0, 1, 2}, optional\n",
      "        Level of algorithm's verbosity:\n",
      "\n",
      "            * 0 (default) : work silently.\n",
      "            * 1 : display a termination report.\n",
      "            * 2 : display progress during iterations.\n",
      "            * 3 : display progress during iterations (more complete report).\n",
      "\n",
      "    disp : bool, optional\n",
      "        If True (default), then `verbose` will be set to 1 if it was 0.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    `OptimizeResult` with the fields documented below. Note the following:\n",
      "\n",
      "        1. All values corresponding to the constraints are ordered as they\n",
      "           were passed to the solver. And values corresponding to `bounds`\n",
      "           constraints are put *after* other constraints.\n",
      "        2. All numbers of function, Jacobian or Hessian evaluations correspond\n",
      "           to numbers of actual Python function calls. It means, for example,\n",
      "           that if a Jacobian is estimated by finite differences, then the\n",
      "           number of Jacobian evaluations will be zero and the number of\n",
      "           function evaluations will be incremented by all calls during the\n",
      "           finite difference estimation.\n",
      "\n",
      "    x : ndarray, shape (n,)\n",
      "        Solution found.\n",
      "    optimality : float\n",
      "        Infinity norm of the Lagrangian gradient at the solution.\n",
      "    constr_violation : float\n",
      "        Maximum constraint violation at the solution.\n",
      "    fun : float\n",
      "        Objective function at the solution.\n",
      "    grad : ndarray, shape (n,)\n",
      "        Gradient of the objective function at the solution.\n",
      "    lagrangian_grad : ndarray, shape (n,)\n",
      "        Gradient of the Lagrangian function at the solution.\n",
      "    nit : int\n",
      "        Total number of iterations.\n",
      "    nfev : integer\n",
      "        Number of the objective function evaluations.\n",
      "    njev : integer\n",
      "        Number of the objective function gradient evaluations.\n",
      "    nhev : integer\n",
      "        Number of the objective function Hessian evaluations.\n",
      "    cg_niter : int\n",
      "        Total number of the conjugate gradient method iterations.\n",
      "    method : {'equality_constrained_sqp', 'tr_interior_point'}\n",
      "        Optimization method used.\n",
      "    constr : list of ndarray\n",
      "        List of constraint values at the solution.\n",
      "    jac : list of {ndarray, sparse matrix}\n",
      "        List of the Jacobian matrices of the constraints at the solution.\n",
      "    v : list of ndarray\n",
      "        List of the Lagrange multipliers for the constraints at the solution.\n",
      "        For an inequality constraint a positive multiplier means that the upper\n",
      "        bound is active, a negative multiplier means that the lower bound is\n",
      "        active and if a multiplier is zero it means the constraint is not\n",
      "        active.\n",
      "    constr_nfev : list of int\n",
      "        Number of constraint evaluations for each of the constraints.\n",
      "    constr_njev : list of int\n",
      "        Number of Jacobian matrix evaluations for each of the constraints.\n",
      "    constr_nhev : list of int\n",
      "        Number of Hessian evaluations for each of the constraints.\n",
      "    tr_radius : float\n",
      "        Radius of the trust region at the last iteration.\n",
      "    constr_penalty : float\n",
      "        Penalty parameter at the last iteration, see `initial_constr_penalty`.\n",
      "    barrier_tolerance : float\n",
      "        Tolerance for the barrier subproblem at the last iteration.\n",
      "        Only for problems with inequality constraints.\n",
      "    barrier_parameter : float\n",
      "        Barrier parameter at the last iteration. Only for problems\n",
      "        with inequality constraints.\n",
      "    execution_time : float\n",
      "        Total execution time.\n",
      "    message : str\n",
      "        Termination message.\n",
      "    status : {0, 1, 2, 3}\n",
      "        Termination status:\n",
      "\n",
      "            * 0 : The maximum number of function evaluations is exceeded.\n",
      "            * 1 : `gtol` termination condition is satisfied.\n",
      "            * 2 : `xtol` termination condition is satisfied.\n",
      "            * 3 : `callback` function requested termination.\n",
      "\n",
      "    cg_stop_cond : int\n",
      "        Reason for CG subproblem termination at the last iteration:\n",
      "\n",
      "            * 0 : CG subproblem not evaluated.\n",
      "            * 1 : Iteration limit was reached.\n",
      "            * 2 : Reached the trust-region boundary.\n",
      "            * 3 : Negative curvature detected.\n",
      "            * 4 : Tolerance was satisfied.\n",
      "\n",
      "    References\n",
      "    ----------\n",
      "    .. [1] Conn, A. R., Gould, N. I., & Toint, P. L.\n",
      "           Trust region methods. 2000. Siam. pp. 19.\n",
      "\n",
      "trust-exact\n",
      "===========\n",
      "\n",
      "Minimization of scalar function of one or more variables using\n",
      "a nearly exact trust-region algorithm.\n",
      "\n",
      "Options\n",
      "-------\n",
      "initial_tr_radius : float\n",
      "    Initial trust-region radius.\n",
      "max_tr_radius : float\n",
      "    Maximum value of the trust-region radius. No steps that are longer\n",
      "    than this value will be proposed.\n",
      "eta : float\n",
      "    Trust region related acceptance stringency for proposed steps.\n",
      "gtol : float\n",
      "    Gradient norm must be less than ``gtol`` before successful\n",
      "    termination.\n",
      "\n",
      "trust-krylov\n",
      "============\n",
      "\n",
      "Minimization of a scalar function of one or more variables using\n",
      "a nearly exact trust-region algorithm that only requires matrix\n",
      "vector products with the hessian matrix.\n",
      "\n",
      ".. versionadded:: 1.0.0\n",
      "\n",
      "Options\n",
      "-------\n",
      "inexact : bool, optional\n",
      "    Accuracy to solve subproblems. If True requires less nonlinear\n",
      "    iterations, but more vector products.\n",
      "\n",
      "===============\n",
      "minimize_scalar\n",
      "===============\n",
      "\n",
      "\n",
      "brent\n",
      "=====\n",
      "\n",
      "Options\n",
      "-------\n",
      "maxiter : int\n",
      "    Maximum number of iterations to perform.\n",
      "xtol : float\n",
      "    Relative error in solution `xopt` acceptable for convergence.\n",
      "disp: int, optional\n",
      "    If non-zero, print messages.\n",
      "        0 : no message printing.\n",
      "        1 : non-convergence notification messages only.\n",
      "        2 : print a message on convergence too.\n",
      "        3 : print iteration results.\n",
      "Notes\n",
      "-----\n",
      "Uses inverse parabolic interpolation when possible to speed up\n",
      "convergence of golden section method.\n",
      "\n",
      "bounded\n",
      "=======\n",
      "\n",
      "Options\n",
      "-------\n",
      "maxiter : int\n",
      "    Maximum number of iterations to perform.\n",
      "disp: int, optional\n",
      "    If non-zero, print messages.\n",
      "        0 : no message printing.\n",
      "        1 : non-convergence notification messages only.\n",
      "        2 : print a message on convergence too.\n",
      "        3 : print iteration results.\n",
      "xatol : float\n",
      "    Absolute error in solution `xopt` acceptable for convergence.\n",
      "\n",
      "golden\n",
      "======\n",
      "\n",
      "Options\n",
      "-------\n",
      "xtol : float\n",
      "    Relative error in solution `xopt` acceptable for convergence.\n",
      "maxiter : int\n",
      "    Maximum number of iterations to perform.\n",
      "disp: int, optional\n",
      "    If non-zero, print messages.\n",
      "        0 : no message printing.\n",
      "        1 : non-convergence notification messages only.\n",
      "        2 : print a message on convergence too.\n",
      "        3 : print iteration results.\n",
      "\n",
      "\n",
      "====\n",
      "root\n",
      "====\n",
      "\n",
      "\n",
      "hybr\n",
      "====\n",
      "\n",
      "Find the roots of a multivariate function using MINPACK's hybrd and\n",
      "hybrj routines (modified Powell method).\n",
      "\n",
      "Options\n",
      "-------\n",
      "col_deriv : bool\n",
      "    Specify whether the Jacobian function computes derivatives down\n",
      "    the columns (faster, because there is no transpose operation).\n",
      "xtol : float\n",
      "    The calculation will terminate if the relative error between two\n",
      "    consecutive iterates is at most `xtol`.\n",
      "maxfev : int\n",
      "    The maximum number of calls to the function. If zero, then\n",
      "    ``100*(N+1)`` is the maximum where N is the number of elements\n",
      "    in `x0`.\n",
      "band : tuple\n",
      "    If set to a two-sequence containing the number of sub- and\n",
      "    super-diagonals within the band of the Jacobi matrix, the\n",
      "    Jacobi matrix is considered banded (only for ``fprime=None``).\n",
      "eps : float\n",
      "    A suitable step length for the forward-difference\n",
      "    approximation of the Jacobian (for ``fprime=None``). If\n",
      "    `eps` is less than the machine precision, it is assumed\n",
      "    that the relative errors in the functions are of the order of\n",
      "    the machine precision.\n",
      "factor : float\n",
      "    A parameter determining the initial step bound\n",
      "    (``factor * || diag * x||``). Should be in the interval\n",
      "    ``(0.1, 100)``.\n",
      "diag : sequence\n",
      "    N positive entries that serve as a scale factors for the\n",
      "    variables.\n",
      "\n",
      "lm\n",
      "==\n",
      "\n",
      "Solve for least squares with Levenberg-Marquardt\n",
      "\n",
      "Options\n",
      "-------\n",
      "col_deriv : bool\n",
      "    non-zero to specify that the Jacobian function computes derivatives\n",
      "    down the columns (faster, because there is no transpose operation).\n",
      "ftol : float\n",
      "    Relative error desired in the sum of squares.\n",
      "xtol : float\n",
      "    Relative error desired in the approximate solution.\n",
      "gtol : float\n",
      "    Orthogonality desired between the function vector and the columns\n",
      "    of the Jacobian.\n",
      "maxiter : int\n",
      "    The maximum number of calls to the function. If zero, then\n",
      "    100*(N+1) is the maximum where N is the number of elements in x0.\n",
      "epsfcn : float\n",
      "    A suitable step length for the forward-difference approximation of\n",
      "    the Jacobian (for Dfun=None). If epsfcn is less than the machine\n",
      "    precision, it is assumed that the relative errors in the functions\n",
      "    are of the order of the machine precision.\n",
      "factor : float\n",
      "    A parameter determining the initial step bound\n",
      "    (``factor * || diag * x||``). Should be in interval ``(0.1, 100)``.\n",
      "diag : sequence\n",
      "    N positive entries that serve as a scale factors for the variables.\n",
      "\n",
      "broyden1\n",
      "========\n",
      "\n",
      "Options\n",
      "-------\n",
      "nit : int, optional\n",
      "    Number of iterations to make. If omitted (default), make as many\n",
      "    as required to meet tolerances.\n",
      "disp : bool, optional\n",
      "    Print status to stdout on every iteration.\n",
      "maxiter : int, optional\n",
      "    Maximum number of iterations to make. If more are needed to\n",
      "    meet convergence, `NoConvergence` is raised.\n",
      "ftol : float, optional\n",
      "    Relative tolerance for the residual. If omitted, not used.\n",
      "fatol : float, optional\n",
      "    Absolute tolerance (in max-norm) for the residual.\n",
      "    If omitted, default is 6e-6.\n",
      "xtol : float, optional\n",
      "    Relative minimum step size. If omitted, not used.\n",
      "xatol : float, optional\n",
      "    Absolute minimum step size, as determined from the Jacobian\n",
      "    approximation. If the step size is smaller than this, optimization\n",
      "    is terminated as successful. If omitted, not used.\n",
      "tol_norm : function(vector) -> scalar, optional\n",
      "    Norm to use in convergence check. Default is the maximum norm.\n",
      "line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "    Which type of a line search to use to determine the step size in\n",
      "    the direction given by the Jacobian approximation. Defaults to\n",
      "    'armijo'.\n",
      "jac_options : dict, optional\n",
      "    Options for the respective Jacobian approximation.\n",
      "        alpha : float, optional\n",
      "            Initial guess for the Jacobian is (-1/alpha).\n",
      "        reduction_method : str or tuple, optional\n",
      "            Method used in ensuring that the rank of the Broyden\n",
      "            matrix stays low. Can either be a string giving the\n",
      "            name of the method, or a tuple of the form ``(method,\n",
      "            param1, param2, ...)`` that gives the name of the\n",
      "            method and values for additional parameters.\n",
      "\n",
      "            Methods available:\n",
      "\n",
      "                - ``restart``\n",
      "                    Drop all matrix columns. Has no\n",
      "                    extra parameters.\n",
      "                - ``simple``\n",
      "                    Drop oldest matrix column. Has no\n",
      "                    extra parameters.\n",
      "                - ``svd``\n",
      "                    Keep only the most significant SVD\n",
      "                    components.\n",
      "\n",
      "                    Extra parameters:\n",
      "\n",
      "                        - ``to_retain``\n",
      "                            Number of SVD components to\n",
      "                            retain when rank reduction is done.\n",
      "                            Default is ``max_rank - 2``.\n",
      "        max_rank : int, optional\n",
      "            Maximum rank for the Broyden matrix.\n",
      "            Default is infinity (i.e., no rank reduction).\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> def func(x):\n",
      "...     return np.cos(x) + x[::-1] - [1, 2, 3, 4]\n",
      "...\n",
      ">>> from scipy import optimize\n",
      ">>> res = optimize.root(func, [1, 1, 1, 1], method='broyden1', tol=1e-14)\n",
      ">>> x = res.x\n",
      ">>> x\n",
      "array([4.04674914, 3.91158389, 2.71791677, 1.61756251])\n",
      ">>> np.cos(x) + x[::-1]\n",
      "array([1., 2., 3., 4.])\n",
      "\n",
      "broyden2\n",
      "========\n",
      "\n",
      "Options\n",
      "-------\n",
      "nit : int, optional\n",
      "    Number of iterations to make. If omitted (default), make as many\n",
      "    as required to meet tolerances.\n",
      "disp : bool, optional\n",
      "    Print status to stdout on every iteration.\n",
      "maxiter : int, optional\n",
      "    Maximum number of iterations to make. If more are needed to\n",
      "    meet convergence, `NoConvergence` is raised.\n",
      "ftol : float, optional\n",
      "    Relative tolerance for the residual. If omitted, not used.\n",
      "fatol : float, optional\n",
      "    Absolute tolerance (in max-norm) for the residual.\n",
      "    If omitted, default is 6e-6.\n",
      "xtol : float, optional\n",
      "    Relative minimum step size. If omitted, not used.\n",
      "xatol : float, optional\n",
      "    Absolute minimum step size, as determined from the Jacobian\n",
      "    approximation. If the step size is smaller than this, optimization\n",
      "    is terminated as successful. If omitted, not used.\n",
      "tol_norm : function(vector) -> scalar, optional\n",
      "    Norm to use in convergence check. Default is the maximum norm.\n",
      "line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "    Which type of a line search to use to determine the step size in\n",
      "    the direction given by the Jacobian approximation. Defaults to\n",
      "    'armijo'.\n",
      "jac_options : dict, optional\n",
      "    Options for the respective Jacobian approximation.\n",
      "\n",
      "    alpha : float, optional\n",
      "        Initial guess for the Jacobian is (-1/alpha).\n",
      "    reduction_method : str or tuple, optional\n",
      "        Method used in ensuring that the rank of the Broyden\n",
      "        matrix stays low. Can either be a string giving the\n",
      "        name of the method, or a tuple of the form ``(method,\n",
      "        param1, param2, ...)`` that gives the name of the\n",
      "        method and values for additional parameters.\n",
      "\n",
      "        Methods available:\n",
      "\n",
      "            - ``restart``\n",
      "                Drop all matrix columns. Has no\n",
      "                extra parameters.\n",
      "            - ``simple``\n",
      "                Drop oldest matrix column. Has no\n",
      "                extra parameters.\n",
      "            - ``svd``\n",
      "                Keep only the most significant SVD\n",
      "                components.\n",
      "\n",
      "                Extra parameters:\n",
      "\n",
      "                    - ``to_retain``\n",
      "                        Number of SVD components to\n",
      "                        retain when rank reduction is done.\n",
      "                        Default is ``max_rank - 2``.\n",
      "    max_rank : int, optional\n",
      "        Maximum rank for the Broyden matrix.\n",
      "        Default is infinity (i.e., no rank reduction).\n",
      "\n",
      "anderson\n",
      "========\n",
      "\n",
      "Options\n",
      "-------\n",
      "nit : int, optional\n",
      "    Number of iterations to make. If omitted (default), make as many\n",
      "    as required to meet tolerances.\n",
      "disp : bool, optional\n",
      "    Print status to stdout on every iteration.\n",
      "maxiter : int, optional\n",
      "    Maximum number of iterations to make. If more are needed to\n",
      "    meet convergence, `NoConvergence` is raised.\n",
      "ftol : float, optional\n",
      "    Relative tolerance for the residual. If omitted, not used.\n",
      "fatol : float, optional\n",
      "    Absolute tolerance (in max-norm) for the residual.\n",
      "    If omitted, default is 6e-6.\n",
      "xtol : float, optional\n",
      "    Relative minimum step size. If omitted, not used.\n",
      "xatol : float, optional\n",
      "    Absolute minimum step size, as determined from the Jacobian\n",
      "    approximation. If the step size is smaller than this, optimization\n",
      "    is terminated as successful. If omitted, not used.\n",
      "tol_norm : function(vector) -> scalar, optional\n",
      "    Norm to use in convergence check. Default is the maximum norm.\n",
      "line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "    Which type of a line search to use to determine the step size in\n",
      "    the direction given by the Jacobian approximation. Defaults to\n",
      "    'armijo'.\n",
      "jac_options : dict, optional\n",
      "    Options for the respective Jacobian approximation.\n",
      "\n",
      "    alpha : float, optional\n",
      "        Initial guess for the Jacobian is (-1/alpha).\n",
      "    M : float, optional\n",
      "        Number of previous vectors to retain. Defaults to 5.\n",
      "    w0 : float, optional\n",
      "        Regularization parameter for numerical stability.\n",
      "        Compared to unity, good values of the order of 0.01.\n",
      "\n",
      "diagbroyden\n",
      "===========\n",
      "\n",
      "Options\n",
      "-------\n",
      "nit : int, optional\n",
      "    Number of iterations to make. If omitted (default), make as many\n",
      "    as required to meet tolerances.\n",
      "disp : bool, optional\n",
      "    Print status to stdout on every iteration.\n",
      "maxiter : int, optional\n",
      "    Maximum number of iterations to make. If more are needed to\n",
      "    meet convergence, `NoConvergence` is raised.\n",
      "ftol : float, optional\n",
      "    Relative tolerance for the residual. If omitted, not used.\n",
      "fatol : float, optional\n",
      "    Absolute tolerance (in max-norm) for the residual.\n",
      "    If omitted, default is 6e-6.\n",
      "xtol : float, optional\n",
      "    Relative minimum step size. If omitted, not used.\n",
      "xatol : float, optional\n",
      "    Absolute minimum step size, as determined from the Jacobian\n",
      "    approximation. If the step size is smaller than this, optimization\n",
      "    is terminated as successful. If omitted, not used.\n",
      "tol_norm : function(vector) -> scalar, optional\n",
      "    Norm to use in convergence check. Default is the maximum norm.\n",
      "line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "    Which type of a line search to use to determine the step size in\n",
      "    the direction given by the Jacobian approximation. Defaults to\n",
      "    'armijo'.\n",
      "jac_options : dict, optional\n",
      "    Options for the respective Jacobian approximation.\n",
      "\n",
      "    alpha : float, optional\n",
      "        initial guess for the jacobian is (-1/alpha).\n",
      "\n",
      "excitingmixing\n",
      "==============\n",
      "\n",
      "Options\n",
      "-------\n",
      "nit : int, optional\n",
      "    Number of iterations to make. If omitted (default), make as many\n",
      "    as required to meet tolerances.\n",
      "disp : bool, optional\n",
      "    Print status to stdout on every iteration.\n",
      "maxiter : int, optional\n",
      "    Maximum number of iterations to make. If more are needed to\n",
      "    meet convergence, `NoConvergence` is raised.\n",
      "ftol : float, optional\n",
      "    Relative tolerance for the residual. If omitted, not used.\n",
      "fatol : float, optional\n",
      "    Absolute tolerance (in max-norm) for the residual.\n",
      "    If omitted, default is 6e-6.\n",
      "xtol : float, optional\n",
      "    Relative minimum step size. If omitted, not used.\n",
      "xatol : float, optional\n",
      "    Absolute minimum step size, as determined from the Jacobian\n",
      "    approximation. If the step size is smaller than this, optimization\n",
      "    is terminated as successful. If omitted, not used.\n",
      "tol_norm : function(vector) -> scalar, optional\n",
      "    Norm to use in convergence check. Default is the maximum norm.\n",
      "line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "    Which type of a line search to use to determine the step size in\n",
      "    the direction given by the Jacobian approximation. Defaults to\n",
      "    'armijo'.\n",
      "jac_options : dict, optional\n",
      "    Options for the respective Jacobian approximation.\n",
      "\n",
      "    alpha : float, optional\n",
      "        Initial Jacobian approximation is (-1/alpha).\n",
      "    alphamax : float, optional\n",
      "        The entries of the diagonal Jacobian are kept in the range\n",
      "        ``[alpha, alphamax]``.\n",
      "\n",
      "linearmixing\n",
      "============\n",
      "\n",
      "Options\n",
      "-------\n",
      "nit : int, optional\n",
      "    Number of iterations to make. If omitted (default), make as many\n",
      "    as required to meet tolerances.\n",
      "disp : bool, optional\n",
      "    Print status to stdout on every iteration.\n",
      "maxiter : int, optional\n",
      "    Maximum number of iterations to make. If more are needed to\n",
      "    meet convergence, ``NoConvergence`` is raised.\n",
      "ftol : float, optional\n",
      "    Relative tolerance for the residual. If omitted, not used.\n",
      "fatol : float, optional\n",
      "    Absolute tolerance (in max-norm) for the residual.\n",
      "    If omitted, default is 6e-6.\n",
      "xtol : float, optional\n",
      "    Relative minimum step size. If omitted, not used.\n",
      "xatol : float, optional\n",
      "    Absolute minimum step size, as determined from the Jacobian\n",
      "    approximation. If the step size is smaller than this, optimization\n",
      "    is terminated as successful. If omitted, not used.\n",
      "tol_norm : function(vector) -> scalar, optional\n",
      "    Norm to use in convergence check. Default is the maximum norm.\n",
      "line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "    Which type of a line search to use to determine the step size in\n",
      "    the direction given by the Jacobian approximation. Defaults to\n",
      "    'armijo'.\n",
      "jac_options : dict, optional\n",
      "    Options for the respective Jacobian approximation.\n",
      "\n",
      "    alpha : float, optional\n",
      "        initial guess for the jacobian is (-1/alpha).\n",
      "\n",
      "krylov\n",
      "======\n",
      "\n",
      "Options\n",
      "-------\n",
      "nit : int, optional\n",
      "    Number of iterations to make. If omitted (default), make as many\n",
      "    as required to meet tolerances.\n",
      "disp : bool, optional\n",
      "    Print status to stdout on every iteration.\n",
      "maxiter : int, optional\n",
      "    Maximum number of iterations to make. If more are needed to\n",
      "    meet convergence, `NoConvergence` is raised.\n",
      "ftol : float, optional\n",
      "    Relative tolerance for the residual. If omitted, not used.\n",
      "fatol : float, optional\n",
      "    Absolute tolerance (in max-norm) for the residual.\n",
      "    If omitted, default is 6e-6.\n",
      "xtol : float, optional\n",
      "    Relative minimum step size. If omitted, not used.\n",
      "xatol : float, optional\n",
      "    Absolute minimum step size, as determined from the Jacobian\n",
      "    approximation. If the step size is smaller than this, optimization\n",
      "    is terminated as successful. If omitted, not used.\n",
      "tol_norm : function(vector) -> scalar, optional\n",
      "    Norm to use in convergence check. Default is the maximum norm.\n",
      "line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "    Which type of a line search to use to determine the step size in\n",
      "    the direction given by the Jacobian approximation. Defaults to\n",
      "    'armijo'.\n",
      "jac_options : dict, optional\n",
      "    Options for the respective Jacobian approximation.\n",
      "\n",
      "    rdiff : float, optional\n",
      "        Relative step size to use in numerical differentiation.\n",
      "    method : str or callable, optional\n",
      "        Krylov method to use to approximate the Jacobian.  Can be a string,\n",
      "        or a function implementing the same interface as the iterative\n",
      "        solvers in `scipy.sparse.linalg`. If a string, needs to be one of:\n",
      "        ``'lgmres'``, ``'gmres'``, ``'bicgstab'``, ``'cgs'``, ``'minres'``,\n",
      "        ``'tfqmr'``.\n",
      "\n",
      "        The default is `scipy.sparse.linalg.lgmres`.\n",
      "    inner_M : LinearOperator or InverseJacobian\n",
      "        Preconditioner for the inner Krylov iteration.\n",
      "        Note that you can use also inverse Jacobians as (adaptive)\n",
      "        preconditioners. For example,\n",
      "\n",
      "        >>> jac = BroydenFirst()\n",
      "        >>> kjac = KrylovJacobian(inner_M=jac.inverse).\n",
      "\n",
      "        If the preconditioner has a method named 'update', it will\n",
      "        be called as ``update(x, f)`` after each nonlinear step,\n",
      "        with ``x`` giving the current point, and ``f`` the current\n",
      "        function value.\n",
      "    inner_tol, inner_maxiter, ...\n",
      "        Parameters to pass on to the \"inner\" Krylov solver.\n",
      "        See `scipy.sparse.linalg.gmres` for details.\n",
      "    outer_k : int, optional\n",
      "        Size of the subspace kept across LGMRES nonlinear\n",
      "        iterations.\n",
      "\n",
      "        See `scipy.sparse.linalg.lgmres` for details.\n",
      "\n",
      "df-sane\n",
      "=======\n",
      "\n",
      "Solve nonlinear equation with the DF-SANE method\n",
      "\n",
      "Options\n",
      "-------\n",
      "ftol : float, optional\n",
      "    Relative norm tolerance.\n",
      "fatol : float, optional\n",
      "    Absolute norm tolerance.\n",
      "    Algorithm terminates when ``||func(x)|| < fatol + ftol ||func(x_0)||``.\n",
      "fnorm : callable, optional\n",
      "    Norm to use in the convergence check. If None, 2-norm is used.\n",
      "maxfev : int, optional\n",
      "    Maximum number of function evaluations.\n",
      "disp : bool, optional\n",
      "    Whether to print convergence process to stdout.\n",
      "eta_strategy : callable, optional\n",
      "    Choice of the ``eta_k`` parameter, which gives slack for growth\n",
      "    of ``||F||**2``.  Called as ``eta_k = eta_strategy(k, x, F)`` with\n",
      "    `k` the iteration number, `x` the current iterate and `F` the current\n",
      "    residual. Should satisfy ``eta_k > 0`` and ``sum(eta, k=0..inf) < inf``.\n",
      "    Default: ``||F||**2 / (1 + k)**2``.\n",
      "sigma_eps : float, optional\n",
      "    The spectral coefficient is constrained to ``sigma_eps < sigma < 1/sigma_eps``.\n",
      "    Default: 1e-10\n",
      "sigma_0 : float, optional\n",
      "    Initial spectral coefficient.\n",
      "    Default: 1.0\n",
      "M : int, optional\n",
      "    Number of iterates to include in the nonmonotonic line search.\n",
      "    Default: 10\n",
      "line_search : {'cruz', 'cheng'}\n",
      "    Type of line search to employ. 'cruz' is the original one defined in\n",
      "    [Martinez & Raydan. Math. Comp. 75, 1429 (2006)], 'cheng' is\n",
      "    a modified search defined in [Cheng & Li. IMA J. Numer. Anal. 29, 814 (2009)].\n",
      "    Default: 'cruz'\n",
      "\n",
      "References\n",
      "----------\n",
      ".. [1] \"Spectral residual method without gradient information for solving\n",
      "       large-scale nonlinear systems of equations.\" W. La Cruz,\n",
      "       J.M. Martinez, M. Raydan. Math. Comp. **75**, 1429 (2006).\n",
      ".. [2] W. La Cruz, Opt. Meth. Software, 29, 24 (2014).\n",
      ".. [3] W. Cheng, D.-H. Li. IMA J. Numer. Anal. **29**, 814 (2009).\n",
      "\n",
      "\n",
      "=======\n",
      "linprog\n",
      "=======\n",
      "\n",
      "\n",
      "simplex\n",
      "=======\n",
      "\n",
      "Linear programming: minimize a linear objective function subject to linear\n",
      "equality and inequality constraints using the tableau-based simplex method.\n",
      "\n",
      ".. deprecated:: 1.9.0\n",
      "    `method='simplex'` will be removed in SciPy 1.11.0.\n",
      "    It is replaced by `method='highs'` because the latter is\n",
      "    faster and more robust.\n",
      "\n",
      "Linear programming solves problems of the following form:\n",
      "\n",
      ".. math::\n",
      "\n",
      "    \\min_x \\ & c^T x \\\\\n",
      "    \\mbox{such that} \\ & A_{ub} x \\leq b_{ub},\\\\\n",
      "    & A_{eq} x = b_{eq},\\\\\n",
      "    & l \\leq x \\leq u ,\n",
      "\n",
      "where :math:`x` is a vector of decision variables; :math:`c`,\n",
      ":math:`b_{ub}`, :math:`b_{eq}`, :math:`l`, and :math:`u` are vectors; and\n",
      ":math:`A_{ub}` and :math:`A_{eq}` are matrices.\n",
      "\n",
      "Alternatively, that's:\n",
      "\n",
      "minimize::\n",
      "\n",
      "    c @ x\n",
      "\n",
      "such that::\n",
      "\n",
      "    A_ub @ x <= b_ub\n",
      "    A_eq @ x == b_eq\n",
      "    lb <= x <= ub\n",
      "\n",
      "Note that by default ``lb = 0`` and ``ub = None`` unless specified with\n",
      "``bounds``.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "c : 1-D array\n",
      "    The coefficients of the linear objective function to be minimized.\n",
      "A_ub : 2-D array, optional\n",
      "    The inequality constraint matrix. Each row of ``A_ub`` specifies the\n",
      "    coefficients of a linear inequality constraint on ``x``.\n",
      "b_ub : 1-D array, optional\n",
      "    The inequality constraint vector. Each element represents an\n",
      "    upper bound on the corresponding value of ``A_ub @ x``.\n",
      "A_eq : 2-D array, optional\n",
      "    The equality constraint matrix. Each row of ``A_eq`` specifies the\n",
      "    coefficients of a linear equality constraint on ``x``.\n",
      "b_eq : 1-D array, optional\n",
      "    The equality constraint vector. Each element of ``A_eq @ x`` must equal\n",
      "    the corresponding element of ``b_eq``.\n",
      "bounds : sequence, optional\n",
      "    A sequence of ``(min, max)`` pairs for each element in ``x``, defining\n",
      "    the minimum and maximum values of that decision variable. Use ``None``\n",
      "    to indicate that there is no bound. By default, bounds are\n",
      "    ``(0, None)`` (all decision variables are non-negative).\n",
      "    If a single tuple ``(min, max)`` is provided, then ``min`` and\n",
      "    ``max`` will serve as bounds for all decision variables.\n",
      "method : str\n",
      "    This is the method-specific documentation for 'simplex'.\n",
      "    :ref:`'highs' <optimize.linprog-highs>`,\n",
      "    :ref:`'highs-ds' <optimize.linprog-highs-ds>`,\n",
      "    :ref:`'highs-ipm' <optimize.linprog-highs-ipm>`,\n",
      "    :ref:`'interior-point' <optimize.linprog-interior-point>` (default),\n",
      "    and :ref:`'revised simplex' <optimize.linprog-revised_simplex>`\n",
      "    are also available.\n",
      "callback : callable, optional\n",
      "    Callback function to be executed once per iteration.\n",
      "\n",
      "Options\n",
      "-------\n",
      "maxiter : int (default: 5000)\n",
      "   The maximum number of iterations to perform in either phase.\n",
      "disp : bool (default: False)\n",
      "    Set to ``True`` if indicators of optimization status are to be printed\n",
      "    to the console each iteration.\n",
      "presolve : bool (default: True)\n",
      "    Presolve attempts to identify trivial infeasibilities,\n",
      "    identify trivial unboundedness, and simplify the problem before\n",
      "    sending it to the main solver. It is generally recommended\n",
      "    to keep the default setting ``True``; set to ``False`` if\n",
      "    presolve is to be disabled.\n",
      "tol : float (default: 1e-12)\n",
      "    The tolerance which determines when a solution is \"close enough\" to\n",
      "    zero in Phase 1 to be considered a basic feasible solution or close\n",
      "    enough to positive to serve as an optimal solution.\n",
      "autoscale : bool (default: False)\n",
      "    Set to ``True`` to automatically perform equilibration.\n",
      "    Consider using this option if the numerical values in the\n",
      "    constraints are separated by several orders of magnitude.\n",
      "rr : bool (default: True)\n",
      "    Set to ``False`` to disable automatic redundancy removal.\n",
      "bland : bool\n",
      "    If True, use Bland's anti-cycling rule [3]_ to choose pivots to\n",
      "    prevent cycling. If False, choose pivots which should lead to a\n",
      "    converged solution more quickly. The latter method is subject to\n",
      "    cycling (non-convergence) in rare instances.\n",
      "unknown_options : dict\n",
      "    Optional arguments not used by this particular solver. If\n",
      "    `unknown_options` is non-empty a warning is issued listing all\n",
      "    unused options.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "res : OptimizeResult\n",
      "    A :class:`scipy.optimize.OptimizeResult` consisting of the fields:\n",
      "\n",
      "    x : 1-D array\n",
      "        The values of the decision variables that minimizes the\n",
      "        objective function while satisfying the constraints.\n",
      "    fun : float\n",
      "        The optimal value of the objective function ``c @ x``.\n",
      "    slack : 1-D array\n",
      "        The (nominally positive) values of the slack variables,\n",
      "        ``b_ub - A_ub @ x``.\n",
      "    con : 1-D array\n",
      "        The (nominally zero) residuals of the equality constraints,\n",
      "        ``b_eq - A_eq @ x``.\n",
      "    success : bool\n",
      "        ``True`` when the algorithm succeeds in finding an optimal\n",
      "        solution.\n",
      "    status : int\n",
      "        An integer representing the exit status of the algorithm.\n",
      "\n",
      "        ``0`` : Optimization terminated successfully.\n",
      "\n",
      "        ``1`` : Iteration limit reached.\n",
      "\n",
      "        ``2`` : Problem appears to be infeasible.\n",
      "\n",
      "        ``3`` : Problem appears to be unbounded.\n",
      "\n",
      "        ``4`` : Numerical difficulties encountered.\n",
      "\n",
      "    message : str\n",
      "        A string descriptor of the exit status of the algorithm.\n",
      "    nit : int\n",
      "        The total number of iterations performed in all phases.\n",
      "\n",
      "References\n",
      "----------\n",
      ".. [1] Dantzig, George B., Linear programming and extensions. Rand\n",
      "       Corporation Research Study Princeton Univ. Press, Princeton, NJ,\n",
      "       1963\n",
      ".. [2] Hillier, S.H. and Lieberman, G.J. (1995), \"Introduction to\n",
      "       Mathematical Programming\", McGraw-Hill, Chapter 4.\n",
      ".. [3] Bland, Robert G. New finite pivoting rules for the simplex method.\n",
      "       Mathematics of Operations Research (2), 1977: pp. 103-107.\n",
      "\n",
      "interior-point\n",
      "==============\n",
      "\n",
      "Linear programming: minimize a linear objective function subject to linear\n",
      "equality and inequality constraints using the interior-point method of\n",
      "[4]_.\n",
      "\n",
      ".. deprecated:: 1.9.0\n",
      "    `method='interior-point'` will be removed in SciPy 1.11.0.\n",
      "    It is replaced by `method='highs'` because the latter is\n",
      "    faster and more robust.\n",
      "\n",
      "Linear programming solves problems of the following form:\n",
      "\n",
      ".. math::\n",
      "\n",
      "    \\min_x \\ & c^T x \\\\\n",
      "    \\mbox{such that} \\ & A_{ub} x \\leq b_{ub},\\\\\n",
      "    & A_{eq} x = b_{eq},\\\\\n",
      "    & l \\leq x \\leq u ,\n",
      "\n",
      "where :math:`x` is a vector of decision variables; :math:`c`,\n",
      ":math:`b_{ub}`, :math:`b_{eq}`, :math:`l`, and :math:`u` are vectors; and\n",
      ":math:`A_{ub}` and :math:`A_{eq}` are matrices.\n",
      "\n",
      "Alternatively, that's:\n",
      "\n",
      "minimize::\n",
      "\n",
      "    c @ x\n",
      "\n",
      "such that::\n",
      "\n",
      "    A_ub @ x <= b_ub\n",
      "    A_eq @ x == b_eq\n",
      "    lb <= x <= ub\n",
      "\n",
      "Note that by default ``lb = 0`` and ``ub = None`` unless specified with\n",
      "``bounds``.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "c : 1-D array\n",
      "    The coefficients of the linear objective function to be minimized.\n",
      "A_ub : 2-D array, optional\n",
      "    The inequality constraint matrix. Each row of ``A_ub`` specifies the\n",
      "    coefficients of a linear inequality constraint on ``x``.\n",
      "b_ub : 1-D array, optional\n",
      "    The inequality constraint vector. Each element represents an\n",
      "    upper bound on the corresponding value of ``A_ub @ x``.\n",
      "A_eq : 2-D array, optional\n",
      "    The equality constraint matrix. Each row of ``A_eq`` specifies the\n",
      "    coefficients of a linear equality constraint on ``x``.\n",
      "b_eq : 1-D array, optional\n",
      "    The equality constraint vector. Each element of ``A_eq @ x`` must equal\n",
      "    the corresponding element of ``b_eq``.\n",
      "bounds : sequence, optional\n",
      "    A sequence of ``(min, max)`` pairs for each element in ``x``, defining\n",
      "    the minimum and maximum values of that decision variable. Use ``None``\n",
      "    to indicate that there is no bound. By default, bounds are\n",
      "    ``(0, None)`` (all decision variables are non-negative).\n",
      "    If a single tuple ``(min, max)`` is provided, then ``min`` and\n",
      "    ``max`` will serve as bounds for all decision variables.\n",
      "method : str\n",
      "    This is the method-specific documentation for 'interior-point'.\n",
      "    :ref:`'highs' <optimize.linprog-highs>`,\n",
      "    :ref:`'highs-ds' <optimize.linprog-highs-ds>`,\n",
      "    :ref:`'highs-ipm' <optimize.linprog-highs-ipm>`,\n",
      "    :ref:`'revised simplex' <optimize.linprog-revised_simplex>`, and\n",
      "    :ref:`'simplex' <optimize.linprog-simplex>` (legacy)\n",
      "    are also available.\n",
      "callback : callable, optional\n",
      "    Callback function to be executed once per iteration.\n",
      "\n",
      "Options\n",
      "-------\n",
      "maxiter : int (default: 1000)\n",
      "    The maximum number of iterations of the algorithm.\n",
      "disp : bool (default: False)\n",
      "    Set to ``True`` if indicators of optimization status are to be printed\n",
      "    to the console each iteration.\n",
      "presolve : bool (default: True)\n",
      "    Presolve attempts to identify trivial infeasibilities,\n",
      "    identify trivial unboundedness, and simplify the problem before\n",
      "    sending it to the main solver. It is generally recommended\n",
      "    to keep the default setting ``True``; set to ``False`` if\n",
      "    presolve is to be disabled.\n",
      "tol : float (default: 1e-8)\n",
      "    Termination tolerance to be used for all termination criteria;\n",
      "    see [4]_ Section 4.5.\n",
      "autoscale : bool (default: False)\n",
      "    Set to ``True`` to automatically perform equilibration.\n",
      "    Consider using this option if the numerical values in the\n",
      "    constraints are separated by several orders of magnitude.\n",
      "rr : bool (default: True)\n",
      "    Set to ``False`` to disable automatic redundancy removal.\n",
      "alpha0 : float (default: 0.99995)\n",
      "    The maximal step size for Mehrota's predictor-corrector search\n",
      "    direction; see :math:`\\beta_{3}` of [4]_ Table 8.1.\n",
      "beta : float (default: 0.1)\n",
      "    The desired reduction of the path parameter :math:`\\mu` (see [6]_)\n",
      "    when Mehrota's predictor-corrector is not in use (uncommon).\n",
      "sparse : bool (default: False)\n",
      "    Set to ``True`` if the problem is to be treated as sparse after\n",
      "    presolve. If either ``A_eq`` or ``A_ub`` is a sparse matrix,\n",
      "    this option will automatically be set ``True``, and the problem\n",
      "    will be treated as sparse even during presolve. If your constraint\n",
      "    matrices contain mostly zeros and the problem is not very small (less\n",
      "    than about 100 constraints or variables), consider setting ``True``\n",
      "    or providing ``A_eq`` and ``A_ub`` as sparse matrices.\n",
      "lstsq : bool (default: ``False``)\n",
      "    Set to ``True`` if the problem is expected to be very poorly\n",
      "    conditioned. This should always be left ``False`` unless severe\n",
      "    numerical difficulties are encountered. Leave this at the default\n",
      "    unless you receive a warning message suggesting otherwise.\n",
      "sym_pos : bool (default: True)\n",
      "    Leave ``True`` if the problem is expected to yield a well conditioned\n",
      "    symmetric positive definite normal equation matrix\n",
      "    (almost always). Leave this at the default unless you receive\n",
      "    a warning message suggesting otherwise.\n",
      "cholesky : bool (default: True)\n",
      "    Set to ``True`` if the normal equations are to be solved by explicit\n",
      "    Cholesky decomposition followed by explicit forward/backward\n",
      "    substitution. This is typically faster for problems\n",
      "    that are numerically well-behaved.\n",
      "pc : bool (default: True)\n",
      "    Leave ``True`` if the predictor-corrector method of Mehrota is to be\n",
      "    used. This is almost always (if not always) beneficial.\n",
      "ip : bool (default: False)\n",
      "    Set to ``True`` if the improved initial point suggestion due to [4]_\n",
      "    Section 4.3 is desired. Whether this is beneficial or not\n",
      "    depends on the problem.\n",
      "permc_spec : str (default: 'MMD_AT_PLUS_A')\n",
      "    (Has effect only with ``sparse = True``, ``lstsq = False``, ``sym_pos =\n",
      "    True``, and no SuiteSparse.)\n",
      "    A matrix is factorized in each iteration of the algorithm.\n",
      "    This option specifies how to permute the columns of the matrix for\n",
      "    sparsity preservation. Acceptable values are:\n",
      "\n",
      "    - ``NATURAL``: natural ordering.\n",
      "    - ``MMD_ATA``: minimum degree ordering on the structure of A^T A.\n",
      "    - ``MMD_AT_PLUS_A``: minimum degree ordering on the structure of A^T+A.\n",
      "    - ``COLAMD``: approximate minimum degree column ordering.\n",
      "\n",
      "    This option can impact the convergence of the\n",
      "    interior point algorithm; test different values to determine which\n",
      "    performs best for your problem. For more information, refer to\n",
      "    ``scipy.sparse.linalg.splu``.\n",
      "unknown_options : dict\n",
      "    Optional arguments not used by this particular solver. If\n",
      "    `unknown_options` is non-empty a warning is issued listing all\n",
      "    unused options.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "res : OptimizeResult\n",
      "    A :class:`scipy.optimize.OptimizeResult` consisting of the fields:\n",
      "\n",
      "    x : 1-D array\n",
      "        The values of the decision variables that minimizes the\n",
      "        objective function while satisfying the constraints.\n",
      "    fun : float\n",
      "        The optimal value of the objective function ``c @ x``.\n",
      "    slack : 1-D array\n",
      "        The (nominally positive) values of the slack variables,\n",
      "        ``b_ub - A_ub @ x``.\n",
      "    con : 1-D array\n",
      "        The (nominally zero) residuals of the equality constraints,\n",
      "        ``b_eq - A_eq @ x``.\n",
      "    success : bool\n",
      "        ``True`` when the algorithm succeeds in finding an optimal\n",
      "        solution.\n",
      "    status : int\n",
      "        An integer representing the exit status of the algorithm.\n",
      "\n",
      "        ``0`` : Optimization terminated successfully.\n",
      "\n",
      "        ``1`` : Iteration limit reached.\n",
      "\n",
      "        ``2`` : Problem appears to be infeasible.\n",
      "\n",
      "        ``3`` : Problem appears to be unbounded.\n",
      "\n",
      "        ``4`` : Numerical difficulties encountered.\n",
      "\n",
      "    message : str\n",
      "        A string descriptor of the exit status of the algorithm.\n",
      "    nit : int\n",
      "        The total number of iterations performed in all phases.\n",
      "\n",
      "\n",
      "Notes\n",
      "-----\n",
      "This method implements the algorithm outlined in [4]_ with ideas from [8]_\n",
      "and a structure inspired by the simpler methods of [6]_.\n",
      "\n",
      "The primal-dual path following method begins with initial 'guesses' of\n",
      "the primal and dual variables of the standard form problem and iteratively\n",
      "attempts to solve the (nonlinear) Karush-Kuhn-Tucker conditions for the\n",
      "problem with a gradually reduced logarithmic barrier term added to the\n",
      "objective. This particular implementation uses a homogeneous self-dual\n",
      "formulation, which provides certificates of infeasibility or unboundedness\n",
      "where applicable.\n",
      "\n",
      "The default initial point for the primal and dual variables is that\n",
      "defined in [4]_ Section 4.4 Equation 8.22. Optionally (by setting initial\n",
      "point option ``ip=True``), an alternate (potentially improved) starting\n",
      "point can be calculated according to the additional recommendations of\n",
      "[4]_ Section 4.4.\n",
      "\n",
      "A search direction is calculated using the predictor-corrector method\n",
      "(single correction) proposed by Mehrota and detailed in [4]_ Section 4.1.\n",
      "(A potential improvement would be to implement the method of multiple\n",
      "corrections described in [4]_ Section 4.2.) In practice, this is\n",
      "accomplished by solving the normal equations, [4]_ Section 5.1 Equations\n",
      "8.31 and 8.32, derived from the Newton equations [4]_ Section 5 Equations\n",
      "8.25 (compare to [4]_ Section 4 Equations 8.6-8.8). The advantage of\n",
      "solving the normal equations rather than 8.25 directly is that the\n",
      "matrices involved are symmetric positive definite, so Cholesky\n",
      "decomposition can be used rather than the more expensive LU factorization.\n",
      "\n",
      "With default options, the solver used to perform the factorization depends\n",
      "on third-party software availability and the conditioning of the problem.\n",
      "\n",
      "For dense problems, solvers are tried in the following order:\n",
      "\n",
      "1. ``scipy.linalg.cho_factor``\n",
      "\n",
      "2. ``scipy.linalg.solve`` with option ``sym_pos=True``\n",
      "\n",
      "3. ``scipy.linalg.solve`` with option ``sym_pos=False``\n",
      "\n",
      "4. ``scipy.linalg.lstsq``\n",
      "\n",
      "For sparse problems:\n",
      "\n",
      "1. ``sksparse.cholmod.cholesky`` (if scikit-sparse and SuiteSparse are\n",
      "   installed)\n",
      "\n",
      "2. ``scipy.sparse.linalg.factorized`` (if scikit-umfpack and SuiteSparse\n",
      "   are installed)\n",
      "\n",
      "3. ``scipy.sparse.linalg.splu`` (which uses SuperLU distributed with SciPy)\n",
      "\n",
      "4. ``scipy.sparse.linalg.lsqr``\n",
      "\n",
      "If the solver fails for any reason, successively more robust (but slower)\n",
      "solvers are attempted in the order indicated. Attempting, failing, and\n",
      "re-starting factorization can be time consuming, so if the problem is\n",
      "numerically challenging, options can be set to  bypass solvers that are\n",
      "failing. Setting ``cholesky=False`` skips to solver 2,\n",
      "``sym_pos=False`` skips to solver 3, and ``lstsq=True`` skips\n",
      "to solver 4 for both sparse and dense problems.\n",
      "\n",
      "Potential improvements for combatting issues associated with dense\n",
      "columns in otherwise sparse problems are outlined in [4]_ Section 5.3 and\n",
      "[10]_ Section 4.1-4.2; the latter also discusses the alleviation of\n",
      "accuracy issues associated with the substitution approach to free\n",
      "variables.\n",
      "\n",
      "After calculating the search direction, the maximum possible step size\n",
      "that does not activate the non-negativity constraints is calculated, and\n",
      "the smaller of this step size and unity is applied (as in [4]_ Section\n",
      "4.1.) [4]_ Section 4.3 suggests improvements for choosing the step size.\n",
      "\n",
      "The new point is tested according to the termination conditions of [4]_\n",
      "Section 4.5. The same tolerance, which can be set using the ``tol`` option,\n",
      "is used for all checks. (A potential improvement would be to expose\n",
      "the different tolerances to be set independently.) If optimality,\n",
      "unboundedness, or infeasibility is detected, the solve procedure\n",
      "terminates; otherwise it repeats.\n",
      "\n",
      "Whereas the top level ``linprog`` module expects a problem of form:\n",
      "\n",
      "Minimize::\n",
      "\n",
      "    c @ x\n",
      "\n",
      "Subject to::\n",
      "\n",
      "    A_ub @ x <= b_ub\n",
      "    A_eq @ x == b_eq\n",
      "     lb <= x <= ub\n",
      "\n",
      "where ``lb = 0`` and ``ub = None`` unless set in ``bounds``. The problem\n",
      "is automatically converted to the form:\n",
      "\n",
      "Minimize::\n",
      "\n",
      "    c @ x\n",
      "\n",
      "Subject to::\n",
      "\n",
      "    A @ x == b\n",
      "        x >= 0\n",
      "\n",
      "for solution. That is, the original problem contains equality, upper-bound\n",
      "and variable constraints whereas the method specific solver requires\n",
      "equality constraints and variable non-negativity. ``linprog`` converts the\n",
      "original problem to standard form by converting the simple bounds to upper\n",
      "bound constraints, introducing non-negative slack variables for inequality\n",
      "constraints, and expressing unbounded variables as the difference between\n",
      "two non-negative variables. The problem is converted back to the original\n",
      "form before results are reported.\n",
      "\n",
      "References\n",
      "----------\n",
      ".. [4] Andersen, Erling D., and Knud D. Andersen. \"The MOSEK interior point\n",
      "       optimizer for linear programming: an implementation of the\n",
      "       homogeneous algorithm.\" High performance optimization. Springer US,\n",
      "       2000. 197-232.\n",
      ".. [6] Freund, Robert M. \"Primal-Dual Interior-Point Methods for Linear\n",
      "       Programming based on Newton's Method.\" Unpublished Course Notes,\n",
      "       March 2004. Available 2/25/2017 at\n",
      "       https://ocw.mit.edu/courses/sloan-school-of-management/15-084j-nonlinear-programming-spring-2004/lecture-notes/lec14_int_pt_mthd.pdf\n",
      ".. [8] Andersen, Erling D., and Knud D. Andersen. \"Presolving in linear\n",
      "       programming.\" Mathematical Programming 71.2 (1995): 221-245.\n",
      ".. [9] Bertsimas, Dimitris, and J. Tsitsiklis. \"Introduction to linear\n",
      "       programming.\" Athena Scientific 1 (1997): 997.\n",
      ".. [10] Andersen, Erling D., et al. Implementation of interior point\n",
      "        methods for large scale linear programming. HEC/Universite de\n",
      "        Geneve, 1996.\n",
      "\n",
      "revised simplex\n",
      "===============\n",
      "\n",
      "Linear programming: minimize a linear objective function subject to linear\n",
      "equality and inequality constraints using the revised simplex method.\n",
      "\n",
      ".. deprecated:: 1.9.0\n",
      "    `method='revised simplex'` will be removed in SciPy 1.11.0.\n",
      "    It is replaced by `method='highs'` because the latter is\n",
      "    faster and more robust.\n",
      "\n",
      "Linear programming solves problems of the following form:\n",
      "\n",
      ".. math::\n",
      "\n",
      "    \\min_x \\ & c^T x \\\\\n",
      "    \\mbox{such that} \\ & A_{ub} x \\leq b_{ub},\\\\\n",
      "    & A_{eq} x = b_{eq},\\\\\n",
      "    & l \\leq x \\leq u ,\n",
      "\n",
      "where :math:`x` is a vector of decision variables; :math:`c`,\n",
      ":math:`b_{ub}`, :math:`b_{eq}`, :math:`l`, and :math:`u` are vectors; and\n",
      ":math:`A_{ub}` and :math:`A_{eq}` are matrices.\n",
      "\n",
      "Alternatively, that's:\n",
      "\n",
      "minimize::\n",
      "\n",
      "    c @ x\n",
      "\n",
      "such that::\n",
      "\n",
      "    A_ub @ x <= b_ub\n",
      "    A_eq @ x == b_eq\n",
      "    lb <= x <= ub\n",
      "\n",
      "Note that by default ``lb = 0`` and ``ub = None`` unless specified with\n",
      "``bounds``.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "c : 1-D array\n",
      "    The coefficients of the linear objective function to be minimized.\n",
      "A_ub : 2-D array, optional\n",
      "    The inequality constraint matrix. Each row of ``A_ub`` specifies the\n",
      "    coefficients of a linear inequality constraint on ``x``.\n",
      "b_ub : 1-D array, optional\n",
      "    The inequality constraint vector. Each element represents an\n",
      "    upper bound on the corresponding value of ``A_ub @ x``.\n",
      "A_eq : 2-D array, optional\n",
      "    The equality constraint matrix. Each row of ``A_eq`` specifies the\n",
      "    coefficients of a linear equality constraint on ``x``.\n",
      "b_eq : 1-D array, optional\n",
      "    The equality constraint vector. Each element of ``A_eq @ x`` must equal\n",
      "    the corresponding element of ``b_eq``.\n",
      "bounds : sequence, optional\n",
      "    A sequence of ``(min, max)`` pairs for each element in ``x``, defining\n",
      "    the minimum and maximum values of that decision variable. Use ``None``\n",
      "    to indicate that there is no bound. By default, bounds are\n",
      "    ``(0, None)`` (all decision variables are non-negative).\n",
      "    If a single tuple ``(min, max)`` is provided, then ``min`` and\n",
      "    ``max`` will serve as bounds for all decision variables.\n",
      "method : str\n",
      "    This is the method-specific documentation for 'revised simplex'.\n",
      "    :ref:`'highs' <optimize.linprog-highs>`,\n",
      "    :ref:`'highs-ds' <optimize.linprog-highs-ds>`,\n",
      "    :ref:`'highs-ipm' <optimize.linprog-highs-ipm>`,\n",
      "    :ref:`'interior-point' <optimize.linprog-interior-point>` (default),\n",
      "    and :ref:`'simplex' <optimize.linprog-simplex>` (legacy)\n",
      "    are also available.\n",
      "callback : callable, optional\n",
      "    Callback function to be executed once per iteration.\n",
      "x0 : 1-D array, optional\n",
      "    Guess values of the decision variables, which will be refined by\n",
      "    the optimization algorithm. This argument is currently used only by the\n",
      "    'revised simplex' method, and can only be used if `x0` represents a\n",
      "    basic feasible solution.\n",
      "\n",
      "Options\n",
      "-------\n",
      "maxiter : int (default: 5000)\n",
      "   The maximum number of iterations to perform in either phase.\n",
      "disp : bool (default: False)\n",
      "    Set to ``True`` if indicators of optimization status are to be printed\n",
      "    to the console each iteration.\n",
      "presolve : bool (default: True)\n",
      "    Presolve attempts to identify trivial infeasibilities,\n",
      "    identify trivial unboundedness, and simplify the problem before\n",
      "    sending it to the main solver. It is generally recommended\n",
      "    to keep the default setting ``True``; set to ``False`` if\n",
      "    presolve is to be disabled.\n",
      "tol : float (default: 1e-12)\n",
      "    The tolerance which determines when a solution is \"close enough\" to\n",
      "    zero in Phase 1 to be considered a basic feasible solution or close\n",
      "    enough to positive to serve as an optimal solution.\n",
      "autoscale : bool (default: False)\n",
      "    Set to ``True`` to automatically perform equilibration.\n",
      "    Consider using this option if the numerical values in the\n",
      "    constraints are separated by several orders of magnitude.\n",
      "rr : bool (default: True)\n",
      "    Set to ``False`` to disable automatic redundancy removal.\n",
      "maxupdate : int (default: 10)\n",
      "    The maximum number of updates performed on the LU factorization.\n",
      "    After this many updates is reached, the basis matrix is factorized\n",
      "    from scratch.\n",
      "mast : bool (default: False)\n",
      "    Minimize Amortized Solve Time. If enabled, the average time to solve\n",
      "    a linear system using the basis factorization is measured. Typically,\n",
      "    the average solve time will decrease with each successive solve after\n",
      "    initial factorization, as factorization takes much more time than the\n",
      "    solve operation (and updates). Eventually, however, the updated\n",
      "    factorization becomes sufficiently complex that the average solve time\n",
      "    begins to increase. When this is detected, the basis is refactorized\n",
      "    from scratch. Enable this option to maximize speed at the risk of\n",
      "    nondeterministic behavior. Ignored if ``maxupdate`` is 0.\n",
      "pivot : \"mrc\" or \"bland\" (default: \"mrc\")\n",
      "    Pivot rule: Minimum Reduced Cost (\"mrc\") or Bland's rule (\"bland\").\n",
      "    Choose Bland's rule if iteration limit is reached and cycling is\n",
      "    suspected.\n",
      "unknown_options : dict\n",
      "    Optional arguments not used by this particular solver. If\n",
      "    `unknown_options` is non-empty a warning is issued listing all\n",
      "    unused options.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "res : OptimizeResult\n",
      "    A :class:`scipy.optimize.OptimizeResult` consisting of the fields:\n",
      "\n",
      "    x : 1-D array\n",
      "        The values of the decision variables that minimizes the\n",
      "        objective function while satisfying the constraints.\n",
      "    fun : float\n",
      "        The optimal value of the objective function ``c @ x``.\n",
      "    slack : 1-D array\n",
      "        The (nominally positive) values of the slack variables,\n",
      "        ``b_ub - A_ub @ x``.\n",
      "    con : 1-D array\n",
      "        The (nominally zero) residuals of the equality constraints,\n",
      "        ``b_eq - A_eq @ x``.\n",
      "    success : bool\n",
      "        ``True`` when the algorithm succeeds in finding an optimal\n",
      "        solution.\n",
      "    status : int\n",
      "        An integer representing the exit status of the algorithm.\n",
      "\n",
      "        ``0`` : Optimization terminated successfully.\n",
      "\n",
      "        ``1`` : Iteration limit reached.\n",
      "\n",
      "        ``2`` : Problem appears to be infeasible.\n",
      "\n",
      "        ``3`` : Problem appears to be unbounded.\n",
      "\n",
      "        ``4`` : Numerical difficulties encountered.\n",
      "\n",
      "        ``5`` : Problem has no constraints; turn presolve on.\n",
      "\n",
      "        ``6`` : Invalid guess provided.\n",
      "\n",
      "    message : str\n",
      "        A string descriptor of the exit status of the algorithm.\n",
      "    nit : int\n",
      "        The total number of iterations performed in all phases.\n",
      "\n",
      "\n",
      "Notes\n",
      "-----\n",
      "Method *revised simplex* uses the revised simplex method as described in\n",
      "[9]_, except that a factorization [11]_ of the basis matrix, rather than\n",
      "its inverse, is efficiently maintained and used to solve the linear systems\n",
      "at each iteration of the algorithm.\n",
      "\n",
      "References\n",
      "----------\n",
      ".. [9] Bertsimas, Dimitris, and J. Tsitsiklis. \"Introduction to linear\n",
      "       programming.\" Athena Scientific 1 (1997): 997.\n",
      ".. [11] Bartels, Richard H. \"A stabilization of the simplex method.\"\n",
      "        Journal in  Numerische Mathematik 16.5 (1971): 414-434.\n",
      "\n",
      "highs-ipm\n",
      "=========\n",
      "\n",
      "Linear programming: minimize a linear objective function subject to linear\n",
      "equality and inequality constraints using the HiGHS interior point solver.\n",
      "\n",
      "Linear programming solves problems of the following form:\n",
      "\n",
      ".. math::\n",
      "\n",
      "    \\min_x \\ & c^T x \\\\\n",
      "    \\mbox{such that} \\ & A_{ub} x \\leq b_{ub},\\\\\n",
      "    & A_{eq} x = b_{eq},\\\\\n",
      "    & l \\leq x \\leq u ,\n",
      "\n",
      "where :math:`x` is a vector of decision variables; :math:`c`,\n",
      ":math:`b_{ub}`, :math:`b_{eq}`, :math:`l`, and :math:`u` are vectors; and\n",
      ":math:`A_{ub}` and :math:`A_{eq}` are matrices.\n",
      "\n",
      "Alternatively, that's:\n",
      "\n",
      "minimize::\n",
      "\n",
      "    c @ x\n",
      "\n",
      "such that::\n",
      "\n",
      "    A_ub @ x <= b_ub\n",
      "    A_eq @ x == b_eq\n",
      "    lb <= x <= ub\n",
      "\n",
      "Note that by default ``lb = 0`` and ``ub = None`` unless specified with\n",
      "``bounds``.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "c : 1-D array\n",
      "    The coefficients of the linear objective function to be minimized.\n",
      "A_ub : 2-D array, optional\n",
      "    The inequality constraint matrix. Each row of ``A_ub`` specifies the\n",
      "    coefficients of a linear inequality constraint on ``x``.\n",
      "b_ub : 1-D array, optional\n",
      "    The inequality constraint vector. Each element represents an\n",
      "    upper bound on the corresponding value of ``A_ub @ x``.\n",
      "A_eq : 2-D array, optional\n",
      "    The equality constraint matrix. Each row of ``A_eq`` specifies the\n",
      "    coefficients of a linear equality constraint on ``x``.\n",
      "b_eq : 1-D array, optional\n",
      "    The equality constraint vector. Each element of ``A_eq @ x`` must equal\n",
      "    the corresponding element of ``b_eq``.\n",
      "bounds : sequence, optional\n",
      "    A sequence of ``(min, max)`` pairs for each element in ``x``, defining\n",
      "    the minimum and maximum values of that decision variable. Use ``None``\n",
      "    to indicate that there is no bound. By default, bounds are\n",
      "    ``(0, None)`` (all decision variables are non-negative).\n",
      "    If a single tuple ``(min, max)`` is provided, then ``min`` and\n",
      "    ``max`` will serve as bounds for all decision variables.\n",
      "method : str\n",
      "\n",
      "    This is the method-specific documentation for 'highs-ipm'.\n",
      "    :ref:`'highs-ipm' <optimize.linprog-highs>`,\n",
      "    :ref:`'highs-ds' <optimize.linprog-highs-ds>`,\n",
      "    :ref:`'interior-point' <optimize.linprog-interior-point>` (default),\n",
      "    :ref:`'revised simplex' <optimize.linprog-revised_simplex>`, and\n",
      "    :ref:`'simplex' <optimize.linprog-simplex>` (legacy)\n",
      "    are also available.\n",
      "\n",
      "Options\n",
      "-------\n",
      "maxiter : int\n",
      "    The maximum number of iterations to perform in either phase.\n",
      "    For :ref:`'highs-ipm' <optimize.linprog-highs-ipm>`, this does not\n",
      "    include the number of crossover iterations. Default is the largest\n",
      "    possible value for an ``int`` on the platform.\n",
      "disp : bool (default: ``False``)\n",
      "    Set to ``True`` if indicators of optimization status are to be\n",
      "    printed to the console during optimization.\n",
      "presolve : bool (default: ``True``)\n",
      "    Presolve attempts to identify trivial infeasibilities,\n",
      "    identify trivial unboundedness, and simplify the problem before\n",
      "    sending it to the main solver. It is generally recommended\n",
      "    to keep the default setting ``True``; set to ``False`` if\n",
      "    presolve is to be disabled.\n",
      "time_limit : float\n",
      "    The maximum time in seconds allotted to solve the problem;\n",
      "    default is the largest possible value for a ``double`` on the\n",
      "    platform.\n",
      "dual_feasibility_tolerance : double (default: 1e-07)\n",
      "    The minimum of this and ``primal_feasibility_tolerance``\n",
      "    is used for the feasibility tolerance of\n",
      "    :ref:`'highs-ipm' <optimize.linprog-highs-ipm>`.\n",
      "primal_feasibility_tolerance : double (default: 1e-07)\n",
      "    The minimum of this and ``dual_feasibility_tolerance``\n",
      "    is used for the feasibility tolerance of\n",
      "    :ref:`'highs-ipm' <optimize.linprog-highs-ipm>`.\n",
      "ipm_optimality_tolerance : double (default: ``1e-08``)\n",
      "    Optimality tolerance for\n",
      "    :ref:`'highs-ipm' <optimize.linprog-highs-ipm>`.\n",
      "    Minimum allowable value is 1e-12.\n",
      "unknown_options : dict\n",
      "    Optional arguments not used by this particular solver. If\n",
      "    ``unknown_options`` is non-empty, a warning is issued listing\n",
      "    all unused options.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "res : OptimizeResult\n",
      "    A :class:`scipy.optimize.OptimizeResult` consisting of the fields:\n",
      "\n",
      "    x : 1D array\n",
      "        The values of the decision variables that minimizes the\n",
      "        objective function while satisfying the constraints.\n",
      "    fun : float\n",
      "        The optimal value of the objective function ``c @ x``.\n",
      "    slack : 1D array\n",
      "        The (nominally positive) values of the slack,\n",
      "        ``b_ub - A_ub @ x``.\n",
      "    con : 1D array\n",
      "        The (nominally zero) residuals of the equality constraints,\n",
      "        ``b_eq - A_eq @ x``.\n",
      "    success : bool\n",
      "        ``True`` when the algorithm succeeds in finding an optimal\n",
      "        solution.\n",
      "    status : int\n",
      "        An integer representing the exit status of the algorithm.\n",
      "\n",
      "        ``0`` : Optimization terminated successfully.\n",
      "\n",
      "        ``1`` : Iteration or time limit reached.\n",
      "\n",
      "        ``2`` : Problem appears to be infeasible.\n",
      "\n",
      "        ``3`` : Problem appears to be unbounded.\n",
      "\n",
      "        ``4`` : The HiGHS solver ran into a problem.\n",
      "\n",
      "    message : str\n",
      "        A string descriptor of the exit status of the algorithm.\n",
      "    nit : int\n",
      "        The total number of iterations performed.\n",
      "        For the HiGHS interior-point method, this does not include\n",
      "        crossover iterations.\n",
      "    crossover_nit : int\n",
      "        The number of primal/dual pushes performed during the\n",
      "        crossover routine for the HiGHS interior-point method.\n",
      "    ineqlin : OptimizeResult\n",
      "        Solution and sensitivity information corresponding to the\n",
      "        inequality constraints, `b_ub`. A dictionary consisting of the\n",
      "        fields:\n",
      "\n",
      "        residual : np.ndnarray\n",
      "            The (nominally positive) values of the slack variables,\n",
      "            ``b_ub - A_ub @ x``.  This quantity is also commonly\n",
      "            referred to as \"slack\".\n",
      "\n",
      "        marginals : np.ndarray\n",
      "            The sensitivity (partial derivative) of the objective\n",
      "            function with respect to the right-hand side of the\n",
      "            inequality constraints, `b_ub`.\n",
      "\n",
      "    eqlin : OptimizeResult\n",
      "        Solution and sensitivity information corresponding to the\n",
      "        equality constraints, `b_eq`.  A dictionary consisting of the\n",
      "        fields:\n",
      "\n",
      "        residual : np.ndarray\n",
      "            The (nominally zero) residuals of the equality constraints,\n",
      "            ``b_eq - A_eq @ x``.\n",
      "\n",
      "        marginals : np.ndarray\n",
      "            The sensitivity (partial derivative) of the objective\n",
      "            function with respect to the right-hand side of the\n",
      "            equality constraints, `b_eq`.\n",
      "\n",
      "    lower, upper : OptimizeResult\n",
      "        Solution and sensitivity information corresponding to the\n",
      "        lower and upper bounds on decision variables, `bounds`.\n",
      "\n",
      "        residual : np.ndarray\n",
      "            The (nominally positive) values of the quantity\n",
      "            ``x - lb`` (lower) or ``ub - x`` (upper).\n",
      "\n",
      "        marginals : np.ndarray\n",
      "            The sensitivity (partial derivative) of the objective\n",
      "            function with respect to the lower and upper\n",
      "            `bounds`.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "\n",
      "Method :ref:`'highs-ipm' <optimize.linprog-highs-ipm>`\n",
      "is a wrapper of a C++ implementation of an **i**\\ nterior-\\ **p**\\ oint\n",
      "**m**\\ ethod [13]_; it features a crossover routine, so it is as accurate\n",
      "as a simplex solver.\n",
      "Method :ref:`'highs-ds' <optimize.linprog-highs-ds>` is a wrapper\n",
      "of the C++ high performance dual revised simplex implementation (HSOL)\n",
      "[13]_, [14]_. Method :ref:`'highs' <optimize.linprog-highs>` chooses\n",
      "between the two automatically. For new code involving `linprog`, we\n",
      "recommend explicitly choosing one of these three method values instead of\n",
      ":ref:`'interior-point' <optimize.linprog-interior-point>` (default),\n",
      ":ref:`'revised simplex' <optimize.linprog-revised_simplex>`, and\n",
      ":ref:`'simplex' <optimize.linprog-simplex>` (legacy).\n",
      "\n",
      "The result fields `ineqlin`, `eqlin`, `lower`, and `upper` all contain\n",
      "`marginals`, or partial derivatives of the objective function with respect\n",
      "to the right-hand side of each constraint. These partial derivatives are\n",
      "also referred to as \"Lagrange multipliers\", \"dual values\", and\n",
      "\"shadow prices\". The sign convention of `marginals` is opposite that\n",
      "of Lagrange multipliers produced by many nonlinear solvers.\n",
      "\n",
      "References\n",
      "----------\n",
      ".. [13] Huangfu, Q., Galabova, I., Feldmeier, M., and Hall, J. A. J.\n",
      "       \"HiGHS - high performance software for linear optimization.\"\n",
      "       https://highs.dev/\n",
      ".. [14] Huangfu, Q. and Hall, J. A. J. \"Parallelizing the dual revised\n",
      "       simplex method.\" Mathematical Programming Computation, 10 (1),\n",
      "       119-142, 2018. DOI: 10.1007/s12532-017-0130-5\n",
      "\n",
      "highs-ds\n",
      "========\n",
      "\n",
      "Linear programming: minimize a linear objective function subject to linear\n",
      "equality and inequality constraints using the HiGHS dual simplex solver.\n",
      "\n",
      "Linear programming solves problems of the following form:\n",
      "\n",
      ".. math::\n",
      "\n",
      "    \\min_x \\ & c^T x \\\\\n",
      "    \\mbox{such that} \\ & A_{ub} x \\leq b_{ub},\\\\\n",
      "    & A_{eq} x = b_{eq},\\\\\n",
      "    & l \\leq x \\leq u ,\n",
      "\n",
      "where :math:`x` is a vector of decision variables; :math:`c`,\n",
      ":math:`b_{ub}`, :math:`b_{eq}`, :math:`l`, and :math:`u` are vectors; and\n",
      ":math:`A_{ub}` and :math:`A_{eq}` are matrices.\n",
      "\n",
      "Alternatively, that's:\n",
      "\n",
      "minimize::\n",
      "\n",
      "    c @ x\n",
      "\n",
      "such that::\n",
      "\n",
      "    A_ub @ x <= b_ub\n",
      "    A_eq @ x == b_eq\n",
      "    lb <= x <= ub\n",
      "\n",
      "Note that by default ``lb = 0`` and ``ub = None`` unless specified with\n",
      "``bounds``.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "c : 1-D array\n",
      "    The coefficients of the linear objective function to be minimized.\n",
      "A_ub : 2-D array, optional\n",
      "    The inequality constraint matrix. Each row of ``A_ub`` specifies the\n",
      "    coefficients of a linear inequality constraint on ``x``.\n",
      "b_ub : 1-D array, optional\n",
      "    The inequality constraint vector. Each element represents an\n",
      "    upper bound on the corresponding value of ``A_ub @ x``.\n",
      "A_eq : 2-D array, optional\n",
      "    The equality constraint matrix. Each row of ``A_eq`` specifies the\n",
      "    coefficients of a linear equality constraint on ``x``.\n",
      "b_eq : 1-D array, optional\n",
      "    The equality constraint vector. Each element of ``A_eq @ x`` must equal\n",
      "    the corresponding element of ``b_eq``.\n",
      "bounds : sequence, optional\n",
      "    A sequence of ``(min, max)`` pairs for each element in ``x``, defining\n",
      "    the minimum and maximum values of that decision variable. Use ``None``\n",
      "    to indicate that there is no bound. By default, bounds are\n",
      "    ``(0, None)`` (all decision variables are non-negative).\n",
      "    If a single tuple ``(min, max)`` is provided, then ``min`` and\n",
      "    ``max`` will serve as bounds for all decision variables.\n",
      "method : str\n",
      "\n",
      "    This is the method-specific documentation for 'highs-ds'.\n",
      "    :ref:`'highs' <optimize.linprog-highs>`,\n",
      "    :ref:`'highs-ipm' <optimize.linprog-highs-ipm>`,\n",
      "    :ref:`'interior-point' <optimize.linprog-interior-point>` (default),\n",
      "    :ref:`'revised simplex' <optimize.linprog-revised_simplex>`, and\n",
      "    :ref:`'simplex' <optimize.linprog-simplex>` (legacy)\n",
      "    are also available.\n",
      "\n",
      "Options\n",
      "-------\n",
      "maxiter : int\n",
      "    The maximum number of iterations to perform in either phase.\n",
      "    Default is the largest possible value for an ``int`` on the platform.\n",
      "disp : bool (default: ``False``)\n",
      "    Set to ``True`` if indicators of optimization status are to be\n",
      "    printed to the console during optimization.\n",
      "presolve : bool (default: ``True``)\n",
      "    Presolve attempts to identify trivial infeasibilities,\n",
      "    identify trivial unboundedness, and simplify the problem before\n",
      "    sending it to the main solver. It is generally recommended\n",
      "    to keep the default setting ``True``; set to ``False`` if\n",
      "    presolve is to be disabled.\n",
      "time_limit : float\n",
      "    The maximum time in seconds allotted to solve the problem;\n",
      "    default is the largest possible value for a ``double`` on the\n",
      "    platform.\n",
      "dual_feasibility_tolerance : double (default: 1e-07)\n",
      "    Dual feasibility tolerance for\n",
      "    :ref:`'highs-ds' <optimize.linprog-highs-ds>`.\n",
      "primal_feasibility_tolerance : double (default: 1e-07)\n",
      "    Primal feasibility tolerance for\n",
      "    :ref:`'highs-ds' <optimize.linprog-highs-ds>`.\n",
      "simplex_dual_edge_weight_strategy : str (default: None)\n",
      "    Strategy for simplex dual edge weights. The default, ``None``,\n",
      "    automatically selects one of the following.\n",
      "\n",
      "    ``'dantzig'`` uses Dantzig's original strategy of choosing the most\n",
      "    negative reduced cost.\n",
      "\n",
      "    ``'devex'`` uses the strategy described in [15]_.\n",
      "\n",
      "    ``steepest`` uses the exact steepest edge strategy as described in\n",
      "    [16]_.\n",
      "\n",
      "    ``'steepest-devex'`` begins with the exact steepest edge strategy\n",
      "    until the computation is too costly or inexact and then switches to\n",
      "    the devex method.\n",
      "\n",
      "    Curently, ``None`` always selects ``'steepest-devex'``, but this\n",
      "    may change as new options become available.\n",
      "unknown_options : dict\n",
      "    Optional arguments not used by this particular solver. If\n",
      "    ``unknown_options`` is non-empty, a warning is issued listing\n",
      "    all unused options.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "res : OptimizeResult\n",
      "    A :class:`scipy.optimize.OptimizeResult` consisting of the fields:\n",
      "\n",
      "    x : 1D array\n",
      "        The values of the decision variables that minimizes the\n",
      "        objective function while satisfying the constraints.\n",
      "    fun : float\n",
      "        The optimal value of the objective function ``c @ x``.\n",
      "    slack : 1D array\n",
      "        The (nominally positive) values of the slack,\n",
      "        ``b_ub - A_ub @ x``.\n",
      "    con : 1D array\n",
      "        The (nominally zero) residuals of the equality constraints,\n",
      "        ``b_eq - A_eq @ x``.\n",
      "    success : bool\n",
      "        ``True`` when the algorithm succeeds in finding an optimal\n",
      "        solution.\n",
      "    status : int\n",
      "        An integer representing the exit status of the algorithm.\n",
      "\n",
      "        ``0`` : Optimization terminated successfully.\n",
      "\n",
      "        ``1`` : Iteration or time limit reached.\n",
      "\n",
      "        ``2`` : Problem appears to be infeasible.\n",
      "\n",
      "        ``3`` : Problem appears to be unbounded.\n",
      "\n",
      "        ``4`` : The HiGHS solver ran into a problem.\n",
      "\n",
      "    message : str\n",
      "        A string descriptor of the exit status of the algorithm.\n",
      "    nit : int\n",
      "        The total number of iterations performed. This includes iterations\n",
      "        in all phases.\n",
      "    crossover_nit : int\n",
      "        This is always ``0`` for the HiGHS simplex method.\n",
      "        For the HiGHS interior-point method, this is the number of\n",
      "        primal/dual pushes performed during the crossover routine.\n",
      "    ineqlin : OptimizeResult\n",
      "        Solution and sensitivity information corresponding to the\n",
      "        inequality constraints, `b_ub`. A dictionary consisting of the\n",
      "        fields:\n",
      "\n",
      "        residual : np.ndnarray\n",
      "            The (nominally positive) values of the slack variables,\n",
      "            ``b_ub - A_ub @ x``.  This quantity is also commonly\n",
      "            referred to as \"slack\".\n",
      "\n",
      "        marginals : np.ndarray\n",
      "            The sensitivity (partial derivative) of the objective\n",
      "            function with respect to the right-hand side of the\n",
      "            inequality constraints, `b_ub`.\n",
      "\n",
      "    eqlin : OptimizeResult\n",
      "        Solution and sensitivity information corresponding to the\n",
      "        equality constraints, `b_eq`.  A dictionary consisting of the\n",
      "        fields:\n",
      "\n",
      "        residual : np.ndarray\n",
      "            The (nominally zero) residuals of the equality constraints,\n",
      "            ``b_eq - A_eq @ x``.\n",
      "\n",
      "        marginals : np.ndarray\n",
      "            The sensitivity (partial derivative) of the objective\n",
      "            function with respect to the right-hand side of the\n",
      "            equality constraints, `b_eq`.\n",
      "\n",
      "    lower, upper : OptimizeResult\n",
      "        Solution and sensitivity information corresponding to the\n",
      "        lower and upper bounds on decision variables, `bounds`.\n",
      "\n",
      "        residual : np.ndarray\n",
      "            The (nominally positive) values of the quantity\n",
      "            ``x - lb`` (lower) or ``ub - x`` (upper).\n",
      "\n",
      "        marginals : np.ndarray\n",
      "            The sensitivity (partial derivative) of the objective\n",
      "            function with respect to the lower and upper\n",
      "            `bounds`.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "\n",
      "Method :ref:`'highs-ds' <optimize.linprog-highs-ds>` is a wrapper\n",
      "of the C++ high performance dual revised simplex implementation (HSOL)\n",
      "[13]_, [14]_. Method :ref:`'highs-ipm' <optimize.linprog-highs-ipm>`\n",
      "is a wrapper of a C++ implementation of an **i**\\ nterior-\\ **p**\\ oint\n",
      "**m**\\ ethod [13]_; it features a crossover routine, so it is as accurate\n",
      "as a simplex solver. Method :ref:`'highs' <optimize.linprog-highs>` chooses\n",
      "between the two automatically. For new code involving `linprog`, we\n",
      "recommend explicitly choosing one of these three method values instead of\n",
      ":ref:`'interior-point' <optimize.linprog-interior-point>` (default),\n",
      ":ref:`'revised simplex' <optimize.linprog-revised_simplex>`, and\n",
      ":ref:`'simplex' <optimize.linprog-simplex>` (legacy).\n",
      "\n",
      "The result fields `ineqlin`, `eqlin`, `lower`, and `upper` all contain\n",
      "`marginals`, or partial derivatives of the objective function with respect\n",
      "to the right-hand side of each constraint. These partial derivatives are\n",
      "also referred to as \"Lagrange multipliers\", \"dual values\", and\n",
      "\"shadow prices\". The sign convention of `marginals` is opposite that\n",
      "of Lagrange multipliers produced by many nonlinear solvers.\n",
      "\n",
      "References\n",
      "----------\n",
      ".. [13] Huangfu, Q., Galabova, I., Feldmeier, M., and Hall, J. A. J.\n",
      "       \"HiGHS - high performance software for linear optimization.\"\n",
      "       https://highs.dev/\n",
      ".. [14] Huangfu, Q. and Hall, J. A. J. \"Parallelizing the dual revised\n",
      "       simplex method.\" Mathematical Programming Computation, 10 (1),\n",
      "       119-142, 2018. DOI: 10.1007/s12532-017-0130-5\n",
      ".. [15] Harris, Paula MJ. \"Pivot selection methods of the Devex LP code.\"\n",
      "        Mathematical programming 5.1 (1973): 1-28.\n",
      ".. [16] Goldfarb, Donald, and John Ker Reid. \"A practicable steepest-edge\n",
      "        simplex algorithm.\" Mathematical Programming 12.1 (1977): 361-371.\n",
      "\n",
      "highs\n",
      "=====\n",
      "\n",
      "Linear programming: minimize a linear objective function subject to linear\n",
      "equality and inequality constraints using one of the HiGHS solvers.\n",
      "\n",
      "Linear programming solves problems of the following form:\n",
      "\n",
      ".. math::\n",
      "\n",
      "    \\min_x \\ & c^T x \\\\\n",
      "    \\mbox{such that} \\ & A_{ub} x \\leq b_{ub},\\\\\n",
      "    & A_{eq} x = b_{eq},\\\\\n",
      "    & l \\leq x \\leq u ,\n",
      "\n",
      "where :math:`x` is a vector of decision variables; :math:`c`,\n",
      ":math:`b_{ub}`, :math:`b_{eq}`, :math:`l`, and :math:`u` are vectors; and\n",
      ":math:`A_{ub}` and :math:`A_{eq}` are matrices.\n",
      "\n",
      "Alternatively, that's:\n",
      "\n",
      "minimize::\n",
      "\n",
      "    c @ x\n",
      "\n",
      "such that::\n",
      "\n",
      "    A_ub @ x <= b_ub\n",
      "    A_eq @ x == b_eq\n",
      "    lb <= x <= ub\n",
      "\n",
      "Note that by default ``lb = 0`` and ``ub = None`` unless specified with\n",
      "``bounds``.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "c : 1-D array\n",
      "    The coefficients of the linear objective function to be minimized.\n",
      "A_ub : 2-D array, optional\n",
      "    The inequality constraint matrix. Each row of ``A_ub`` specifies the\n",
      "    coefficients of a linear inequality constraint on ``x``.\n",
      "b_ub : 1-D array, optional\n",
      "    The inequality constraint vector. Each element represents an\n",
      "    upper bound on the corresponding value of ``A_ub @ x``.\n",
      "A_eq : 2-D array, optional\n",
      "    The equality constraint matrix. Each row of ``A_eq`` specifies the\n",
      "    coefficients of a linear equality constraint on ``x``.\n",
      "b_eq : 1-D array, optional\n",
      "    The equality constraint vector. Each element of ``A_eq @ x`` must equal\n",
      "    the corresponding element of ``b_eq``.\n",
      "bounds : sequence, optional\n",
      "    A sequence of ``(min, max)`` pairs for each element in ``x``, defining\n",
      "    the minimum and maximum values of that decision variable. Use ``None``\n",
      "    to indicate that there is no bound. By default, bounds are\n",
      "    ``(0, None)`` (all decision variables are non-negative).\n",
      "    If a single tuple ``(min, max)`` is provided, then ``min`` and\n",
      "    ``max`` will serve as bounds for all decision variables.\n",
      "method : str\n",
      "\n",
      "    This is the method-specific documentation for 'highs', which chooses\n",
      "    automatically between\n",
      "    :ref:`'highs-ds' <optimize.linprog-highs-ds>` and\n",
      "    :ref:`'highs-ipm' <optimize.linprog-highs-ipm>`.\n",
      "    :ref:`'interior-point' <optimize.linprog-interior-point>` (default),\n",
      "    :ref:`'revised simplex' <optimize.linprog-revised_simplex>`, and\n",
      "    :ref:`'simplex' <optimize.linprog-simplex>` (legacy)\n",
      "    are also available.\n",
      "integrality : 1-D array or int, optional\n",
      "    Indicates the type of integrality constraint on each decision variable.\n",
      "\n",
      "    ``0`` : Continuous variable; no integrality constraint.\n",
      "\n",
      "    ``1`` : Integer variable; decision variable must be an integer\n",
      "    within `bounds`.\n",
      "\n",
      "    ``2`` : Semi-continuous variable; decision variable must be within\n",
      "    `bounds` or take value ``0``.\n",
      "\n",
      "    ``3`` : Semi-integer variable; decision variable must be an integer\n",
      "    within `bounds` or take value ``0``.\n",
      "\n",
      "    By default, all variables are continuous.\n",
      "\n",
      "    For mixed integrality constraints, supply an array of shape `c.shape`.\n",
      "    To infer a constraint on each decision variable from shorter inputs,\n",
      "    the argument will be broadcasted to `c.shape` using `np.broadcast_to`.\n",
      "\n",
      "    This argument is currently used only by the ``'highs'`` method and\n",
      "    ignored otherwise.\n",
      "\n",
      "Options\n",
      "-------\n",
      "maxiter : int\n",
      "    The maximum number of iterations to perform in either phase.\n",
      "    For :ref:`'highs-ipm' <optimize.linprog-highs-ipm>`, this does not\n",
      "    include the number of crossover iterations. Default is the largest\n",
      "    possible value for an ``int`` on the platform.\n",
      "disp : bool (default: ``False``)\n",
      "    Set to ``True`` if indicators of optimization status are to be\n",
      "    printed to the console during optimization.\n",
      "presolve : bool (default: ``True``)\n",
      "    Presolve attempts to identify trivial infeasibilities,\n",
      "    identify trivial unboundedness, and simplify the problem before\n",
      "    sending it to the main solver. It is generally recommended\n",
      "    to keep the default setting ``True``; set to ``False`` if\n",
      "    presolve is to be disabled.\n",
      "time_limit : float\n",
      "    The maximum time in seconds allotted to solve the problem;\n",
      "    default is the largest possible value for a ``double`` on the\n",
      "    platform.\n",
      "dual_feasibility_tolerance : double (default: 1e-07)\n",
      "    Dual feasibility tolerance for\n",
      "    :ref:`'highs-ds' <optimize.linprog-highs-ds>`.\n",
      "    The minimum of this and ``primal_feasibility_tolerance``\n",
      "    is used for the feasibility tolerance of\n",
      "    :ref:`'highs-ipm' <optimize.linprog-highs-ipm>`.\n",
      "primal_feasibility_tolerance : double (default: 1e-07)\n",
      "    Primal feasibility tolerance for\n",
      "    :ref:`'highs-ds' <optimize.linprog-highs-ds>`.\n",
      "    The minimum of this and ``dual_feasibility_tolerance``\n",
      "    is used for the feasibility tolerance of\n",
      "    :ref:`'highs-ipm' <optimize.linprog-highs-ipm>`.\n",
      "ipm_optimality_tolerance : double (default: ``1e-08``)\n",
      "    Optimality tolerance for\n",
      "    :ref:`'highs-ipm' <optimize.linprog-highs-ipm>`.\n",
      "    Minimum allowable value is 1e-12.\n",
      "simplex_dual_edge_weight_strategy : str (default: None)\n",
      "    Strategy for simplex dual edge weights. The default, ``None``,\n",
      "    automatically selects one of the following.\n",
      "\n",
      "    ``'dantzig'`` uses Dantzig's original strategy of choosing the most\n",
      "    negative reduced cost.\n",
      "\n",
      "    ``'devex'`` uses the strategy described in [15]_.\n",
      "\n",
      "    ``steepest`` uses the exact steepest edge strategy as described in\n",
      "    [16]_.\n",
      "\n",
      "    ``'steepest-devex'`` begins with the exact steepest edge strategy\n",
      "    until the computation is too costly or inexact and then switches to\n",
      "    the devex method.\n",
      "\n",
      "    Curently, ``None`` always selects ``'steepest-devex'``, but this\n",
      "    may change as new options become available.\n",
      "mip_rel_gap : double (default: None)\n",
      "    Termination criterion for MIP solver: solver will terminate when the\n",
      "    gap between the primal objective value and the dual objective bound,\n",
      "    scaled by the primal objective value, is <= mip_rel_gap.\n",
      "unknown_options : dict\n",
      "    Optional arguments not used by this particular solver. If\n",
      "    ``unknown_options`` is non-empty, a warning is issued listing\n",
      "    all unused options.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "res : OptimizeResult\n",
      "    A :class:`scipy.optimize.OptimizeResult` consisting of the fields:\n",
      "\n",
      "    x : 1D array\n",
      "        The values of the decision variables that minimizes the\n",
      "        objective function while satisfying the constraints.\n",
      "    fun : float\n",
      "        The optimal value of the objective function ``c @ x``.\n",
      "    slack : 1D array\n",
      "        The (nominally positive) values of the slack,\n",
      "        ``b_ub - A_ub @ x``.\n",
      "    con : 1D array\n",
      "        The (nominally zero) residuals of the equality constraints,\n",
      "        ``b_eq - A_eq @ x``.\n",
      "    success : bool\n",
      "        ``True`` when the algorithm succeeds in finding an optimal\n",
      "        solution.\n",
      "    status : int\n",
      "        An integer representing the exit status of the algorithm.\n",
      "\n",
      "        ``0`` : Optimization terminated successfully.\n",
      "\n",
      "        ``1`` : Iteration or time limit reached.\n",
      "\n",
      "        ``2`` : Problem appears to be infeasible.\n",
      "\n",
      "        ``3`` : Problem appears to be unbounded.\n",
      "\n",
      "        ``4`` : The HiGHS solver ran into a problem.\n",
      "\n",
      "    message : str\n",
      "        A string descriptor of the exit status of the algorithm.\n",
      "    nit : int\n",
      "        The total number of iterations performed.\n",
      "        For the HiGHS simplex method, this includes iterations in all\n",
      "        phases. For the HiGHS interior-point method, this does not include\n",
      "        crossover iterations.\n",
      "    crossover_nit : int\n",
      "        The number of primal/dual pushes performed during the\n",
      "        crossover routine for the HiGHS interior-point method.\n",
      "        This is ``0`` for the HiGHS simplex method.\n",
      "    ineqlin : OptimizeResult\n",
      "        Solution and sensitivity information corresponding to the\n",
      "        inequality constraints, `b_ub`. A dictionary consisting of the\n",
      "        fields:\n",
      "\n",
      "        residual : np.ndnarray\n",
      "            The (nominally positive) values of the slack variables,\n",
      "            ``b_ub - A_ub @ x``.  This quantity is also commonly\n",
      "            referred to as \"slack\".\n",
      "\n",
      "        marginals : np.ndarray\n",
      "            The sensitivity (partial derivative) of the objective\n",
      "            function with respect to the right-hand side of the\n",
      "            inequality constraints, `b_ub`.\n",
      "\n",
      "    eqlin : OptimizeResult\n",
      "        Solution and sensitivity information corresponding to the\n",
      "        equality constraints, `b_eq`.  A dictionary consisting of the\n",
      "        fields:\n",
      "\n",
      "        residual : np.ndarray\n",
      "            The (nominally zero) residuals of the equality constraints,\n",
      "            ``b_eq - A_eq @ x``.\n",
      "\n",
      "        marginals : np.ndarray\n",
      "            The sensitivity (partial derivative) of the objective\n",
      "            function with respect to the right-hand side of the\n",
      "            equality constraints, `b_eq`.\n",
      "\n",
      "    lower, upper : OptimizeResult\n",
      "        Solution and sensitivity information corresponding to the\n",
      "        lower and upper bounds on decision variables, `bounds`.\n",
      "\n",
      "        residual : np.ndarray\n",
      "            The (nominally positive) values of the quantity\n",
      "            ``x - lb`` (lower) or ``ub - x`` (upper).\n",
      "\n",
      "        marginals : np.ndarray\n",
      "            The sensitivity (partial derivative) of the objective\n",
      "            function with respect to the lower and upper\n",
      "            `bounds`.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "\n",
      "Method :ref:`'highs-ds' <optimize.linprog-highs-ds>` is a wrapper\n",
      "of the C++ high performance dual revised simplex implementation (HSOL)\n",
      "[13]_, [14]_. Method :ref:`'highs-ipm' <optimize.linprog-highs-ipm>`\n",
      "is a wrapper of a C++ implementation of an **i**\\ nterior-\\ **p**\\ oint\n",
      "**m**\\ ethod [13]_; it features a crossover routine, so it is as accurate\n",
      "as a simplex solver. Method :ref:`'highs' <optimize.linprog-highs>` chooses\n",
      "between the two automatically. For new code involving `linprog`, we\n",
      "recommend explicitly choosing one of these three method values instead of\n",
      ":ref:`'interior-point' <optimize.linprog-interior-point>` (default),\n",
      ":ref:`'revised simplex' <optimize.linprog-revised_simplex>`, and\n",
      ":ref:`'simplex' <optimize.linprog-simplex>` (legacy).\n",
      "\n",
      "The result fields `ineqlin`, `eqlin`, `lower`, and `upper` all contain\n",
      "`marginals`, or partial derivatives of the objective function with respect\n",
      "to the right-hand side of each constraint. These partial derivatives are\n",
      "also referred to as \"Lagrange multipliers\", \"dual values\", and\n",
      "\"shadow prices\". The sign convention of `marginals` is opposite that\n",
      "of Lagrange multipliers produced by many nonlinear solvers.\n",
      "\n",
      "References\n",
      "----------\n",
      ".. [13] Huangfu, Q., Galabova, I., Feldmeier, M., and Hall, J. A. J.\n",
      "       \"HiGHS - high performance software for linear optimization.\"\n",
      "       https://highs.dev/\n",
      ".. [14] Huangfu, Q. and Hall, J. A. J. \"Parallelizing the dual revised\n",
      "       simplex method.\" Mathematical Programming Computation, 10 (1),\n",
      "       119-142, 2018. DOI: 10.1007/s12532-017-0130-5\n",
      ".. [15] Harris, Paula MJ. \"Pivot selection methods of the Devex LP code.\"\n",
      "        Mathematical programming 5.1 (1973): 1-28.\n",
      ".. [16] Goldfarb, Donald, and John Ker Reid. \"A practicable steepest-edge\n",
      "        simplex algorithm.\" Mathematical Programming 12.1 (1977): 361-371.\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "scipy.optimize.show_options(method='L-BFGS-B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = girgs.generateWeights(n, tau)\n",
    "target_avg_degree = 20.0\n",
    "c_0 = girgs.scaleWeights(weights, target_avg_degree, d, 1/0.99) \n",
    "c_1 = girgs.scaleWeights(weights, target_avg_degree, d, 1/0.01) * 20\n",
    "c_0log, c_1log = np.log(c_0), np.log(c_1)\n",
    "c_midlog = 0.5*(c_1log + c_0log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.08856800880146827, 10.10187573498115)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_0, c_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9458874240664947"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(np.log(c_1/c_0)/2 + np.log(c_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.62 s, sys: 63.7 ms, total: 6.68 s\n",
      "Wall time: 6.68 s\n"
     ]
    }
   ],
   "source": [
    "%time g2 = generation.cgirg_gen_cube_coupling(n, d, tau, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.81 s, sys: 7 ms, total: 2.82 s\n",
      "Wall time: 2.81 s\n"
     ]
    }
   ],
   "source": [
    "%time gnk, edges, weights, pts, const, id2gnk = generation.cgirg_gen(n, d, tau, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.08 s, sys: 30.9 ms, total: 3.11 s\n",
      "Wall time: 3.1 s\n"
     ]
    }
   ],
   "source": [
    "%time g3 = generation.cgirg_gen_cube_coupling2(n, d, tau, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network Properties:\n",
      "nodes, edges\t\t\t5000, 135253\n",
      "directed?\t\t\tFalse\n",
      "weighted?\t\t\tFalse\n",
      "isolated nodes\t\t\t0\n",
      "self-loops\t\t\t0\n",
      "density\t\t\t\t0.010822\n",
      "clustering coefficient\t\t0.576200\n",
      "min/max/avg degree\t\t1, 4486, 54.101200\n",
      "degree assortativity\t\t-0.140397\n",
      "number of connected components\t1\n",
      "size of largest component\t5000 (100.00 %)\n"
     ]
    }
   ],
   "source": [
    "nk.overview(g2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network Properties:\n",
      "nodes, edges\t\t\t5000, 137511\n",
      "directed?\t\t\tFalse\n",
      "weighted?\t\t\tFalse\n",
      "isolated nodes\t\t\t0\n",
      "self-loops\t\t\t0\n",
      "density\t\t\t\t0.011003\n",
      "clustering coefficient\t\t0.586918\n",
      "min/max/avg degree\t\t4, 2270, 55.004400\n",
      "degree assortativity\t\t-0.155410\n",
      "number of connected components\t1\n",
      "size of largest component\t5000 (100.00 %)\n"
     ]
    }
   ],
   "source": [
    "nk.overview(g3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnk, edges, weights, pts, const, id2gnk = generation.cgirg_gen(n, d, tau, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "pts = np.array(pts)\n",
    "weights = np.array(weights)\n",
    "W = np.sum(weights)\n",
    "\n",
    "u, v = edges[0]\n",
    "xu, xv = pts[u], pts[v]\n",
    "\n",
    "wu, wv = weights[u], weights[v]\n",
    "diff = np.abs(xu - xv)\n",
    "torus_diff = np.minimum(diff, 1 - diff)\n",
    "torus_inf_norm = torus_diff.max()\n",
    "cube_inf_norm = diff.max()\n",
    "p_uv = min(1, ((wu * wv / W) / torus_inf_norm**d)**alpha)\n",
    "p_uv_cube = min(1, ((wu * wv / W) / cube_inf_norm ** d) ** alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = np.array(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, v = edges[:, 0], edges[:, 1]\n",
    "xu, xv = pts[u], pts[v]\n",
    "wu, wv = weights[u], weights[v]\n",
    "diff = np.abs(xu - xv)\n",
    "torus_diff = np.stack([diff, 1-diff]).min(axis=0)\n",
    "torus_inf_norm = torus_diff.max(axis=1)\n",
    "cube_inf_norm = diff.max(axis=1)\n",
    "\n",
    "puv = np.stack([np.ones(cube_inf_norm.shape), ((wu * wv / W) / torus_inf_norm**d)**alpha]).min(axis=0)\n",
    "puv_cube = np.stack([np.ones(cube_inf_norm.shape), ((wu * wv / W) / cube_inf_norm**d)**alpha]).min(axis=0)\n",
    "\n",
    "samples = np.random.uniform(size=cube_inf_norm.shape)\n",
    "to_remove = samples > puv_cube/puv\n",
    "\n",
    "for u, v in edges[to_remove]:\n",
    "    gnk.removeEdge(id2gnk[u], id2gnk[v])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70272, 2)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges[to_remove].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{769: 0,\n",
       " 1536: 1,\n",
       " 4058: 2,\n",
       " 3873: 3,\n",
       " 21: 4,\n",
       " 4407: 5,\n",
       " 771: 6,\n",
       " 486: 7,\n",
       " 232: 8,\n",
       " 1171: 9,\n",
       " 4226: 10,\n",
       " 367: 11,\n",
       " 1737: 12,\n",
       " 4616: 13,\n",
       " 313: 14,\n",
       " 3415: 15,\n",
       " 3599: 16,\n",
       " 735: 17,\n",
       " 3302: 18,\n",
       " 4983: 19,\n",
       " 316: 20,\n",
       " 2379: 21,\n",
       " 4238: 22,\n",
       " 3962: 23,\n",
       " 4853: 24,\n",
       " 950: 25,\n",
       " 2933: 26,\n",
       " 630: 27,\n",
       " 1157: 28,\n",
       " 4573: 29,\n",
       " 2942: 30,\n",
       " 1885: 31,\n",
       " 1087: 32,\n",
       " 2013: 33,\n",
       " 2992: 34,\n",
       " 3488: 35,\n",
       " 1099: 36,\n",
       " 2124: 37,\n",
       " 4659: 38,\n",
       " 1727: 39,\n",
       " 674: 40,\n",
       " 1359: 41,\n",
       " 789: 42,\n",
       " 2900: 43,\n",
       " 2224: 44,\n",
       " 2634: 45,\n",
       " 4430: 46,\n",
       " 2326: 47,\n",
       " 307: 48,\n",
       " 2560: 49,\n",
       " 2299: 50,\n",
       " 2490: 51,\n",
       " 4634: 52,\n",
       " 2419: 53,\n",
       " 2930: 54,\n",
       " 3825: 55,\n",
       " 1168: 56,\n",
       " 3824: 57,\n",
       " 1922: 58,\n",
       " 1347: 59,\n",
       " 219: 60,\n",
       " 4301: 61,\n",
       " 2372: 62,\n",
       " 566: 63,\n",
       " 678: 64,\n",
       " 1030: 65,\n",
       " 2789: 66,\n",
       " 2135: 67,\n",
       " 3279: 68,\n",
       " 3432: 69,\n",
       " 1715: 70,\n",
       " 2028: 71,\n",
       " 4607: 72,\n",
       " 2825: 73,\n",
       " 1133: 74,\n",
       " 4122: 75,\n",
       " 2758: 76,\n",
       " 3411: 77,\n",
       " 2467: 78,\n",
       " 4244: 79,\n",
       " 94: 80,\n",
       " 1067: 81,\n",
       " 686: 82,\n",
       " 3734: 83,\n",
       " 1250: 84,\n",
       " 616: 85,\n",
       " 4960: 86,\n",
       " 250: 87,\n",
       " 2017: 88,\n",
       " 1840: 89,\n",
       " 1090: 90,\n",
       " 4161: 91,\n",
       " 1141: 92,\n",
       " 1112: 93,\n",
       " 4577: 94,\n",
       " 4003: 95,\n",
       " 805: 96,\n",
       " 1814: 97,\n",
       " 2433: 98,\n",
       " 3259: 99,\n",
       " 3865: 100,\n",
       " 2834: 101,\n",
       " 690: 102,\n",
       " 4387: 103,\n",
       " 1654: 104,\n",
       " 4492: 105,\n",
       " 958: 106,\n",
       " 4684: 107,\n",
       " 1815: 108,\n",
       " 2899: 109,\n",
       " 3199: 110,\n",
       " 1895: 111,\n",
       " 2377: 112,\n",
       " 4405: 113,\n",
       " 1401: 114,\n",
       " 2296: 115,\n",
       " 348: 116,\n",
       " 905: 117,\n",
       " 4840: 118,\n",
       " 1407: 119,\n",
       " 4847: 120,\n",
       " 4373: 121,\n",
       " 4715: 122,\n",
       " 3832: 123,\n",
       " 3405: 124,\n",
       " 4713: 125,\n",
       " 1238: 126,\n",
       " 1593: 127,\n",
       " 2352: 128,\n",
       " 2240: 129,\n",
       " 3061: 130,\n",
       " 4595: 131,\n",
       " 2475: 132,\n",
       " 4630: 133,\n",
       " 137: 134,\n",
       " 2697: 135,\n",
       " 985: 136,\n",
       " 1060: 137,\n",
       " 4223: 138,\n",
       " 3935: 139,\n",
       " 4611: 140,\n",
       " 3795: 141,\n",
       " 4424: 142,\n",
       " 3536: 143,\n",
       " 2692: 144,\n",
       " 4063: 145,\n",
       " 244: 146,\n",
       " 669: 147,\n",
       " 4887: 148,\n",
       " 2335: 149,\n",
       " 204: 150,\n",
       " 1750: 151,\n",
       " 3321: 152,\n",
       " 4627: 153,\n",
       " 1472: 154,\n",
       " 4254: 155,\n",
       " 3418: 156,\n",
       " 4135: 157,\n",
       " 4571: 158,\n",
       " 4748: 159,\n",
       " 334: 160,\n",
       " 1295: 161,\n",
       " 4092: 162,\n",
       " 1964: 163,\n",
       " 3740: 164,\n",
       " 3234: 165,\n",
       " 2130: 166,\n",
       " 767: 167,\n",
       " 4317: 168,\n",
       " 1876: 169,\n",
       " 3931: 170,\n",
       " 1901: 171,\n",
       " 2940: 172,\n",
       " 325: 173,\n",
       " 1544: 174,\n",
       " 724: 175,\n",
       " 4991: 176,\n",
       " 4512: 177,\n",
       " 3435: 178,\n",
       " 3284: 179,\n",
       " 1484: 180,\n",
       " 1651: 181,\n",
       " 3157: 182,\n",
       " 2852: 183,\n",
       " 4543: 184,\n",
       " 1790: 185,\n",
       " 4371: 186,\n",
       " 1365: 187,\n",
       " 2357: 188,\n",
       " 3633: 189,\n",
       " 3876: 190,\n",
       " 2823: 191,\n",
       " 50: 192,\n",
       " 4046: 193,\n",
       " 4822: 194,\n",
       " 2479: 195,\n",
       " 1450: 196,\n",
       " 1219: 197,\n",
       " 3572: 198,\n",
       " 1724: 199,\n",
       " 2903: 200,\n",
       " 995: 201,\n",
       " 1075: 202,\n",
       " 2587: 203,\n",
       " 2427: 204,\n",
       " 3976: 205,\n",
       " 1032: 206,\n",
       " 2109: 207,\n",
       " 2626: 208,\n",
       " 4899: 209,\n",
       " 575: 210,\n",
       " 1512: 211,\n",
       " 2037: 212,\n",
       " 633: 213,\n",
       " 4954: 214,\n",
       " 17: 215,\n",
       " 4601: 216,\n",
       " 456: 217,\n",
       " 3151: 218,\n",
       " 33: 219,\n",
       " 4193: 220,\n",
       " 3702: 221,\n",
       " 86: 222,\n",
       " 1866: 223,\n",
       " 544: 224,\n",
       " 2581: 225,\n",
       " 2783: 226,\n",
       " 1951: 227,\n",
       " 1080: 228,\n",
       " 2568: 229,\n",
       " 1747: 230,\n",
       " 2386: 231,\n",
       " 1019: 232,\n",
       " 2611: 233,\n",
       " 406: 234,\n",
       " 646: 235,\n",
       " 2513: 236,\n",
       " 40: 237,\n",
       " 4106: 238,\n",
       " 2155: 239,\n",
       " 2211: 240,\n",
       " 4005: 241,\n",
       " 1622: 242,\n",
       " 2027: 243,\n",
       " 495: 244,\n",
       " 3499: 245,\n",
       " 275: 246,\n",
       " 2308: 247,\n",
       " 4212: 248,\n",
       " 1693: 249,\n",
       " 4783: 250,\n",
       " 822: 251,\n",
       " 3835: 252,\n",
       " 2688: 253,\n",
       " 688: 254,\n",
       " 1942: 255,\n",
       " 2259: 256,\n",
       " 1590: 257,\n",
       " 3866: 258,\n",
       " 2134: 259,\n",
       " 1997: 260,\n",
       " 74: 261,\n",
       " 4972: 262,\n",
       " 4191: 263,\n",
       " 3034: 264,\n",
       " 3692: 265,\n",
       " 4114: 266,\n",
       " 4938: 267,\n",
       " 2043: 268,\n",
       " 2425: 269,\n",
       " 1953: 270,\n",
       " 1653: 271,\n",
       " 3998: 272,\n",
       " 1627: 273,\n",
       " 4508: 274,\n",
       " 2889: 275,\n",
       " 3844: 276,\n",
       " 940: 277,\n",
       " 1505: 278,\n",
       " 4389: 279,\n",
       " 4623: 280,\n",
       " 3348: 281,\n",
       " 813: 282,\n",
       " 1603: 283,\n",
       " 3372: 284,\n",
       " 825: 285,\n",
       " 3834: 286,\n",
       " 3132: 287,\n",
       " 1577: 288,\n",
       " 4766: 289,\n",
       " 4051: 290,\n",
       " 4581: 291,\n",
       " 2305: 292,\n",
       " 957: 293,\n",
       " 1909: 294,\n",
       " 3658: 295,\n",
       " 2509: 296,\n",
       " 3628: 297,\n",
       " 930: 298,\n",
       " 3991: 299,\n",
       " 1890: 300,\n",
       " 551: 301,\n",
       " 4464: 302,\n",
       " 1422: 303,\n",
       " 1638: 304,\n",
       " 1568: 305,\n",
       " 2616: 306,\n",
       " 634: 307,\n",
       " 4723: 308,\n",
       " 2925: 309,\n",
       " 4631: 310,\n",
       " 1585: 311,\n",
       " 2790: 312,\n",
       " 4481: 313,\n",
       " 4909: 314,\n",
       " 4269: 315,\n",
       " 91: 316,\n",
       " 284: 317,\n",
       " 715: 318,\n",
       " 4098: 319,\n",
       " 434: 320,\n",
       " 422: 321,\n",
       " 1299: 322,\n",
       " 4785: 323,\n",
       " 998: 324,\n",
       " 3010: 325,\n",
       " 3391: 326,\n",
       " 559: 327,\n",
       " 991: 328,\n",
       " 3895: 329,\n",
       " 2586: 330,\n",
       " 894: 331,\n",
       " 776: 332,\n",
       " 1574: 333,\n",
       " 2892: 334,\n",
       " 2919: 335,\n",
       " 795: 336,\n",
       " 2441: 337,\n",
       " 4707: 338,\n",
       " 4967: 339,\n",
       " 1229: 340,\n",
       " 740: 341,\n",
       " 4209: 342,\n",
       " 2764: 343,\n",
       " 3961: 344,\n",
       " 3552: 345,\n",
       " 1027: 346,\n",
       " 3352: 347,\n",
       " 1240: 348,\n",
       " 3755: 349,\n",
       " 1959: 350,\n",
       " 102: 351,\n",
       " 322: 352,\n",
       " 1000: 353,\n",
       " 4162: 354,\n",
       " 4343: 355,\n",
       " 2971: 356,\n",
       " 16: 357,\n",
       " 4167: 358,\n",
       " 2327: 359,\n",
       " 3224: 360,\n",
       " 4821: 361,\n",
       " 2461: 362,\n",
       " 1810: 363,\n",
       " 4253: 364,\n",
       " 3162: 365,\n",
       " 3616: 366,\n",
       " 1308: 367,\n",
       " 3143: 368,\n",
       " 1844: 369,\n",
       " 719: 370,\n",
       " 1249: 371,\n",
       " 3220: 372,\n",
       " 4350: 373,\n",
       " 4231: 374,\n",
       " 2337: 375,\n",
       " 2848: 376,\n",
       " 4779: 377,\n",
       " 3700: 378,\n",
       " 4907: 379,\n",
       " 4166: 380,\n",
       " 4476: 381,\n",
       " 3713: 382,\n",
       " 1886: 383,\n",
       " 2577: 384,\n",
       " 2632: 385,\n",
       " 375: 386,\n",
       " 4739: 387,\n",
       " 4867: 388,\n",
       " 472: 389,\n",
       " 442: 390,\n",
       " 1237: 391,\n",
       " 473: 392,\n",
       " 643: 393,\n",
       " 4306: 394,\n",
       " 2495: 395,\n",
       " 2693: 396,\n",
       " 150: 397,\n",
       " 4081: 398,\n",
       " 4198: 399,\n",
       " 4844: 400,\n",
       " 4380: 401,\n",
       " 357: 402,\n",
       " 2826: 403,\n",
       " 496: 404,\n",
       " 881: 405,\n",
       " 2250: 406,\n",
       " 25: 407,\n",
       " 522: 408,\n",
       " 3474: 409,\n",
       " 4640: 410,\n",
       " 802: 411,\n",
       " 2436: 412,\n",
       " 3783: 413,\n",
       " 1148: 414,\n",
       " 155: 415,\n",
       " 127: 416,\n",
       " 4079: 417,\n",
       " 1809: 418,\n",
       " 1894: 419,\n",
       " 1573: 420,\n",
       " 4201: 421,\n",
       " 3210: 422,\n",
       " 4609: 423,\n",
       " 934: 424,\n",
       " 294: 425,\n",
       " 2887: 426,\n",
       " 734: 427,\n",
       " 4152: 428,\n",
       " 1218: 429,\n",
       " 606: 430,\n",
       " 1729: 431,\n",
       " 1774: 432,\n",
       " 2079: 433,\n",
       " 2983: 434,\n",
       " 3822: 435,\n",
       " 1383: 436,\n",
       " 4200: 437,\n",
       " 2806: 438,\n",
       " 3084: 439,\n",
       " 4897: 440,\n",
       " 216: 441,\n",
       " 3018: 442,\n",
       " 4531: 443,\n",
       " 1109: 444,\n",
       " 4015: 445,\n",
       " 4342: 446,\n",
       " 2204: 447,\n",
       " 1330: 448,\n",
       " 2696: 449,\n",
       " 3213: 450,\n",
       " 555: 451,\n",
       " 3661: 452,\n",
       " 3202: 453,\n",
       " 3101: 454,\n",
       " 1555: 455,\n",
       " 1743: 456,\n",
       " 951: 457,\n",
       " 2998: 458,\n",
       " 3170: 459,\n",
       " 1375: 460,\n",
       " 3050: 461,\n",
       " 4169: 462,\n",
       " 658: 463,\n",
       " 1689: 464,\n",
       " 2934: 465,\n",
       " 1266: 466,\n",
       " 4806: 467,\n",
       " 2549: 468,\n",
       " 3152: 469,\n",
       " 4560: 470,\n",
       " 2673: 471,\n",
       " 1129: 472,\n",
       " 4061: 473,\n",
       " 32: 474,\n",
       " 3057: 475,\n",
       " 3654: 476,\n",
       " 224: 477,\n",
       " 4230: 478,\n",
       " 335: 479,\n",
       " 3069: 480,\n",
       " 3180: 481,\n",
       " 3930: 482,\n",
       " 3558: 483,\n",
       " 3969: 484,\n",
       " 4177: 485,\n",
       " 1447: 486,\n",
       " 3063: 487,\n",
       " 2980: 488,\n",
       " 4501: 489,\n",
       " 1842: 490,\n",
       " 2222: 491,\n",
       " 181: 492,\n",
       " 4988: 493,\n",
       " 2458: 494,\n",
       " 1619: 495,\n",
       " 4395: 496,\n",
       " 2042: 497,\n",
       " 523: 498,\n",
       " 4557: 499,\n",
       " 196: 500,\n",
       " 1483: 501,\n",
       " 3915: 502,\n",
       " 2064: 503,\n",
       " 3358: 504,\n",
       " 1239: 505,\n",
       " 1860: 506,\n",
       " 3297: 507,\n",
       " 3750: 508,\n",
       " 3979: 509,\n",
       " 3761: 510,\n",
       " 1898: 511,\n",
       " 3690: 512,\n",
       " 1769: 513,\n",
       " 2902: 514,\n",
       " 1139: 515,\n",
       " 3296: 516,\n",
       " 3299: 517,\n",
       " 202: 518,\n",
       " 4619: 519,\n",
       " 419: 520,\n",
       " 4036: 521,\n",
       " 2765: 522,\n",
       " 345: 523,\n",
       " 4382: 524,\n",
       " 2358: 525,\n",
       " 3492: 526,\n",
       " 4735: 527,\n",
       " 3666: 528,\n",
       " 289: 529,\n",
       " 744: 530,\n",
       " 4022: 531,\n",
       " 1773: 532,\n",
       " 1525: 533,\n",
       " 4584: 534,\n",
       " 3919: 535,\n",
       " 2859: 536,\n",
       " 1725: 537,\n",
       " 709: 538,\n",
       " 3320: 539,\n",
       " 1664: 540,\n",
       " 3327: 541,\n",
       " 4559: 542,\n",
       " 4931: 543,\n",
       " 191: 544,\n",
       " 2638: 545,\n",
       " 1943: 546,\n",
       " 2373: 547,\n",
       " 3254: 548,\n",
       " 3036: 549,\n",
       " 77: 550,\n",
       " 3937: 551,\n",
       " 1516: 552,\n",
       " 4242: 553,\n",
       " 3513: 554,\n",
       " 2292: 555,\n",
       " 277: 556,\n",
       " 535: 557,\n",
       " 3363: 558,\n",
       " 585: 559,\n",
       " 2142: 560,\n",
       " 482: 561,\n",
       " 4082: 562,\n",
       " 3429: 563,\n",
       " 52: 564,\n",
       " 2323: 565,\n",
       " 4109: 566,\n",
       " 4228: 567,\n",
       " 4288: 568,\n",
       " 4080: 569,\n",
       " 2046: 570,\n",
       " 4480: 571,\n",
       " 4519: 572,\n",
       " 1908: 573,\n",
       " 2882: 574,\n",
       " 3564: 575,\n",
       " 1924: 576,\n",
       " 875: 577,\n",
       " 1902: 578,\n",
       " 2681: 579,\n",
       " 1225: 580,\n",
       " 3964: 581,\n",
       " 3448: 582,\n",
       " 2574: 583,\n",
       " 4107: 584,\n",
       " 2010: 585,\n",
       " 2181: 586,\n",
       " 2712: 587,\n",
       " 918: 588,\n",
       " 3059: 589,\n",
       " 4920: 590,\n",
       " 4300: 591,\n",
       " 156: 592,\n",
       " 327: 593,\n",
       " 4604: 594,\n",
       " 458: 595,\n",
       " 4199: 596,\n",
       " 1711: 597,\n",
       " 3614: 598,\n",
       " 4952: 599,\n",
       " 4267: 600,\n",
       " 3554: 601,\n",
       " 1882: 602,\n",
       " 1633: 603,\n",
       " 941: 604,\n",
       " 3421: 605,\n",
       " 4882: 606,\n",
       " 240: 607,\n",
       " 1985: 608,\n",
       " 1960: 609,\n",
       " 185: 610,\n",
       " 4345: 611,\n",
       " 2477: 612,\n",
       " 3243: 613,\n",
       " 4814: 614,\n",
       " 910: 615,\n",
       " 3246: 616,\n",
       " 2792: 617,\n",
       " 2281: 618,\n",
       " 3333: 619,\n",
       " 2353: 620,\n",
       " 4716: 621,\n",
       " 309: 622,\n",
       " 469: 623,\n",
       " 1034: 624,\n",
       " 627: 625,\n",
       " 1996: 626,\n",
       " 3033: 627,\n",
       " 976: 628,\n",
       " 4860: 629,\n",
       " 3270: 630,\n",
       " 4714: 631,\n",
       " 2780: 632,\n",
       " 4281: 633,\n",
       " 2514: 634,\n",
       " 3541: 635,\n",
       " 3042: 636,\n",
       " 3245: 637,\n",
       " 3048: 638,\n",
       " 3885: 639,\n",
       " 803: 640,\n",
       " 1404: 641,\n",
       " 4364: 642,\n",
       " 2095: 643,\n",
       " 3841: 644,\n",
       " 2598: 645,\n",
       " 1120: 646,\n",
       " 3970: 647,\n",
       " 2864: 648,\n",
       " 1862: 649,\n",
       " 4289: 650,\n",
       " 2944: 651,\n",
       " 2713: 652,\n",
       " 2719: 653,\n",
       " 3127: 654,\n",
       " 3163: 655,\n",
       " 3721: 656,\n",
       " 661: 657,\n",
       " 2839: 658,\n",
       " 3123: 659,\n",
       " 1201: 660,\n",
       " 1900: 661,\n",
       " 4012: 662,\n",
       " 3968: 663,\n",
       " 2570: 664,\n",
       " 4922: 665,\n",
       " 3960: 666,\n",
       " 126: 667,\n",
       " 2487: 668,\n",
       " 2608: 669,\n",
       " 4732: 670,\n",
       " 1786: 671,\n",
       " 3399: 672,\n",
       " 491: 673,\n",
       " 4556: 674,\n",
       " 581: 675,\n",
       " 2163: 676,\n",
       " 2830: 677,\n",
       " 3732: 678,\n",
       " 721: 679,\n",
       " 3419: 680,\n",
       " 2111: 681,\n",
       " 3295: 682,\n",
       " 3785: 683,\n",
       " 3173: 684,\n",
       " 407: 685,\n",
       " 1002: 686,\n",
       " 3106: 687,\n",
       " 3341: 688,\n",
       " 3041: 689,\n",
       " 4807: 690,\n",
       " 2635: 691,\n",
       " 1371: 692,\n",
       " 593: 693,\n",
       " 4769: 694,\n",
       " 1232: 695,\n",
       " 2836: 696,\n",
       " 3206: 697,\n",
       " 4851: 698,\n",
       " 1402: 699,\n",
       " 4824: 700,\n",
       " 3229: 701,\n",
       " 4528: 702,\n",
       " 3028: 703,\n",
       " 4555: 704,\n",
       " 148: 705,\n",
       " 2526: 706,\n",
       " 4429: 707,\n",
       " 3481: 708,\n",
       " 478: 709,\n",
       " 4730: 710,\n",
       " 308: 711,\n",
       " 2592: 712,\n",
       " 2322: 713,\n",
       " 3587: 714,\n",
       " 1031: 715,\n",
       " 2298: 716,\n",
       " 4432: 717,\n",
       " 1223: 718,\n",
       " 157: 719,\n",
       " 657: 720,\n",
       " 4507: 721,\n",
       " 1659: 722,\n",
       " 2471: 723,\n",
       " 3939: 724,\n",
       " 81: 725,\n",
       " 4542: 726,\n",
       " 4454: 727,\n",
       " 2463: 728,\n",
       " 4346: 729,\n",
       " 1871: 730,\n",
       " 3294: 731,\n",
       " 2628: 732,\n",
       " 917: 733,\n",
       " 248: 734,\n",
       " 4691: 735,\n",
       " 1278: 736,\n",
       " 3581: 737,\n",
       " 1194: 738,\n",
       " 1963: 739,\n",
       " 3959: 740,\n",
       " 3537: 741,\n",
       " 1808: 742,\n",
       " 1096: 743,\n",
       " 4831: 744,\n",
       " 97: 745,\n",
       " 3073: 746,\n",
       " 3045: 747,\n",
       " 2738: 748,\n",
       " 4885: 749,\n",
       " 2444: 750,\n",
       " 2877: 751,\n",
       " 206: 752,\n",
       " 2160: 753,\n",
       " 4608: 754,\n",
       " 955: 755,\n",
       " 3007: 756,\n",
       " 2846: 757,\n",
       " 1781: 758,\n",
       " 4329: 759,\n",
       " 1757: 760,\n",
       " 2575: 761,\n",
       " 1674: 762,\n",
       " 430: 763,\n",
       " 1089: 764,\n",
       " 1496: 765,\n",
       " 2813: 766,\n",
       " 3694: 767,\n",
       " 4009: 768,\n",
       " 1222: 769,\n",
       " 3072: 770,\n",
       " 388: 771,\n",
       " 2157: 772,\n",
       " 3724: 773,\n",
       " 1625: 774,\n",
       " 1677: 775,\n",
       " 1853: 776,\n",
       " 457: 777,\n",
       " 1554: 778,\n",
       " 4295: 779,\n",
       " 3024: 780,\n",
       " 1547: 781,\n",
       " 1189: 782,\n",
       " 4767: 783,\n",
       " 3592: 784,\n",
       " 4468: 785,\n",
       " 4968: 786,\n",
       " 4205: 787,\n",
       " 4833: 788,\n",
       " 2008: 789,\n",
       " 12: 790,\n",
       " 2289: 791,\n",
       " 3150: 792,\n",
       " 199: 793,\n",
       " 1591: 794,\n",
       " 4910: 795,\n",
       " 3463: 796,\n",
       " 3119: 797,\n",
       " 3651: 798,\n",
       " 4175: 799,\n",
       " 397: 800,\n",
       " 660: 801,\n",
       " 2265: 802,\n",
       " 3874: 803,\n",
       " 2263: 804,\n",
       " 815: 805,\n",
       " 2491: 806,\n",
       " 4517: 807,\n",
       " 439: 808,\n",
       " 306: 809,\n",
       " 2600: 810,\n",
       " 201: 811,\n",
       " 2788: 812,\n",
       " 4020: 813,\n",
       " 852: 814,\n",
       " 2396: 815,\n",
       " 1503: 816,\n",
       " 1918: 817,\n",
       " 3131: 818,\n",
       " 568: 819,\n",
       " 525: 820,\n",
       " 968: 821,\n",
       " 448: 822,\n",
       " 2650: 823,\n",
       " 4420: 824,\n",
       " 3933: 825,\n",
       " 4590: 826,\n",
       " 4890: 827,\n",
       " 2489: 828,\n",
       " 3871: 829,\n",
       " 3275: 830,\n",
       " 2187: 831,\n",
       " 597: 832,\n",
       " 4096: 833,\n",
       " 4724: 834,\n",
       " 4388: 835,\n",
       " 1210: 836,\n",
       " 2274: 837,\n",
       " 2148: 838,\n",
       " 3929: 839,\n",
       " 2171: 840,\n",
       " 882: 841,\n",
       " 1649: 842,\n",
       " 3669: 843,\n",
       " 3070: 844,\n",
       " 4016: 845,\n",
       " 95: 846,\n",
       " 1028: 847,\n",
       " 4990: 848,\n",
       " 2407: 849,\n",
       " 4544: 850,\n",
       " 2362: 851,\n",
       " 2361: 852,\n",
       " 3423: 853,\n",
       " 4444: 854,\n",
       " 1927: 855,\n",
       " 4799: 856,\n",
       " 1502: 857,\n",
       " 401: 858,\n",
       " 431: 859,\n",
       " 4736: 860,\n",
       " 629: 861,\n",
       " 1309: 862,\n",
       " 3756: 863,\n",
       " 1053: 864,\n",
       " 1513: 865,\n",
       " 226: 866,\n",
       " 4762: 867,\n",
       " 3808: 868,\n",
       " 1235: 869,\n",
       " 3870: 870,\n",
       " 2999: 871,\n",
       " 4313: 872,\n",
       " 876: 873,\n",
       " 2494: 874,\n",
       " 4964: 875,\n",
       " 3589: 876,\n",
       " 4103: 877,\n",
       " 2021: 878,\n",
       " 943: 879,\n",
       " 2686: 880,\n",
       " 3184: 881,\n",
       " 58: 882,\n",
       " 4569: 883,\n",
       " 54: 884,\n",
       " 1629: 885,\n",
       " 1824: 886,\n",
       " 3565: 887,\n",
       " 3794: 888,\n",
       " 2307: 889,\n",
       " 3999: 890,\n",
       " 2364: 891,\n",
       " 1246: 892,\n",
       " 2120: 893,\n",
       " 1630: 894,\n",
       " 947: 895,\n",
       " 3932: 896,\n",
       " 444: 897,\n",
       " 3880: 898,\n",
       " 834: 899,\n",
       " 2731: 900,\n",
       " 2794: 901,\n",
       " 1150: 902,\n",
       " 1782: 903,\n",
       " 134: 904,\n",
       " 3431: 905,\n",
       " 3190: 906,\n",
       " 3707: 907,\n",
       " 2246: 908,\n",
       " 1420: 909,\n",
       " 3798: 910,\n",
       " 4274: 911,\n",
       " 821: 912,\n",
       " 3097: 913,\n",
       " 3847: 914,\n",
       " 3370: 915,\n",
       " 850: 916,\n",
       " 276: 917,\n",
       " 4475: 918,\n",
       " 4825: 919,\n",
       " 973: 920,\n",
       " 3349: 921,\n",
       " 3176: 922,\n",
       " 699: 923,\n",
       " 2442: 924,\n",
       " 423: 925,\n",
       " 840: 926,\n",
       " 1987: 927,\n",
       " 2905: 928,\n",
       " 3802: 929,\n",
       " 611: 930,\n",
       " 3985: 931,\n",
       " 4072: 932,\n",
       " 2931: 933,\n",
       " 4037: 934,\n",
       " 614: 935,\n",
       " 4759: 936,\n",
       " 2331: 937,\n",
       " 1478: 938,\n",
       " 1456: 939,\n",
       " 4877: 940,\n",
       " 2935: 941,\n",
       " 4942: 942,\n",
       " 4104: 943,\n",
       " 781: 944,\n",
       " 3859: 945,\n",
       " 18: 946,\n",
       " 1751: 947,\n",
       " 3954: 948,\n",
       " 1631: 949,\n",
       " 420: 950,\n",
       " 220: 951,\n",
       " 2457: 952,\n",
       " 4478: 953,\n",
       " 140: 954,\n",
       " 1481: 955,\n",
       " 524: 956,\n",
       " 3818: 957,\n",
       " 3221: 958,\n",
       " 1081: 959,\n",
       " 1542: 960,\n",
       " 865: 961,\n",
       " 4497: 962,\n",
       " 2230: 963,\n",
       " 590: 964,\n",
       " 2300: 965,\n",
       " 3506: 966,\n",
       " 2480: 967,\n",
       " 3683: 968,\n",
       " 3014: 969,\n",
       " 4549: 970,\n",
       " 872: 971,\n",
       " 4966: 972,\n",
       " 942: 973,\n",
       " 3518: 974,\n",
       " 3207: 975,\n",
       " 3251: 976,\n",
       " 3524: 977,\n",
       " 2369: 978,\n",
       " 1329: 979,\n",
       " 1417: 980,\n",
       " 2050: 981,\n",
       " 4979: 982,\n",
       " 1643: 983,\n",
       " 4744: 984,\n",
       " 2602: 985,\n",
       " 3456: 986,\n",
       " 4855: 987,\n",
       " 1969: 988,\n",
       " 4172: 989,\n",
       " 4261: 990,\n",
       " 1787: 991,\n",
       " 2559: 992,\n",
       " 3386: 993,\n",
       " 4097: 994,\n",
       " 4188: 995,\n",
       " 2901: 996,\n",
       " 1465: 997,\n",
       " 1121: 998,\n",
       " 4141: 999,\n",
       " ...}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2gnk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "206600"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function rand:\n",
      "\n",
      "rand(...) method of numpy.random.mtrand.RandomState instance\n",
      "    rand(d0, d1, ..., dn)\n",
      "    \n",
      "    Random values in a given shape.\n",
      "    \n",
      "    .. note::\n",
      "        This is a convenience function for users porting code from Matlab,\n",
      "        and wraps `random_sample`. That function takes a\n",
      "        tuple to specify the size of the output, which is consistent with\n",
      "        other NumPy functions like `numpy.zeros` and `numpy.ones`.\n",
      "    \n",
      "    Create an array of the given shape and populate it with\n",
      "    random samples from a uniform distribution\n",
      "    over ``[0, 1)``.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    d0, d1, ..., dn : int, optional\n",
      "        The dimensions of the returned array, must be non-negative.\n",
      "        If no argument is given a single Python float is returned.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    out : ndarray, shape ``(d0, d1, ..., dn)``\n",
      "        Random values.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    random\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> np.random.rand(3,2)\n",
      "    array([[ 0.14022471,  0.96360618],  #random\n",
      "           [ 0.37601032,  0.25528411],  #random\n",
      "           [ 0.49313049,  0.94909878]]) #random\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(np.random.rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(cube_inf_norm >= torus_inf_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.608920619554695"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(cube_inf_norm == torus_inf_norm) / len(cube_inf_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(206600,)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones(cube_inf_norm.shape).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 1.        , 1.        , ..., 0.60193542, 1.        ,\n",
       "       1.        ])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.stack([np.ones(cube_inf_norm.shape), ((wu * wv / W) / torus_inf_norm**d)**alpha]).min(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on ufunc object:\n",
      "\n",
      "minimum = class ufunc(builtins.object)\n",
      " |  Functions that operate element by element on whole arrays.\n",
      " |  \n",
      " |  To see the documentation for a specific ufunc, use `info`.  For\n",
      " |  example, ``np.info(np.sin)``.  Because ufuncs are written in C\n",
      " |  (for speed) and linked into Python with NumPy's ufunc facility,\n",
      " |  Python's help() function finds this page whenever help() is called\n",
      " |  on a ufunc.\n",
      " |  \n",
      " |  A detailed explanation of ufuncs can be found in the docs for :ref:`ufuncs`.\n",
      " |  \n",
      " |  Calling ufuncs:\n",
      " |  ===============\n",
      " |  \n",
      " |  op(*x[, out], where=True, **kwargs)\n",
      " |  Apply `op` to the arguments `*x` elementwise, broadcasting the arguments.\n",
      " |  \n",
      " |  The broadcasting rules are:\n",
      " |  \n",
      " |  * Dimensions of length 1 may be prepended to either array.\n",
      " |  * Arrays may be repeated along dimensions of length 1.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  *x : array_like\n",
      " |      Input arrays.\n",
      " |  out : ndarray, None, or tuple of ndarray and None, optional\n",
      " |      Alternate array object(s) in which to put the result; if provided, it\n",
      " |      must have a shape that the inputs broadcast to. A tuple of arrays\n",
      " |      (possible only as a keyword argument) must have length equal to the\n",
      " |      number of outputs; use None for uninitialized outputs to be\n",
      " |      allocated by the ufunc.\n",
      " |  where : array_like, optional\n",
      " |      This condition is broadcast over the input. At locations where the\n",
      " |      condition is True, the `out` array will be set to the ufunc result.\n",
      " |      Elsewhere, the `out` array will retain its original value.\n",
      " |      Note that if an uninitialized `out` array is created via the default\n",
      " |      ``out=None``, locations within it where the condition is False will\n",
      " |      remain uninitialized.\n",
      " |  **kwargs\n",
      " |      For other keyword-only arguments, see the :ref:`ufunc docs <ufuncs.kwargs>`.\n",
      " |  \n",
      " |  Returns\n",
      " |  -------\n",
      " |  r : ndarray or tuple of ndarray\n",
      " |      `r` will have the shape that the arrays in `x` broadcast to; if `out` is\n",
      " |      provided, it will be returned. If not, `r` will be allocated and\n",
      " |      may contain uninitialized values. If the function has more than one\n",
      " |      output, then the result will be a tuple of arrays.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __call__(self, /, *args, **kwargs)\n",
      " |      Call self as a function.\n",
      " |  \n",
      " |  __repr__(self, /)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __str__(self, /)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  accumulate(...)\n",
      " |      accumulate(array, axis=0, dtype=None, out=None)\n",
      " |      \n",
      " |      Accumulate the result of applying the operator to all elements.\n",
      " |      \n",
      " |      For a one-dimensional array, accumulate produces results equivalent to::\n",
      " |      \n",
      " |        r = np.empty(len(A))\n",
      " |        t = op.identity        # op = the ufunc being applied to A's  elements\n",
      " |        for i in range(len(A)):\n",
      " |            t = op(t, A[i])\n",
      " |            r[i] = t\n",
      " |        return r\n",
      " |      \n",
      " |      For example, add.accumulate() is equivalent to np.cumsum().\n",
      " |      \n",
      " |      For a multi-dimensional array, accumulate is applied along only one\n",
      " |      axis (axis zero by default; see Examples below) so repeated use is\n",
      " |      necessary if one wants to accumulate over multiple axes.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      array : array_like\n",
      " |          The array to act on.\n",
      " |      axis : int, optional\n",
      " |          The axis along which to apply the accumulation; default is zero.\n",
      " |      dtype : data-type code, optional\n",
      " |          The data-type used to represent the intermediate results. Defaults\n",
      " |          to the data-type of the output array if such is provided, or the\n",
      " |          the data-type of the input array if no output array is provided.\n",
      " |      out : ndarray, None, or tuple of ndarray and None, optional\n",
      " |          A location into which the result is stored. If not provided or None,\n",
      " |          a freshly-allocated array is returned. For consistency with\n",
      " |          ``ufunc.__call__``, if given as a keyword, this may be wrapped in a\n",
      " |          1-element tuple.\n",
      " |      \n",
      " |          .. versionchanged:: 1.13.0\n",
      " |             Tuples are allowed for keyword argument.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      r : ndarray\n",
      " |          The accumulated values. If `out` was supplied, `r` is a reference to\n",
      " |          `out`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      1-D array examples:\n",
      " |      \n",
      " |      >>> np.add.accumulate([2, 3, 5])\n",
      " |      array([ 2,  5, 10])\n",
      " |      >>> np.multiply.accumulate([2, 3, 5])\n",
      " |      array([ 2,  6, 30])\n",
      " |      \n",
      " |      2-D array examples:\n",
      " |      \n",
      " |      >>> I = np.eye(2)\n",
      " |      >>> I\n",
      " |      array([[1.,  0.],\n",
      " |             [0.,  1.]])\n",
      " |      \n",
      " |      Accumulate along axis 0 (rows), down columns:\n",
      " |      \n",
      " |      >>> np.add.accumulate(I, 0)\n",
      " |      array([[1.,  0.],\n",
      " |             [1.,  1.]])\n",
      " |      >>> np.add.accumulate(I) # no axis specified = axis zero\n",
      " |      array([[1.,  0.],\n",
      " |             [1.,  1.]])\n",
      " |      \n",
      " |      Accumulate along axis 1 (columns), through rows:\n",
      " |      \n",
      " |      >>> np.add.accumulate(I, 1)\n",
      " |      array([[1.,  1.],\n",
      " |             [0.,  1.]])\n",
      " |  \n",
      " |  at(...)\n",
      " |      at(a, indices, b=None)\n",
      " |      \n",
      " |      Performs unbuffered in place operation on operand 'a' for elements\n",
      " |      specified by 'indices'. For addition ufunc, this method is equivalent to\n",
      " |      ``a[indices] += b``, except that results are accumulated for elements that\n",
      " |      are indexed more than once. For example, ``a[[0,0]] += 1`` will only\n",
      " |      increment the first element once because of buffering, whereas\n",
      " |      ``add.at(a, [0,0], 1)`` will increment the first element twice.\n",
      " |      \n",
      " |      .. versionadded:: 1.8.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      a : array_like\n",
      " |          The array to perform in place operation on.\n",
      " |      indices : array_like or tuple\n",
      " |          Array like index object or slice object for indexing into first\n",
      " |          operand. If first operand has multiple dimensions, indices can be a\n",
      " |          tuple of array like index objects or slice objects.\n",
      " |      b : array_like\n",
      " |          Second operand for ufuncs requiring two operands. Operand must be\n",
      " |          broadcastable over first operand after indexing or slicing.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Set items 0 and 1 to their negative values:\n",
      " |      \n",
      " |      >>> a = np.array([1, 2, 3, 4])\n",
      " |      >>> np.negative.at(a, [0, 1])\n",
      " |      >>> a\n",
      " |      array([-1, -2,  3,  4])\n",
      " |      \n",
      " |      Increment items 0 and 1, and increment item 2 twice:\n",
      " |      \n",
      " |      >>> a = np.array([1, 2, 3, 4])\n",
      " |      >>> np.add.at(a, [0, 1, 2, 2], 1)\n",
      " |      >>> a\n",
      " |      array([2, 3, 5, 4])\n",
      " |      \n",
      " |      Add items 0 and 1 in first array to second array,\n",
      " |      and store results in first array:\n",
      " |      \n",
      " |      >>> a = np.array([1, 2, 3, 4])\n",
      " |      >>> b = np.array([1, 2])\n",
      " |      >>> np.add.at(a, [0, 1], b)\n",
      " |      >>> a\n",
      " |      array([2, 4, 3, 4])\n",
      " |  \n",
      " |  outer(...)\n",
      " |      outer(A, B, **kwargs)\n",
      " |      \n",
      " |      Apply the ufunc `op` to all pairs (a, b) with a in `A` and b in `B`.\n",
      " |      \n",
      " |      Let ``M = A.ndim``, ``N = B.ndim``. Then the result, `C`, of\n",
      " |      ``op.outer(A, B)`` is an array of dimension M + N such that:\n",
      " |      \n",
      " |      .. math:: C[i_0, ..., i_{M-1}, j_0, ..., j_{N-1}] =\n",
      " |         op(A[i_0, ..., i_{M-1}], B[j_0, ..., j_{N-1}])\n",
      " |      \n",
      " |      For `A` and `B` one-dimensional, this is equivalent to::\n",
      " |      \n",
      " |        r = empty(len(A),len(B))\n",
      " |        for i in range(len(A)):\n",
      " |            for j in range(len(B)):\n",
      " |                r[i,j] = op(A[i], B[j]) # op = ufunc in question\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      A : array_like\n",
      " |          First array\n",
      " |      B : array_like\n",
      " |          Second array\n",
      " |      kwargs : any\n",
      " |          Arguments to pass on to the ufunc. Typically `dtype` or `out`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      r : ndarray\n",
      " |          Output array\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.outer : A less powerful version of ``np.multiply.outer``\n",
      " |                    that `ravel`\\ s all inputs to 1D. This exists\n",
      " |                    primarily for compatibility with old code.\n",
      " |      \n",
      " |      tensordot : ``np.tensordot(a, b, axes=((), ()))`` and\n",
      " |                  ``np.multiply.outer(a, b)`` behave same for all\n",
      " |                  dimensions of a and b.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> np.multiply.outer([1, 2, 3], [4, 5, 6])\n",
      " |      array([[ 4,  5,  6],\n",
      " |             [ 8, 10, 12],\n",
      " |             [12, 15, 18]])\n",
      " |      \n",
      " |      A multi-dimensional example:\n",
      " |      \n",
      " |      >>> A = np.array([[1, 2, 3], [4, 5, 6]])\n",
      " |      >>> A.shape\n",
      " |      (2, 3)\n",
      " |      >>> B = np.array([[1, 2, 3, 4]])\n",
      " |      >>> B.shape\n",
      " |      (1, 4)\n",
      " |      >>> C = np.multiply.outer(A, B)\n",
      " |      >>> C.shape; C\n",
      " |      (2, 3, 1, 4)\n",
      " |      array([[[[ 1,  2,  3,  4]],\n",
      " |              [[ 2,  4,  6,  8]],\n",
      " |              [[ 3,  6,  9, 12]]],\n",
      " |             [[[ 4,  8, 12, 16]],\n",
      " |              [[ 5, 10, 15, 20]],\n",
      " |              [[ 6, 12, 18, 24]]]])\n",
      " |  \n",
      " |  reduce(...)\n",
      " |      reduce(a, axis=0, dtype=None, out=None, keepdims=False, initial=<no value>, where=True)\n",
      " |      \n",
      " |      Reduces `a`'s dimension by one, by applying ufunc along one axis.\n",
      " |      \n",
      " |      Let :math:`a.shape = (N_0, ..., N_i, ..., N_{M-1})`.  Then\n",
      " |      :math:`ufunc.reduce(a, axis=i)[k_0, ..,k_{i-1}, k_{i+1}, .., k_{M-1}]` =\n",
      " |      the result of iterating `j` over :math:`range(N_i)`, cumulatively applying\n",
      " |      ufunc to each :math:`a[k_0, ..,k_{i-1}, j, k_{i+1}, .., k_{M-1}]`.\n",
      " |      For a one-dimensional array, reduce produces results equivalent to:\n",
      " |      ::\n",
      " |      \n",
      " |       r = op.identity # op = ufunc\n",
      " |       for i in range(len(A)):\n",
      " |         r = op(r, A[i])\n",
      " |       return r\n",
      " |      \n",
      " |      For example, add.reduce() is equivalent to sum().\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      a : array_like\n",
      " |          The array to act on.\n",
      " |      axis : None or int or tuple of ints, optional\n",
      " |          Axis or axes along which a reduction is performed.\n",
      " |          The default (`axis` = 0) is perform a reduction over the first\n",
      " |          dimension of the input array. `axis` may be negative, in\n",
      " |          which case it counts from the last to the first axis.\n",
      " |      \n",
      " |          .. versionadded:: 1.7.0\n",
      " |      \n",
      " |          If this is None, a reduction is performed over all the axes.\n",
      " |          If this is a tuple of ints, a reduction is performed on multiple\n",
      " |          axes, instead of a single axis or all the axes as before.\n",
      " |      \n",
      " |          For operations which are either not commutative or not associative,\n",
      " |          doing a reduction over multiple axes is not well-defined. The\n",
      " |          ufuncs do not currently raise an exception in this case, but will\n",
      " |          likely do so in the future.\n",
      " |      dtype : data-type code, optional\n",
      " |          The type used to represent the intermediate results. Defaults\n",
      " |          to the data-type of the output array if this is provided, or\n",
      " |          the data-type of the input array if no output array is provided.\n",
      " |      out : ndarray, None, or tuple of ndarray and None, optional\n",
      " |          A location into which the result is stored. If not provided or None,\n",
      " |          a freshly-allocated array is returned. For consistency with\n",
      " |          ``ufunc.__call__``, if given as a keyword, this may be wrapped in a\n",
      " |          1-element tuple.\n",
      " |      \n",
      " |          .. versionchanged:: 1.13.0\n",
      " |             Tuples are allowed for keyword argument.\n",
      " |      keepdims : bool, optional\n",
      " |          If this is set to True, the axes which are reduced are left\n",
      " |          in the result as dimensions with size one. With this option,\n",
      " |          the result will broadcast correctly against the original `arr`.\n",
      " |      \n",
      " |          .. versionadded:: 1.7.0\n",
      " |      initial : scalar, optional\n",
      " |          The value with which to start the reduction.\n",
      " |          If the ufunc has no identity or the dtype is object, this defaults\n",
      " |          to None - otherwise it defaults to ufunc.identity.\n",
      " |          If ``None`` is given, the first element of the reduction is used,\n",
      " |          and an error is thrown if the reduction is empty.\n",
      " |      \n",
      " |          .. versionadded:: 1.15.0\n",
      " |      \n",
      " |      where : array_like of bool, optional\n",
      " |          A boolean array which is broadcasted to match the dimensions\n",
      " |          of `a`, and selects elements to include in the reduction. Note\n",
      " |          that for ufuncs like ``minimum`` that do not have an identity\n",
      " |          defined, one has to pass in also ``initial``.\n",
      " |      \n",
      " |          .. versionadded:: 1.17.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      r : ndarray\n",
      " |          The reduced array. If `out` was supplied, `r` is a reference to it.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> np.multiply.reduce([2,3,5])\n",
      " |      30\n",
      " |      \n",
      " |      A multi-dimensional array example:\n",
      " |      \n",
      " |      >>> X = np.arange(8).reshape((2,2,2))\n",
      " |      >>> X\n",
      " |      array([[[0, 1],\n",
      " |              [2, 3]],\n",
      " |             [[4, 5],\n",
      " |              [6, 7]]])\n",
      " |      >>> np.add.reduce(X, 0)\n",
      " |      array([[ 4,  6],\n",
      " |             [ 8, 10]])\n",
      " |      >>> np.add.reduce(X) # confirm: default axis value is 0\n",
      " |      array([[ 4,  6],\n",
      " |             [ 8, 10]])\n",
      " |      >>> np.add.reduce(X, 1)\n",
      " |      array([[ 2,  4],\n",
      " |             [10, 12]])\n",
      " |      >>> np.add.reduce(X, 2)\n",
      " |      array([[ 1,  5],\n",
      " |             [ 9, 13]])\n",
      " |      \n",
      " |      You can use the ``initial`` keyword argument to initialize the reduction\n",
      " |      with a different value, and ``where`` to select specific elements to include:\n",
      " |      \n",
      " |      >>> np.add.reduce([10], initial=5)\n",
      " |      15\n",
      " |      >>> np.add.reduce(np.ones((2, 2, 2)), axis=(0, 2), initial=10)\n",
      " |      array([14., 14.])\n",
      " |      >>> a = np.array([10., np.nan, 10])\n",
      " |      >>> np.add.reduce(a, where=~np.isnan(a))\n",
      " |      20.0\n",
      " |      \n",
      " |      Allows reductions of empty arrays where they would normally fail, i.e.\n",
      " |      for ufuncs without an identity.\n",
      " |      \n",
      " |      >>> np.minimum.reduce([], initial=np.inf)\n",
      " |      inf\n",
      " |      >>> np.minimum.reduce([[1., 2.], [3., 4.]], initial=10., where=[True, False])\n",
      " |      array([ 1., 10.])\n",
      " |      >>> np.minimum.reduce([])\n",
      " |      Traceback (most recent call last):\n",
      " |          ...\n",
      " |      ValueError: zero-size array to reduction operation minimum which has no identity\n",
      " |  \n",
      " |  reduceat(...)\n",
      " |      reduceat(a, indices, axis=0, dtype=None, out=None)\n",
      " |      \n",
      " |      Performs a (local) reduce with specified slices over a single axis.\n",
      " |      \n",
      " |      For i in ``range(len(indices))``, `reduceat` computes\n",
      " |      ``ufunc.reduce(a[indices[i]:indices[i+1]])``, which becomes the i-th\n",
      " |      generalized \"row\" parallel to `axis` in the final result (i.e., in a\n",
      " |      2-D array, for example, if `axis = 0`, it becomes the i-th row, but if\n",
      " |      `axis = 1`, it becomes the i-th column).  There are three exceptions to this:\n",
      " |      \n",
      " |      * when ``i = len(indices) - 1`` (so for the last index),\n",
      " |        ``indices[i+1] = a.shape[axis]``.\n",
      " |      * if ``indices[i] >= indices[i + 1]``, the i-th generalized \"row\" is\n",
      " |        simply ``a[indices[i]]``.\n",
      " |      * if ``indices[i] >= len(a)`` or ``indices[i] < 0``, an error is raised.\n",
      " |      \n",
      " |      The shape of the output depends on the size of `indices`, and may be\n",
      " |      larger than `a` (this happens if ``len(indices) > a.shape[axis]``).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      a : array_like\n",
      " |          The array to act on.\n",
      " |      indices : array_like\n",
      " |          Paired indices, comma separated (not colon), specifying slices to\n",
      " |          reduce.\n",
      " |      axis : int, optional\n",
      " |          The axis along which to apply the reduceat.\n",
      " |      dtype : data-type code, optional\n",
      " |          The type used to represent the intermediate results. Defaults\n",
      " |          to the data type of the output array if this is provided, or\n",
      " |          the data type of the input array if no output array is provided.\n",
      " |      out : ndarray, None, or tuple of ndarray and None, optional\n",
      " |          A location into which the result is stored. If not provided or None,\n",
      " |          a freshly-allocated array is returned. For consistency with\n",
      " |          ``ufunc.__call__``, if given as a keyword, this may be wrapped in a\n",
      " |          1-element tuple.\n",
      " |      \n",
      " |          .. versionchanged:: 1.13.0\n",
      " |             Tuples are allowed for keyword argument.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      r : ndarray\n",
      " |          The reduced values. If `out` was supplied, `r` is a reference to\n",
      " |          `out`.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      A descriptive example:\n",
      " |      \n",
      " |      If `a` is 1-D, the function `ufunc.accumulate(a)` is the same as\n",
      " |      ``ufunc.reduceat(a, indices)[::2]`` where `indices` is\n",
      " |      ``range(len(array) - 1)`` with a zero placed\n",
      " |      in every other element:\n",
      " |      ``indices = zeros(2 * len(a) - 1)``, ``indices[1::2] = range(1, len(a))``.\n",
      " |      \n",
      " |      Don't be fooled by this attribute's name: `reduceat(a)` is not\n",
      " |      necessarily smaller than `a`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      To take the running sum of four successive values:\n",
      " |      \n",
      " |      >>> np.add.reduceat(np.arange(8),[0,4, 1,5, 2,6, 3,7])[::2]\n",
      " |      array([ 6, 10, 14, 18])\n",
      " |      \n",
      " |      A 2-D example:\n",
      " |      \n",
      " |      >>> x = np.linspace(0, 15, 16).reshape(4,4)\n",
      " |      >>> x\n",
      " |      array([[ 0.,   1.,   2.,   3.],\n",
      " |             [ 4.,   5.,   6.,   7.],\n",
      " |             [ 8.,   9.,  10.,  11.],\n",
      " |             [12.,  13.,  14.,  15.]])\n",
      " |      \n",
      " |      ::\n",
      " |      \n",
      " |       # reduce such that the result has the following five rows:\n",
      " |       # [row1 + row2 + row3]\n",
      " |       # [row4]\n",
      " |       # [row2]\n",
      " |       # [row3]\n",
      " |       # [row1 + row2 + row3 + row4]\n",
      " |      \n",
      " |      >>> np.add.reduceat(x, [0, 3, 1, 2, 0])\n",
      " |      array([[12.,  15.,  18.,  21.],\n",
      " |             [12.,  13.,  14.,  15.],\n",
      " |             [ 4.,   5.,   6.,   7.],\n",
      " |             [ 8.,   9.,  10.,  11.],\n",
      " |             [24.,  28.,  32.,  36.]])\n",
      " |      \n",
      " |      ::\n",
      " |      \n",
      " |       # reduce such that result has the following two columns:\n",
      " |       # [col1 * col2 * col3, col4]\n",
      " |      \n",
      " |      >>> np.multiply.reduceat(x, [0, 3], 1)\n",
      " |      array([[   0.,     3.],\n",
      " |             [ 120.,     7.],\n",
      " |             [ 720.,    11.],\n",
      " |             [2184.,    15.]])\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  identity\n",
      " |      The identity value.\n",
      " |      \n",
      " |      Data attribute containing the identity element for the ufunc, if it has one.\n",
      " |      If it does not, the attribute value is None.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> np.add.identity\n",
      " |      0\n",
      " |      >>> np.multiply.identity\n",
      " |      1\n",
      " |      >>> np.power.identity\n",
      " |      1\n",
      " |      >>> print(np.exp.identity)\n",
      " |      None\n",
      " |  \n",
      " |  nargs\n",
      " |      The number of arguments.\n",
      " |      \n",
      " |      Data attribute containing the number of arguments the ufunc takes, including\n",
      " |      optional ones.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Typically this value will be one more than what you might expect because all\n",
      " |      ufuncs take  the optional \"out\" argument.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> np.add.nargs\n",
      " |      3\n",
      " |      >>> np.multiply.nargs\n",
      " |      3\n",
      " |      >>> np.power.nargs\n",
      " |      3\n",
      " |      >>> np.exp.nargs\n",
      " |      2\n",
      " |  \n",
      " |  nin\n",
      " |      The number of inputs.\n",
      " |      \n",
      " |      Data attribute containing the number of arguments the ufunc treats as input.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> np.add.nin\n",
      " |      2\n",
      " |      >>> np.multiply.nin\n",
      " |      2\n",
      " |      >>> np.power.nin\n",
      " |      2\n",
      " |      >>> np.exp.nin\n",
      " |      1\n",
      " |  \n",
      " |  nout\n",
      " |      The number of outputs.\n",
      " |      \n",
      " |      Data attribute containing the number of arguments the ufunc treats as output.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Since all ufuncs can take output arguments, this will always be (at least) 1.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> np.add.nout\n",
      " |      1\n",
      " |      >>> np.multiply.nout\n",
      " |      1\n",
      " |      >>> np.power.nout\n",
      " |      1\n",
      " |      >>> np.exp.nout\n",
      " |      1\n",
      " |  \n",
      " |  ntypes\n",
      " |      The number of types.\n",
      " |      \n",
      " |      The number of numerical NumPy types - of which there are 18 total - on which\n",
      " |      the ufunc can operate.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.ufunc.types\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> np.add.ntypes\n",
      " |      18\n",
      " |      >>> np.multiply.ntypes\n",
      " |      18\n",
      " |      >>> np.power.ntypes\n",
      " |      17\n",
      " |      >>> np.exp.ntypes\n",
      " |      7\n",
      " |      >>> np.remainder.ntypes\n",
      " |      14\n",
      " |  \n",
      " |  signature\n",
      " |      Definition of the core elements a generalized ufunc operates on.\n",
      " |      \n",
      " |      The signature determines how the dimensions of each input/output array\n",
      " |      are split into core and loop dimensions:\n",
      " |      \n",
      " |      1. Each dimension in the signature is matched to a dimension of the\n",
      " |         corresponding passed-in array, starting from the end of the shape tuple.\n",
      " |      2. Core dimensions assigned to the same label in the signature must have\n",
      " |         exactly matching sizes, no broadcasting is performed.\n",
      " |      3. The core dimensions are removed from all inputs and the remaining\n",
      " |         dimensions are broadcast together, defining the loop dimensions.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Generalized ufuncs are used internally in many linalg functions, and in\n",
      " |      the testing suite; the examples below are taken from these.\n",
      " |      For ufuncs that operate on scalars, the signature is None, which is\n",
      " |      equivalent to '()' for every argument.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> np.core.umath_tests.matrix_multiply.signature\n",
      " |      '(m,n),(n,p)->(m,p)'\n",
      " |      >>> np.linalg._umath_linalg.det.signature\n",
      " |      '(m,m)->()'\n",
      " |      >>> np.add.signature is None\n",
      " |      True  # equivalent to '(),()->()'\n",
      " |  \n",
      " |  types\n",
      " |      Returns a list with types grouped input->output.\n",
      " |      \n",
      " |      Data attribute listing the data-type \"Domain-Range\" groupings the ufunc can\n",
      " |      deliver. The data-types are given using the character codes.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.ufunc.ntypes\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> np.add.types\n",
      " |      ['??->?', 'bb->b', 'BB->B', 'hh->h', 'HH->H', 'ii->i', 'II->I', 'll->l',\n",
      " |      'LL->L', 'qq->q', 'QQ->Q', 'ff->f', 'dd->d', 'gg->g', 'FF->F', 'DD->D',\n",
      " |      'GG->G', 'OO->O']\n",
      " |      \n",
      " |      >>> np.multiply.types\n",
      " |      ['??->?', 'bb->b', 'BB->B', 'hh->h', 'HH->H', 'ii->i', 'II->I', 'll->l',\n",
      " |      'LL->L', 'qq->q', 'QQ->Q', 'ff->f', 'dd->d', 'gg->g', 'FF->F', 'DD->D',\n",
      " |      'GG->G', 'OO->O']\n",
      " |      \n",
      " |      >>> np.power.types\n",
      " |      ['bb->b', 'BB->B', 'hh->h', 'HH->H', 'ii->i', 'II->I', 'll->l', 'LL->L',\n",
      " |      'qq->q', 'QQ->Q', 'ff->f', 'dd->d', 'gg->g', 'FF->F', 'DD->D', 'GG->G',\n",
      " |      'OO->O']\n",
      " |      \n",
      " |      >>> np.exp.types\n",
      " |      ['f->f', 'd->d', 'g->g', 'F->F', 'D->D', 'G->G', 'O->O']\n",
      " |      \n",
      " |      >>> np.remainder.types\n",
      " |      ['bb->b', 'BB->B', 'hh->h', 'HH->H', 'ii->i', 'II->I', 'll->l', 'LL->L',\n",
      " |      'qq->q', 'QQ->Q', 'ff->f', 'dd->d', 'gg->g', 'OO->O']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(np.minimum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "girgenv2",
   "language": "python",
   "name": "girgenv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
